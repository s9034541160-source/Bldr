#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ë–´–°–¢–†–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø –ö–ê–ß–ï–°–¢–í–ê RAG PIPELINE
=======================================

–û—Å–Ω–æ–≤–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
1. –ó–∞–º–µ–Ω–∞ Rubern –Ω–∞ SBERT –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–∞–±–æ—Ç
2. –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥
4. –ê–∫—Ç—É–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

üéØ –¶–µ–ª—å: –£–≤–µ–ª–∏—á–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ 15-20%
"""

import os
import re
import json
import numpy as np
from typing import List, Dict, Any, Tuple
from sentence_transformers import SentenceTransformer
import torch
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

class EnhancedWorkExtractor:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å —Ä–∞–±–æ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SBERT –≤–º–µ—Å—Ç–æ Rubern"""
    
    def __init__(self, embedding_model: SentenceTransformer):
        self.embedding_model = embedding_model
        
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞–±–æ—Ç
        self.work_patterns = [
            "–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–∞–±–æ—Ç –ø–æ",
            "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ",
            "–º–æ–Ω—Ç–∞–∂",
            "—É—Å—Ç–∞–Ω–æ–≤–∫–∞", 
            "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ",
            "—Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è",
            "–∫–∞–ø–∏—Ç–∞–ª—å–Ω—ã–π —Ä–µ–º–æ–Ω—Ç",
            "–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ",
            "–∏–∑–≥–æ—Ç–æ–≤–ª–µ–Ω–∏–µ",
            "–ø–æ—Å—Ç–∞–≤–∫–∞ –∏ –º–æ–Ω—Ç–∞–∂",
            "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ",
            "–∏—Å–ø—ã—Ç–∞–Ω–∏—è",
            "–ø—É—Å–∫–æ–Ω–∞–ª–∞–¥–æ—á–Ω—ã–µ —Ä–∞–±–æ—Ç—ã"
        ]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ–¥–∏–Ω —Ä–∞–∑
        self.pattern_embeddings = self.embedding_model.encode(self.work_patterns)
        
    def extract_works_with_sbert(self, content: str, seed_works: List[str], doc_type: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SBERT –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        –ó–∞–º–µ–Ω—è–µ—Ç Rubern markup –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–∞–±–æ—Ç
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = self._split_into_sentences(content)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –¥–ª–∏–Ω–µ –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é
        candidate_sentences = [
            sent for sent in sentences 
            if 10 < len(sent) < 200 and self._contains_work_indicators(sent)
        ]
        
        if not candidate_sentences:
            return seed_works[:10]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º seed works –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
            
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        try:
            candidate_embeddings = self.embedding_model.encode(candidate_sentences)
        except Exception as e:
            logger.warning(f"Failed to encode candidates: {e}")
            return seed_works[:10]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ —Ä–∞–±–æ—Ç
        similarities = np.dot(candidate_embeddings, self.pattern_embeddings.T)
        max_similarities = np.max(similarities, axis=1)
        
        # –û—Ç–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        threshold = 0.3  # –ü–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        relevant_indices = np.where(max_similarities > threshold)[0]
        
        extracted_works = []
        for idx in relevant_indices:
            work = self._clean_work_description(candidate_sentences[idx])
            if work and len(work) > 5:
                extracted_works.append(work)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å seed works –∏ —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        all_works = list(dict.fromkeys(seed_works + extracted_works))
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ 20 –ª—É—á—à–∏—Ö —Ä–∞–±–æ—Ç
        return all_works[:20]
        
    def _split_into_sentences(self, content: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞ —Å —É—á–µ—Ç–æ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        sentences = re.split(r'[.!?]\s+|\n\s*\n', content)
        return [sent.strip() for sent in sentences if sent.strip()]
        
    def _contains_work_indicators(self, sentence: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∞–±–æ—Ç"""
        indicators = [
            r'\b\d+\.\d+\.\d+\b',  # –ù–æ–º–µ—Ä–∞ –ø—É–Ω–∫—Ç–æ–≤
            r'\b(—Ä–∞–±–æ—Ç|–º–æ–Ω—Ç–∞–∂|—É—Å—Ç–∞–Ω–æ–≤–∫|—É—Å—Ç—Ä–æ–π—Å—Ç–≤|—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤)\w*\b',
            r'\b(–≤—ã–ø–æ–ª–Ω|–ø—Ä–æ–∏–∑–≤–µ–¥|–æ—Å—É—â–µ—Å—Ç–≤–ª)\w*\b',
            r'\b(–ø—Ä–æ–µ–∫—Ç|–∏–∑–≥–æ—Ç–æ–≤–ª|–ø–æ—Å—Ç–∞–≤–∫)\w*\b'
        ]
        return any(re.search(pattern, sentence, re.IGNORECASE) for pattern in indicators)
        
    def _clean_work_description(self, work: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –æ–ø–∏—Å–∞–Ω–∏—è —Ä–∞–±–æ—Ç—ã"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        work = re.sub(r'^\d+\.\d*\.?\s*', '', work)  # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ –ø—É–Ω–∫—Ç–æ–≤
        work = re.sub(r'\s+', ' ', work)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        work = work.strip(' .,-')
        return work[:100]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É


class EnhancedDocumentCategorizer:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.category_patterns = {
            'construction_norms': {
                'keywords': ['–°–ü', '–°–ù–∏–ü', '–ì–û–°–¢', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ'],
                'weight': 1.0,
                'folder': '09. –°–¢–†–û–ò–¢–ï–õ–¨–°–¢–í–û - –°–í–û–î–´ –ü–†–ê–í–ò–õ'
            },
            'estimates': {
                'keywords': ['—Å–º–µ—Ç–∞', '—Ä–∞—Å—Ü–µ–Ω–∫–∏', '–ì–≠–°–ù', '–§–ï–†', '—Å—Ç–æ–∏–º–æ—Å—Ç—å'],
                'weight': 1.2,
                'folder': '05. –°–¢–†–û–ò–¢–ï–õ–¨–°–¢–í–û - –°–ú–ï–¢–´'
            },
            'safety': {
                'keywords': ['–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞', '–°–ò–ó', '—Ç–µ—Ö–Ω–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏'],
                'weight': 1.1,
                'folder': '28. –û–•–†–ê–ù–ê –¢–†–£–î–ê - –ó–ê–ö–û–ù–´'
            },
            'finance': {
                'keywords': ['—Ñ–∏–Ω–∞–Ω—Å', '–±—é–¥–∂–µ—Ç', '–Ω–∞–ª–æ–≥', '–±—É—Ö–≥–∞–ª—Ç–µ—Ä', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å'],
                'weight': 0.9,
                'folder': '10. –§–ò–ù–ê–ù–°–´ - –ó–ê–ö–û–ù–´'
            },
            'hr': {
                'keywords': ['–∫–∞–¥—Ä', '–ø–µ—Ä—Å–æ–Ω–∞–ª', '—Ç—Ä—É–¥–æ–≤', '–æ—Ç–ø—É—Å–∫', '–∑–∞—Ä–ø–ª–∞—Ç'],
                'weight': 0.8,
                'folder': '35. HR - –¢–†–£–î–û–í–û–ï –ü–†–ê–í–û'
            }
        }
        
    def categorize_document(self, content: str, filename: str, doc_type: str) -> Tuple[str, float]:
        """
        –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        """
        content_lower = content.lower()
        filename_lower = filename.lower()
        
        category_scores = {}
        
        for category, config in self.category_patterns.items():
            score = 0.0
            
            # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
            for keyword in config['keywords']:
                # –ü–æ–¥—Å—á–µ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ
                content_matches = len(re.findall(rf'\b{keyword}', content_lower))
                # –ü–æ–¥—Å—á–µ—Ç –≤—Ö–æ–∂–¥–µ–Ω–∏–π –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–±–æ–ª—å—à–∏–π –≤–µ—Å)
                filename_matches = len(re.findall(rf'\b{keyword}', filename_lower)) * 3
                
                score += (content_matches + filename_matches) * config['weight']
            
            # –ë–æ–Ω—É—Å –∑–∞ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if doc_type in ['norms', 'sp'] and category == 'construction_norms':
                score += 10
            elif doc_type in ['smeta', 'estimate'] and category == 'estimates':
                score += 10
                
            category_scores[category] = score
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à—É—é –∫–∞—Ç–µ–≥–æ—Ä–∏—é
        if not category_scores or max(category_scores.values()) == 0:
            return '99. –î–†–£–ì–ò–ï –î–û–ö–£–ú–ï–ù–¢–´', 0.5
            
        best_category = max(category_scores.items(), key=lambda x: x[1])
        folder_name = self.category_patterns[best_category[0]]['folder']
        confidence = min(best_category[1] / 20.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1
        
        return folder_name, confidence


class EnhancedChunker:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.chunk_size = 800  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        self.overlap = 100     # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        
    def smart_chunk(self, content: str, doc_structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        chunks = []
        
        # –ü–æ–ø—Ä–æ–±—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥
        structure_chunks = self._structure_based_chunking(content, doc_structure)
        if structure_chunks:
            chunks.extend(structure_chunks)
        
        # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π
        if len(chunks) < 3:
            semantic_chunks = self._semantic_chunking(content)
            chunks.extend(semantic_chunks)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
        return self._validate_and_enhance_chunks(chunks)
    
    def _structure_based_chunking(self, content: str, doc_structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ß–∞–Ω–∫–∏–Ω–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º
        sections = re.split(r'\n\s*\d+\.\s+[–ê-–Ø–Å][–∞-—è—ë\s]+\n', content)
        
        for i, section in enumerate(sections):
            if len(section.strip()) < 100:
                continue
                
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –Ω–∞ –ø–æ–¥—á–∞–Ω–∫–∏
            if len(section) > self.chunk_size * 2:
                sub_chunks = self._split_long_section(section)
                for j, sub_chunk in enumerate(sub_chunks):
                    chunks.append({
                        'text': sub_chunk,
                        'type': 'section',
                        'section_id': f"{i}.{j}",
                        'length': len(sub_chunk)
                    })
            else:
                chunks.append({
                    'text': section,
                    'type': 'section', 
                    'section_id': str(i),
                    'length': len(section)
                })
        
        return chunks
    
    def _semantic_chunking(self, content: str) -> List[Dict[str, Any]]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —á–∞–Ω–∫–∏–Ω–≥"""
        chunks = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        current_chunk = ""
        chunk_id = 0
        
        for paragraph in paragraphs:
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–±–∑–∞—Ü–∞ –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç
            if len(current_chunk) + len(paragraph) < self.chunk_size:
                current_chunk += paragraph + "\n\n"
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                if current_chunk:
                    chunks.append({
                        'text': current_chunk.strip(),
                        'type': 'semantic',
                        'chunk_id': chunk_id,
                        'length': len(current_chunk)
                    })
                    chunk_id += 1
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
                current_chunk = paragraph + "\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append({
                'text': current_chunk.strip(),
                'type': 'semantic',
                'chunk_id': chunk_id,
                'length': len(current_chunk)
            })
        
        return chunks
    
    def _split_long_section(self, section: str) -> List[str]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω–Ω–æ–π —Å–µ–∫—Ü–∏–∏ –Ω–∞ —á–∞—Å—Ç–∏"""
        parts = []
        sentences = re.split(r'[.!?]\s+', section)
        
        current_part = ""
        for sentence in sentences:
            if len(current_part) + len(sentence) < self.chunk_size:
                current_part += sentence + ". "
            else:
                if current_part:
                    parts.append(current_part.strip())
                current_part = sentence + ". "
        
        if current_part:
            parts.append(current_part.strip())
            
        return parts
    
    def _validate_and_enhance_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤"""
        valid_chunks = []
        
        for chunk in chunks:
            text = chunk['text']
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏
            if len(text) < 50:
                continue
                
            # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            if len(text) > self.chunk_size * 1.5:
                text = text[:self.chunk_size] + "..."
                chunk['text'] = text
                chunk['truncated'] = True
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            chunk['word_count'] = len(text.split())
            chunk['has_numbers'] = bool(re.search(r'\d+', text))
            chunk['has_lists'] = bool(re.search(r'^\s*\d+[\.\)]\s+', text, re.MULTILINE))
            
            valid_chunks.append(chunk)
        
        return valid_chunks


class NormativeDatabaseUpdater:
    """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, db_path: str):
        self.db_path = Path(db_path)
        
    def update_database(self) -> Dict[str, Any]:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –±–∞–∑—É
        existing_data = self._load_existing_data()
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        scanned_files = self._scan_document_files()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        updated_data = {
            'timestamp': '2025-09-17T15:00:00Z',
            'total_documents': len(scanned_files),
            'sources': existing_data.get('sources', {}),
            'categories': self._update_categories(scanned_files),
            'documents': scanned_files
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –±–∞–∑—É
        self._save_updated_data(updated_data)
        
        return {
            'status': 'success',
            'total_documents': len(scanned_files),
            'categories': len(updated_data['categories']),
            'new_documents': len(scanned_files) - len(existing_data.get('documents', []))
        }
    
    def _load_existing_data(self) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        catalog_file = self.db_path / 'ntd_catalog.json'
        if catalog_file.exists():
            try:
                with open(catalog_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Failed to load existing catalog: {e}")
        return {}
    
    def _scan_document_files(self) -> List[Dict[str, Any]]:
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        documents = []
        base_path = Path("I:/docs/clean_base")
        
        if not base_path.exists():
            return documents
        
        # –°–∫–∞–Ω–∏—Ä—É–µ–º PDF –∏ DOC —Ñ–∞–π–ª—ã
        for pattern in ['*.pdf', '*.doc', '*.docx']:
            for file_path in base_path.glob(pattern):
                try:
                    stat = file_path.stat()
                    documents.append({
                        'name': file_path.name,
                        'path': str(file_path),
                        'size': stat.st_size,
                        'modified': stat.st_mtime,
                        'type': self._detect_document_type(file_path.name)
                    })
                except Exception as e:
                    logger.warning(f"Failed to process {file_path}: {e}")
        
        return documents
    
    def _detect_document_type(self, filename: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        filename_lower = filename.lower()
        
        if any(pattern in filename_lower for pattern in ['sp', '—Å–Ω–∏–ø', '–≥–æ—Å—Ç']):
            return 'construction_norm'
        elif any(pattern in filename_lower for pattern in ['gesn', '–≥—ç—Å–Ω', '—Å–º–µ—Ç']):
            return 'estimate'
        elif any(pattern in filename_lower for pattern in ['–º–µ—Ç–æ–¥–∏–∫', '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü']):
            return 'methodology'
        else:
            return 'other'
    
    def _update_categories(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        categories = {}
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ç–∏–ø–∞–º
        type_counts = {}
        for doc in documents:
            doc_type = doc['type']
            type_counts[doc_type] = type_counts.get(doc_type, 0) + 1
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        categories['construction'] = {
            'description': '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –ø—Ä–∞–≤–∏–ª–∞',
            'count': type_counts.get('construction_norm', 0),
            'types': ['–°–ü', '–°–ù–∏–ü', '–ì–û–°–¢']
        }
        
        categories['estimates'] = {
            'description': '–°–º–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ —Ä–∞—Å—Ü–µ–Ω–∫–∏',
            'count': type_counts.get('estimate', 0),
            'types': ['–ì–≠–°–ù', '–§–ï–†', '–¢–ï–†']
        }
        
        categories['methodology'] = {
            'description': '–ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã',
            'count': type_counts.get('methodology', 0),
            'types': ['–ú–î–°', '–ú–†', '–ú–µ—Ç–æ–¥–∏–∫–∞']
        }
        
        return categories
    
    def _save_updated_data(self, data: Dict[str, Any]) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        catalog_file = self.db_path / 'ntd_catalog_updated.json'
        try:
            with open(catalog_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            logger.info(f"Updated catalog saved to {catalog_file}")
        except Exception as e:
            logger.error(f"Failed to save updated catalog: {e}")


def apply_improvements_to_trainer(trainer_instance):
    """
    –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º—É —ç–∫–∑–µ–º–ø–ª—è—Ä—É —Ç—Ä–µ–Ω–µ—Ä–∞
    """
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    work_extractor = EnhancedWorkExtractor(trainer_instance.embedding_model)
    categorizer = EnhancedDocumentCategorizer()
    chunker = EnhancedChunker()
    db_updater = NormativeDatabaseUpdater("I:/docs/clean_base")
    
    # –ó–∞–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥—ã —Ç—Ä–µ–Ω–µ—Ä–∞ –Ω–∞ —É–ª—É—á—à–µ–Ω–Ω—ã–µ –≤–µ—Ä—Å–∏–∏
    def enhanced_stage7_rubern_markup(content: str, doc_type: str, doc_subtype: str, seed_works: List[str], structural_data: Dict[str, Any]) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∑–∞–º–µ–Ω–∞ Rubern markup —Å SBERT"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SBERT –≤–º–µ—Å—Ç–æ Rubern
        enhanced_works = work_extractor.extract_works_with_sbert(content, seed_works, doc_type)
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Å–æ–≤–º–µ—Å—Ç–∏–º—É—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –∫–æ–¥–æ–º
        rubern_data = {
            'works': enhanced_works,
            'dependencies': [],  # –ü–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—É—Å—Ç—ã–º, –º–æ–∂–Ω–æ —Ä–∞–∑–≤–∏—Ç—å –ø–æ–∑–∂–µ
            'doc_structure': structural_data,
            'rubern_markup': '\n'.join([f"\\—Ä–∞–±–æ—Ç–∞{{{work}}}" for work in enhanced_works]),
            'entities': {
                'WORK': enhanced_works[:10],
                'TYPE': [doc_type, doc_subtype]
            }
        }
        
        return rubern_data
    
    def enhanced_categorization(content: str, filename: str, doc_type: str) -> Tuple[str, float]:
        """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è"""
        return categorizer.categorize_document(content, filename, doc_type)
    
    def enhanced_chunking(rubern_data: Dict[str, Any], metadata: Dict[str, Any], doc_type_res: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥"""
        content = str(rubern_data.get('doc_structure', {}))
        return chunker.smart_chunk(content, rubern_data.get('doc_structure', {}))
    
    # –ó–∞–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥—ã
    trainer_instance._stage7_rubern_markup_enhanced = enhanced_stage7_rubern_markup
    trainer_instance.enhanced_categorization = enhanced_categorization
    trainer_instance.enhanced_chunking = enhanced_chunking
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–∑—É –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    update_result = db_updater.update_database()
    
    return {
        'status': 'improvements_applied',
        'components': ['SBERT work extraction', 'Enhanced categorization', 'Optimized chunking', 'Updated normative DB'],
        'db_update': update_result
    }


if __name__ == "__main__":
    print("üöÄ –ë–´–°–¢–†–´–ï –£–õ–£–ß–®–ï–ù–ò–Ø RAG PIPELINE")
    print("================================")
    print("‚úÖ 1. –ó–∞–º–µ–Ω–∞ Rubern –Ω–∞ SBERT –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–∞–±–æ—Ç")
    print("‚úÖ 2. –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º")
    print("‚úÖ 3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã")
    print("‚úÖ 4. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("\nüìà –û–∂–∏–¥–∞–µ–º–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞: 15-20%")
    print("üí° –î–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è: from enhanced_rag_improvements import apply_improvements_to_trainer")