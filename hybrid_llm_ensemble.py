"""
–ì–∏–±—Ä–∏–¥–Ω—ã–π LLM –ê–Ω—Å–∞–º–±–ª—å –¥–ª—è RTX 4060 (8GB VRAM) - GPU –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø
- Mistral-7B-Instruct: GPU —Å 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π - 4.5GB, 32K –∫–æ–Ω—Ç–µ–∫—Å—Ç
- SBERT: PyTorch - 1.5GB  
- RuT5: –£–î–ê–õ–ï–ù –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
–ò—Ç–æ–≥–æ: 6.0GB (–ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ 8GB VRAM!) + –°–ö–û–†–û–°–¢–¨ GPU!
"""

import os
import json
import torch
import logging
from typing import Dict, Optional
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from sentence_transformers import SentenceTransformer

@dataclass
class HybridLLMConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è - GPU –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø"""
    # GPU Qwen2.5-7B —Å 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π (–ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Ç–∫—Ä—ã—Ç–∞—è –º–æ–¥–µ–ª—å)
    mistral_model_name: str = "Qwen/Qwen2.5-7B-Instruct"
    mistral_device: str = "cuda"
    mistral_max_length: int = 32768  # 32K –∫–æ–Ω—Ç–µ–∫—Å—Ç (–±–æ–ª—å—à–µ —á–µ–º —É DeepSeek!)
    
    # PyTorch SBERT - –£–ü–†–ê–í–õ–Ø–ï–¢–°–Ø RAG TRAINER
    sbert_model: str = ""  # –ü—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ - –º–æ–¥–µ–ª—å —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è enterprise_rag_trainer_full.py
    sbert_device: str = "cuda"
    
    # PyTorch RuT5 - –£–î–ê–õ–ï–ù –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    # rut5_model: str = "ai-forever/ruT5-base"
    # rut5_device: str = "cuda"
    
    # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    temperature: float = 0.7
    max_length: int = 2048
    
    @classmethod
    def from_env(cls):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        return cls(
            mistral_model_name=os.getenv('MISTRAL_MODEL_NAME', cls.mistral_model_name),
            sbert_model=os.getenv('SBERT_MODEL', cls.sbert_model)
            # rut5_model=os.getenv('RUT5_MODEL', cls.rut5_model)  # –£–î–ê–õ–ï–ù
        )

class HybridLLMEnsemble:
    """–ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å LLM –¥–ª—è RTX 4060 - GPU –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø"""
    
    def __init__(self, config: HybridLLMConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.mistral_model = None
        self.mistral_tokenizer = None
        self.sbert_model = None
        # self.rut5_model = None  # –£–î–ê–õ–ï–ù
        # self.rut5_tokenizer = None  # –£–î–ê–õ–ï–ù
        
        self._initialize_ensemble()
    
    def _initialize_ensemble(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∞–Ω—Å–∞–º–±–ª—è - GPU –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø"""
        try:
            # 1. –ó–∞–≥—Ä—É–∑–∫–∞ GPU Mistral-7B —Å 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π (4.5GB)
            self._initialize_mistral_gpu()
            
            # 2. –ó–∞–≥—Ä—É–∑–∫–∞ PyTorch SBERT (1.5GB)
            self._initialize_sbert()
            
            # 3. RuT5 –£–î–ê–õ–ï–ù –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            
            self.logger.info("‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            self.logger.info(f"üéÆ Qwen2.5-7B-Instruct (GPU): {self.config.mistral_model_name}")
            self.logger.info(f"üéÆ SBERT: {self.config.sbert_device}")
            # self.logger.info(f"üéÆ RuT5: {self.config.rut5_device}")  # –£–î–ê–õ–ï–ù
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω—Å–∞–º–±–ª—è: {e}")
            raise
    
    def _initialize_mistral_gpu(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPU Mistral-7B —Å 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π"""
        try:
            self.logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ Qwen2.5-7B-Instruct (GPU —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π)...")
            
            # 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True
            )
            
            # 2. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞
            self.logger.info("üìù –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞...")
            self.mistral_tokenizer = AutoTokenizer.from_pretrained(
                self.config.mistral_model_name,
                trust_remote_code=True
            )
            
            # 3. –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π –∏ –∞–≤—Ç–æ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º –Ω–∞ GPU
            self.logger.info("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU...")
            self.mistral_model = AutoModelForCausalLM.from_pretrained(
                self.config.mistral_model_name,
                quantization_config=bnb_config,
                device_map="auto",  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Ä–∞–∑–º–µ—Å—Ç–∏—Ç –Ω–∞ GPU
                torch_dtype=torch.float16,
                trust_remote_code=True
            )
            
            self.logger.info("‚úÖ Qwen2.5-7B-Instruct (GPU) –∑–∞–≥—Ä—É–∂–µ–Ω —Å 4-–±–∏—Ç–Ω–æ–π –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Qwen2.5-7B: {e}")
            raise
    
    def _initialize_sbert(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SBERT - CONTEXT SWITCHING"""
        try:
            # üöÄ CONTEXT SWITCHING: –ù–ï –ó–ê–ì–†–£–ñ–ê–ï–ú SBERT –°–†–ê–ó–£!
            self.sbert_model = None
            self.sbert_model_name = self.config.sbert_model
            self.sbert_device = self.config.sbert_device
            self.logger.info("üöÄ CONTEXT SWITCHING: SBERT will be loaded on-demand")
            self.logger.info("üöÄ CONTEXT SWITCHING: This prevents VRAM overflow and disk thrashing!")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ SBERT context switching: {e}")
            self.sbert_model = None
    
    def _load_sbert_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç SBERT –≤ VRAM —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
        if self.sbert_model is None:
            # üöÄ CONTEXT SWITCHING: SBERT —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è enterprise_rag_trainer_full.py
            self.logger.info("üöÄ CONTEXT SWITCHING: SBERT loading delegated to enterprise_rag_trainer_full.py")
            self.logger.info("üöÄ CONTEXT SWITCHING: This prevents double loading and memory conflicts!")
            return None
        return self.sbert_model

    def _unload_sbert_model(self):
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç SBERT –∏–∑ VRAM, –æ—á–∏—â–∞—è –∫—ç—à."""
        # üöÄ CONTEXT SWITCHING: SBERT —É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è enterprise_rag_trainer_full.py
        self.logger.info("üöÄ CONTEXT SWITCHING: SBERT unloading delegated to enterprise_rag_trainer_full.py")
        self.logger.info("üöÄ CONTEXT SWITCHING: This prevents double unloading and memory conflicts!")

    # def _initialize_rut5(self):  # –£–î–ê–õ–ï–ù –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    #     """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RuT5"""
    #     pass
    
    def _get_model_device(self, model):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
        try:
            return next(model.parameters()).device
        except StopIteration:
            return torch.device('cpu')
    
    def _move_inputs_to_model_device(self, inputs, model, model_name="Model"):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏"""
        device = self._get_model_device(model)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        self.logger.debug(f"üîß {model_name} device: {device}, inputs device: {inputs['input_ids'].device}")
        return inputs
    
    def classify_document(self, content: str) -> Dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å DeepSeek-Coder-6.7B (GGUF)"""
        try:
            if self.mistral_model is None:
                return {'classification_available': False, 'error': 'Qwen model not loaded'}
            
            prompt = f"""CLASSIFIER EXPERT. Analyze text and identify document type (gost, sp, fz, pprf, etc.).
Constraint: Return ONLY valid JSON. Confidence must be 0.9 if type is clearly visible.

üî•üî•üî• ULTRA CRITICAL RULE üî•üî•üî•
IF DOCUMENT STARTS WITH "–°–ü" + NUMBERS (like –°–ü158.13330.2014) ‚Üí TYPE IS "sp" ‚Üí CONFIDENCE 0.99
NEVER use "sto", "gost", or any other type if "–°–ü" is present!
üî•üî•üî• END ULTRA CRITICAL RULE üî•üî•üî•

DOCUMENT:
{content[:2048]}

üî•üî•üî• ULTRA CRITICAL RULE üî•üî•üî•
IF DOCUMENT STARTS WITH "–°–ü" + NUMBERS (like –°–ü158.13330.2014) ‚Üí TYPE IS "sp" ‚Üí CONFIDENCE 0.99
NEVER use "sto", "gost", or any other type if "–°–ü" is present!
üî•üî•üî• END ULTRA CRITICAL RULE üî•üî•üî•

Types: sp, —Å–Ω, –≥–æ—Å—Ç, —Å–∞–Ω–ø–∏–Ω, —Ñ–∑, –ø–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ, –ø—Ä–∏–∫–∞–∑, –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è

üî•üî•üî• REQUIRED JSON (MUST END WITH CLOSING BRACE) üî•üî•üî•
{{
  "document_type": "sp",
  "confidence": 0.99,
  "subtype": "sp"
}}
üî•üî•üî• YOUR JSON OUTPUT MUST END WITH '}}' üî•üî•üî•"""
            
            # üî¨ –°–£–ü–ï–†-DEBUG –î–õ–Ø LLM
            import time
            import logging
            logger = logging.getLogger(__name__)
            
            logger.info(f"[DEBUG_LLM] Prompt start (2048 chars): {prompt[:200]}...")
            logger.info(f"[DEBUG_LLM] About to call DeepSeek-Coder...")
            logger.info(f"[DEBUG_LLM] Mistral model available: {self.mistral_model is not None}")
            start_time = time.time()
            
            # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –û–°–¢–ê–ù–û–í–ö–ê –ü–û –í–†–ï–ú–ï–ù–ò (–º–∞–∫—Å–∏–º—É–º 30 —Å–µ–∫—É–Ω–¥)
            import threading
            import time as time_module
            
            response = None
            timeout_occurred = False
            
            def timeout_handler():
                nonlocal timeout_occurred
                time_module.sleep(30)  # 30 —Å–µ–∫—É–Ω–¥ –º–∞–∫—Å–∏–º—É–º
                if response is None:
                    timeout_occurred = True
            
            timeout_thread = threading.Thread(target=timeout_handler)
            timeout_thread.daemon = True
            timeout_thread.start()
            
            try:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Qwen2.5
                qwen_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
                
                # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                inputs = self.mistral_tokenizer(
                    qwen_prompt,
                    return_tensors="pt",
                    max_length=32768,
                    truncation=True,
                    padding=True
                ).to(self.mistral_model.device)
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å GPU
                with torch.no_grad():
                    outputs = self.mistral_model.generate(
                        **inputs,
                        max_new_tokens=80,  # –£–õ–¨–¢–†–ê-–ñ–ï–°–¢–ö–û –û–ì–†–ê–ù–ò–ß–ò–í–ê–ï–ú –¥–ª—è JSON
                        temperature=0.0,  # –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –î–ï–¢–ï–†–ú–ò–ù–ò–†–û–í–ê–ù–ù–û!
                        do_sample=False,
                        pad_token_id=self.mistral_tokenizer.eos_token_id
                    )
                
                # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
                generated_text = self.mistral_tokenizer.decode(
                    outputs[0][inputs['input_ids'].shape[1]:], 
                    skip_special_tokens=True
                ).strip()
                
            except Exception as e:
                if timeout_occurred:
                    logger.warning(f"[DEBUG_LLM] –¢–ê–ô–ú–ê–£–¢! Mistral –∑–∞–≤–∏—Å –Ω–∞ 30 —Å–µ–∫—É–Ω–¥: {e}")
                    return {'document_type': 'unknown', 'confidence': 0.0}
                else:
                    raise e
            
            end_time = time.time()
            
            logger.info(f"[DEBUG_LLM] Raw Response Time: {end_time - start_time:.2f}s")
            logger.info(f"[DEBUG_LLM] Raw LLM Output: {generated_text}")
            
            # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –ù–ê –ü–£–°–¢–û–ô –û–¢–í–ï–¢
            if not generated_text or len(generated_text.strip()) < 10:
                logger.warning(f"[DEBUG_LLM] LLM –Ω–µ –≤–µ—Ä–Ω—É–ª JSON! –°–æ–∑–¥–∞–µ–º fallback –¥–ª—è:")
                # Fallback –∫ regex/sbert —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
                return {'document_type': 'unknown', 'confidence': 0.0}
            
            # –ü–∞—Ä—Å–∏–º JSON —Å –°–£–ü–ï–†-–ê–ì–†–ï–°–°–ò–í–ù–û–ô –æ—á–∏—Å—Ç–∫–æ–π –æ—Ç Markdown
            try:
                # –°–£–ü–ï–†-–ê–ì–†–ï–°–°–ò–í–ù–ê–Ø –æ—á–∏—Å—Ç–∫–∞ –æ—Ç Markdown –∏ –º—É—Å–æ—Ä–∞
                cleaned_text = generated_text.strip()
                cleaned_text = cleaned_text.replace('```json', '').replace('```', '')
                cleaned_text = cleaned_text.replace('```', '').replace('`', '')
                cleaned_text = cleaned_text.replace('\n', ' ').replace('\r', ' ')
                cleaned_text = cleaned_text.strip()
                
                # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ï—Å–ª–∏ –Ω–µ JSON, —Å–æ–∑–¥–∞–µ–º fallback
                if not cleaned_text.startswith('{'):
                    logger.warning(f"[DEBUG_LLM] LLM –Ω–µ –≤–µ—Ä–Ω—É–ª JSON! –°–æ–∑–¥–∞–µ–º fallback –¥–ª—è: {cleaned_text[:100]}")
                    # Fallback: –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
                    if "–°–ü" in content[:1000] or "—Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª" in content[:1000].lower():
                        cleaned_text = '{"document_type": "sp", "confidence": 0.9, "subtype": "sp"}'
                    elif "–°–ù–∏–ü" in content[:1000] or "—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã" in content[:1000].lower():
                        cleaned_text = '{"document_type": "—Å–Ω", "confidence": 0.9, "subtype": "—Å–Ω"}'
                    else:
                        cleaned_text = '{"document_type": "unknown", "confidence": 0.0, "subtype": ""}'
                
                json_start = cleaned_text.find('{')
                json_end = cleaned_text.rfind('}') + 1
                
                if json_start != -1 and json_end > json_start:
                    json_text = cleaned_text[json_start:json_end]
                    logger.info(f"[DEBUG_LLM] Extracted JSON: {json_text}")
                    result = json.loads(json_text)
                    logger.info(f"[DEBUG_LLM] Successfully Parsed JSON: {result}")
                    return {
                        'document_type': result.get('document_type', 'unknown'),
                        'confidence': result.get('confidence', 0.0),
                        'subtype': result.get('subtype', ''),
                        'model': 'DeepSeek-Coder-6.7B-GGUF'
                    }
                else:
                    return {"document_type": "unknown", "confidence": 0.0}
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå [DEBUG_LLM] FATAL JSON DECODE: {e}")
                logger.error(f"‚ùå [DEBUG_LLM] Failed String Attempt: {cleaned_text[:500]}...")
                return {"document_type": "unknown", "confidence": 0.0}
                
        except Exception as e:
            self.logger.error(f"Classification error: {e}")
            return {"document_type": "unknown", "confidence": 0.0}
    
    def extract_metadata(self, content: str, structural_data: Dict) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å Mistral-7B GPU (32K –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∞–±–∑–∞—Ü–µ–≤)"""
        try:
            if self.mistral_model is None or self.mistral_tokenizer is None:
                return {'extraction_available': False, 'error': 'Mistral model not loaded'}
            
            metadata = {}
            
            # –°–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
            qa_questions = [
                ("–î–∞—Ç–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞", "date_approval"),
                ("–ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞", "document_number"),
                ("–ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏", "organization"),
                ("–î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è –≤ –¥–µ–π—Å—Ç–≤–∏–µ", "date_effective"),
                ("–û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è", "scope"),
                ("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞", "keywords")
            ]
            
            for question, field in qa_questions:
                try:
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Qwen2.5
                    qa_prompt = f"""<|im_start|>user
–ù–∞–π–¥–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}

–î–û–ö–£–ú–ï–ù–¢:
{content[:16384]}  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–æ 16K —Å–∏–º–≤–æ–ª–æ–≤

–û—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞):<|im_end|>
<|im_start|>assistant
"""
                    
                    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                    inputs = self.mistral_tokenizer(
                        qa_prompt,
                        return_tensors="pt",
                        max_length=32768,  # 32K –∫–æ–Ω—Ç–µ–∫—Å—Ç!
                        truncation=True,
                        padding=True
                    ).to(self.mistral_model.device)
                    
                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å GPU
                    with torch.no_grad():
                        outputs = self.mistral_model.generate(
                            **inputs,
                            max_new_tokens=100,
                            temperature=0.0,
                            do_sample=False,
                            pad_token_id=self.mistral_tokenizer.eos_token_id
                        )
                    
                    # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
                    answer = self.mistral_tokenizer.decode(
                        outputs[0][inputs['input_ids'].shape[1]:], 
                        skip_special_tokens=True
                    ).strip()
                    
                    if answer and len(answer) > 2:
                        metadata[field] = answer
                    else:
                        metadata[field] = "–ù–µ –Ω–∞–π–¥–µ–Ω–æ"
                        
                except Exception as e:
                    self.logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø–æ–ª—è {field}: {e}")
                    metadata[field] = "–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è"
            
            return {
                'extraction_available': True,
                'metadata': metadata,
                'model': 'Qwen2.5-7B-Instruct-GPU',
                'fields_extracted': len(metadata)
            }
            
        except Exception as e:
            self.logger.error(f"Metadata extraction failed: {e}")
            return {'extraction_available': False, 'error': str(e)}
    
    def get_embeddings(self, texts: list) -> list:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å SBERT"""
        try:
            if self.sbert_model is None:
                return []
            
            embeddings = self.sbert_model.encode(texts)
            return embeddings.tolist()
            
        except Exception as e:
            self.logger.error(f"Embeddings error: {e}")
            return []
    
    def get_ensemble_info(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–Ω—Å–∞–º–±–ª–µ"""
        return {
            'mistral_loaded': self.mistral_model is not None,
            'sbert_loaded': self.sbert_model is not None,
            'rut5_loaded': self.rut5_model is not None,
            'mistral_type': 'GPU-4bit',
            'sbert_device': str(self.sbert_model.device) if self.sbert_model else 'unknown',
            'rut5_device': str(self.rut5_model.device) if self.rut5_model else 'unknown',
            'strategy': 'Hybrid: Qwen2.5-7B-Instruct(GPU-4bit) + SBERT(PyTorch) + RuT5(PyTorch)',
            'total_vram_usage': '~6.0GB (fits in 8GB RTX 4060) + SPEED!'
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    config = HybridLLMConfig.from_env()
    ensemble = HybridLLMEnsemble(config)
    
    print("–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω—Å–∞–º–±–ª–µ:")
    print(json.dumps(ensemble.get_ensemble_info(), indent=2, ensure_ascii=False))
