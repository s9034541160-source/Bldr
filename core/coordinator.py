import json
import base64
import threading
from typing import Dict, Any, List, Optional
from core.model_manager import ModelManager

class Coordinator:
    def __init__(self, model_manager: ModelManager, tools_system: Any, rag_system: Any):
        """
        Coordinator - —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç —Å–∏—Å—Ç–µ–º—ã SuperBuilder.
        
        Args:
            model_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–æ–ª—è–º–∏
            tools_system: –°–∏—Å—Ç–µ–º–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
            rag_system: RAG —Å–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        """
        self.model_manager = model_manager
        self.tools_system = tools_system
        self.rag_system = rag_system
        
        # –ò—Å—Ç–æ—Ä–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        self.history = []
        self.history_lock = threading.Lock()
        self.max_history_length = 100
        
        # –ü–æ–ª—É—á–∏—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞
        self.coordinator_client = self.model_manager.get_model_client("coordinator")
    
    def _add_to_history(self, entry: Dict[str, Any]):
        """–î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø–∏—Å—å –≤ –∏—Å—Ç–æ—Ä–∏—é —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã"""
        with self.history_lock:
            self.history.append(entry)
            # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–ª–∏–Ω—É –∏—Å—Ç–æ—Ä–∏–∏
            if len(self.history) > self.max_history_length:
                self.history = self.history[-self.max_history_length:]
    
    def analyze_request_complexity(self, user_input: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.
        
        Args:
            user_input: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–ø—Ä–æ—Å–∞
        """
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        text_lower = user_input.lower()
        
        if any(keyword in text_lower for keyword in ["–Ω–æ—Ä–º–∞", "—Å–ø ", "–≥–æ—Å—Ç", "—Å–Ω–∏–ø", "—Ñ–∑", "–ø—É–Ω–∫—Ç", "cl.", "cl ", "cl:"]):
            return "norms"
        elif any(keyword in text_lower for keyword in ["–ø–ø—Ä", "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞"]):
            return "ppr"
        elif any(keyword in text_lower for keyword in ["—Å–º–µ—Ç–∞", "—Ä–∞—Å—á–µ—Ç", "–±—é–¥–∂–µ—Ç", "–≥—ç—Å–Ω", "—Ñ–µ—Ä", "—Å—Ç–æ–∏–º–æ—Å—Ç—å"]):
            return "estimate"
        elif any(keyword in text_lower for keyword in ["—Ä–∞–±–æ—á–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "—á–µ—Ä—Ç–µ–∂", "—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è", "dwg", "dxf", "ifc"]):
            return "rd"
        elif any(keyword in text_lower for keyword in ["—É—á–µ–±–Ω–∏–∫", "—É—á–µ–±–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã", "–æ–±—É—á–µ–Ω–∏–µ", "–∫—É—Ä—Å"]):
            return "educational"
        elif any(keyword in text_lower for keyword in ["–ø—Ä–æ–µ–∫—Ç", "—Ä–∞–±–æ—Ç—ã", "–≥—Ä–∞—Ñ–∏–∫", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"]):
            return "project"
        else:
            return "general"
    
    def analyze_request(self, user_input: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ JSON-–ø–ª–∞–Ω–∞ –¥–µ–π—Å—Ç–≤–∏–π.
        
        Args:
            user_input: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            JSON-–ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
        """
        # If message contains transcription wrapper, extract the actual text
        normalized_input = user_input
        try:
            marker = "–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:"
            if isinstance(user_input, str) and marker in user_input:
                after = user_input.split(marker, 1)[1].strip()
                # Trim surrounding quotes if any
                if after.startswith("'") or after.startswith('"'):
                    after = after[1:]
                if after.endswith("'") or after.endswith('"'):
                    after = after[:-1]
                if len(after) >= 3:
                    normalized_input = after
        except Exception:
            normalized_input = user_input

        print(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: {normalized_input}")
        
        # –î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –≤ –∏—Å—Ç–æ—Ä–∏—é
        self._add_to_history({
            "type": "user_request",
            "content": normalized_input,
            "timestamp": __import__('time').time()
        })
        
        # –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö —É–ø—Ä–æ—â—ë–Ω–Ω—ã—Ö –ø—Ä–∞–≤–∏–ª –Ω–µ—Ç: –¥–∞–ª—å–Ω–µ–π—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ ‚Äî –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ –º–æ–¥–µ–ª–∏/–ø–ª–∞–Ω–∞

        # Parse request using SBERT for better accuracy
        try:
            from core.parse_utils import parse_request_with_sbert
            parse_result = parse_request_with_sbert(normalized_input)
            print(f"SBERT Parse Result: {parse_result}")
        except Exception as e:
            print(f"Error in SBERT parsing: {e}")
            parse_result = {"intent": "unknown", "confidence": 0.0, "entities": {}}
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        plan = self._generate_plan(normalized_input, parse_result)
        
        return plan
    
    def _generate_plan(self, user_input: str, parse_result: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –¥–µ–π—Å—Ç–≤–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞.
        
        Args:
            user_input: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            parse_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –ø–æ–º–æ—â—å—é SBERT
            
        Returns:
            JSON-–ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
        """
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–∞ –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø–ª–∞–Ω–∞
        try:
            # –ü–æ–ª—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞
            coordinator_model = self.model_manager.get_model_client("coordinator")
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
            tools_description = """
–î–û–°–¢–£–ü–ù–´–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´:
- search_knowledge_base: –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
- calculate_estimate: –†–∞—Å—á–µ—Ç —Å–º–µ—Ç—ã
- find_normatives: –ü–æ–∏—Å–∫ –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤
- extract_works_nlp: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–±–æ—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞
- generate_mermaid_diagram: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–∞–≥—Ä–∞–º–º
- create_gantt_chart: –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º –ì–∞–Ω—Ç–∞
- create_pie_chart: –°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä—É–≥–æ–≤—ã—Ö –¥–∏–∞–≥—Ä–∞–º–º
- create_bar_chart: –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–æ–ª–±—á–∞—Ç—ã—Ö –¥–∏–∞–≥—Ä–∞–º–º
- get_work_sequence: –ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç
- extract_construction_data: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- create_construction_schedule: –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
- calculate_critical_path: –†–∞—Å—á–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—É—Ç–∏
- extract_financial_data: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- analyze_image: –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
- generate_letter: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–∏—Å–µ–º
- auto_budget: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç –±—é–¥–∂–µ—Ç–∞
- generate_ppr: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ü–ü–†
- create_gpp: –°–æ–∑–¥–∞–Ω–∏–µ –ì–ü–ü
- parse_gesn_estimate: –ü–∞—Ä—Å–∏–Ω–≥ —Å–º–µ—Ç –ø–æ –ì–≠–°–ù
- analyze_tender: –ê–Ω–∞–ª–∏–∑ —Ç–µ–Ω–¥–µ—Ä–æ–≤
- search_rag_database: –ü–æ–∏—Å–∫ –≤ RAG –±–∞–∑–µ
- check_normative: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤
- create_document: –°–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- generate_construction_schedule: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞
- calculate_financial_metrics: –†–∞—Å—á–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫
- extract_text_from_pdf: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF
- analyze_bentley_model: –ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–µ–π Bentley
- autocad_export: –≠–∫—Å–ø–æ—Ä—Ç –≤ AutoCAD
- monte_carlo_sim: –°–∏–º—É–ª—è—Ü–∏—è Monte Carlo
- semantic_parse: SBERT NLU –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π
"""

            roles_list = """
–î–û–°–¢–£–ü–ù–´–ï –†–û–õ–ò:
- coordinator: –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä (—Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ)
- analyst: –ê–Ω–∞–ª–∏—Ç–∏–∫ (–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞—Å—á–µ—Ç—ã)
- chief_engineer: –ì–ª–∞–≤–Ω—ã–π –∏–Ω–∂–µ–Ω–µ—Ä (—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è)
- project_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞ (—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞–º–∏)
- construction_worker: –°—Ç—Ä–æ–∏—Ç–µ–ª—å (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
"""

            # Prepare input data with SBERT parsing results
            input_data = f"–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_input}"
            if parse_result:
                input_data += f"\nSBERT Parse: intent='{parse_result.get('intent', 'unknown')}', confidence={parse_result.get('confidence', 0.0)}, entities={parse_result.get('entities', {})}"
            
            analysis_prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ —Å—Ñ–æ—Ä–º–∏—Ä—É–π JSON-–ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π.

{input_data}

–í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–π —Å—Ç—Ä–æ–≥–æ JSON-–æ–±—ä–µ–∫—Ç
2. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ —Ä–æ–ª–∏
3. –£–∫–∞–∑—ã–≤–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
4. –ü–ª–∞–Ω–∏—Ä—É–π –ø–æ—à–∞–≥–æ–≤–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á
5. –£—á–∏—Ç—ã–≤–∞–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã SBERT –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ —Ä–æ–ª–µ–π

–ü—Ä–∏–º–µ—Ä –≤–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–≥–æ JSON:
{{
    "status": "planning",
    "query_type": "—Ç–∏–ø_–∑–∞–ø—Ä–æ—Å–∞",
    "requires_tools": true,
    "tools": [
        {{
            "name": "–∏–º—è_–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞",
            "arguments": {{
                "–ø–∞—Ä–∞–º–µ—Ç—Ä1": "–∑–Ω–∞—á–µ–Ω–∏–µ1",
                "–ø–∞—Ä–∞–º–µ—Ç—Ä2": "–∑–Ω–∞—á–µ–Ω–∏–µ2"
            }}
        }}
    ],
    "roles_involved": ["—Ä–æ–ª—å1", "—Ä–æ–ª—å2"],
    "required_data": ["–¥–∞–Ω–Ω—ã–µ1", "–¥–∞–Ω–Ω—ã–µ2"],
    "next_steps": ["—à–∞–≥1", "—à–∞–≥2"]
}}
"""

            # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä—É
            messages = [
                {
                    "role": "system",
                    "content": analysis_prompt + tools_description + roles_list
                },
                {
                    "role": "user",
                    "content": user_input
                }
            ]
            
            response = self.model_manager.query("coordinator", messages)
            
            # –ü–æ–ø—ã—Ç–∞—Ç—å—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
            try:
                # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    plan_json = json_match.group()
                    plan = json.loads(plan_json)
                    return plan
                else:
                    # –ï—Å–ª–∏ JSON –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
                    raise ValueError("JSON not found in response")
            except (json.JSONDecodeError, ValueError):
                # –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                print("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥")
                pass
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏: {e}")
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            pass
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (—Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥)
        if any(keyword in user_input.lower() for keyword in ["—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Å–º–µ—Ç–∞", "—Ä–∞—Å—á–µ—Ç", "—Ñ–∏–Ω–∞–Ω—Å", "–±—é–¥–∂–µ—Ç"]):
            # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            return {
                "status": "planning",
                "query_type": "financial",
                "requires_tools": True,
                "tools": [
                    {"name": "search_rag_database", "arguments": {"query": user_input, "doc_types": ["estimate", "smeta"]}},
                    {"name": "calculate_estimate", "arguments": {"query": user_input}}
                ],
                "roles_involved": ["analyst", "chief_engineer"],
                "required_data": ["—Ä–∞—Å—Ü–µ–Ω–∫–∏", "–º–∞—Ç–µ—Ä–∏–∞–ª—ã", "—Ä–∞–±–æ—Ç—ã"],
                "next_steps": [
                    "–ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ä–∞—Å—Ü–µ–Ω–æ–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π",
                    "–†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏ —Ä–∞–±–æ—Ç"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["–Ω–æ—Ä–º–∞", "—Å–ø ", "–≥–æ—Å—Ç", "—Å–Ω–∏–ø", "—Ñ–∑", "–ø—É–Ω–∫—Ç"]):
            # –ü—Ä–æ—Å—Ç–æ–π –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∑–∞–ø—Ä–æ—Å - —Ç–æ–ª—å–∫–æ –ø–æ–∏—Å–∫ –∏ –æ—Ç–≤–µ—Ç
            return {
                "status": "planning",
                "query_type": "normative_simple",
                "requires_tools": True,
                "tools": [
                    {"name": "search_rag_database", "arguments": {"query": user_input, "doc_types": ["norms"], "n_results": 3}}
                ],
                "roles_involved": ["coordinator"],
                "required_data": ["–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"],
                "next_steps": [
                    "–ü–æ–∏—Å–∫ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
                    "–û—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é"
                ],
                "requires_specialists": False  # –ú–æ–¥–µ–ª—å —Å–∞–º–∞ —Ä–µ—à–∞–µ—Ç
            }
        elif any(keyword in user_input.lower() for keyword in ["–ø—Ä–æ–µ–∫—Ç", "–ø–ø—Ä", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞", "—Ä–∞–±–æ—Ç—ã", "–≥—Ä–∞—Ñ–∏–∫"]):
            # –ü—Ä–æ–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            return {
                "status": "planning",
                "query_type": "project",
                "requires_tools": True,
                "tools": [
                    {"name": "search_rag_database", "arguments": {"query": user_input, "doc_types": ["ppr", "rd"]}},
                    {"name": "extract_works_nlp", "arguments": {"text": user_input, "doc_type": "ppr"}},
                    {"name": "create_gantt_chart", "arguments": {"tasks": [], "title": "–ì—Ä–∞—Ñ–∏–∫ —Ä–∞–±–æ—Ç"}}
                ],
                "roles_involved": ["project_manager", "construction_worker"],
                "required_data": ["–ø—Ä–æ–µ–∫—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–∞—Ä—Ç—ã"],
                "next_steps": [
                    "–ü–æ–∏—Å–∫ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏",
                    "–ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç",
                    "–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–±–æ—Ç"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["–¥–∏–∞–≥—Ä–∞–º–º–∞", "–≥—Ä–∞—Ñ–∏–∫", "chart", "diagram"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º—ã
            return {
                "status": "planning",
                "query_type": "visualization",
                "requires_tools": True,
                "tools": [
                    {"name": "generate_mermaid_diagram", "arguments": {"type": "flow", "data": {}}},
                    {"name": "create_pie_chart", "arguments": {"data": [], "title": "–î–∏–∞–≥—Ä–∞–º–º–∞"}}
                ],
                "roles_involved": ["analyst", "project_manager"],
                "required_data": ["–¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"],
                "next_steps": [
                    "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–∞–≥—Ä–∞–º–º—ã",
                    "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["–ø–∏—Å—å–º–æ", "–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–µ", "–¥–µ–ª–æ–≤–æ–µ"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞
            return {
                "status": "planning",
                "query_type": "official_letter",
                "requires_tools": True,
                "tools": [
                    {"name": "generate_letter", "arguments": {"template": "compliance_sp31", "data": {}}}
                ],
                "roles_involved": ["chief_engineer"],
                "required_data": ["—à–∞–±–ª–æ–Ω –ø–∏—Å—å–º–∞", "–¥–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–∞—Ç–µ–ª—è"],
                "next_steps": [
                    "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–∏—Å—å–º–∞",
                    "–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–∏—Å—å–º–∞",
                    "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ –ø–∏—Å—å–º–∞"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["–±—é–¥–∂–µ—Ç", "–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π", "—Å–º–µ—Ç–∞", "–≥—ç—Å–Ω"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±—é–¥–∂–µ—Ç
            return {
                "status": "planning",
                "query_type": "auto_budget",
                "requires_tools": True,
                "tools": [
                    {"name": "parse_gesn_estimate", "arguments": {"estimate_file": "", "region": "ekaterinburg"}},
                    {"name": "auto_budget", "arguments": {"estimate_data": {}, "gesn_rates": {}}}
                ],
                "roles_involved": ["analyst"],
                "required_data": ["—Å–º–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ", "–ì–≠–°–ù —Ä–∞—Å—Ü–µ–Ω–∫–∏"],
                "next_steps": [
                    "–ü–∞—Ä—Å–∏–Ω–≥ —Å–º–µ—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞",
                    "–ü–æ–ª—É—á–µ–Ω–∏–µ –ì–≠–°–ù —Ä–∞—Å—Ü–µ–Ω–æ–∫",
                    "–†–∞—Å—á–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –±—é–¥–∂–µ—Ç–∞"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["–ø–ø—Ä", "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –ü–ü–†
            return {
                "status": "planning",
                "query_type": "ppr_generation",
                "requires_tools": True,
                "tools": [
                    {"name": "get_work_sequence", "arguments": {"query": "stage11_work_sequence"}},
                    {"name": "generate_ppr", "arguments": {"project_data": {}, "works_seq": []}}
                ],
                "roles_involved": ["chief_engineer"],
                "required_data": ["–¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç–∞", "–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç"],
                "next_steps": [
                    "–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç –∏–∑ stage11",
                    "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["–≥–ø–ø", "–≥—Ä–∞—Ñ–∏–∫", "–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ –ì–ü–ü
            return {
                "status": "planning",
                "query_type": "gpp_creation",
                "requires_tools": True,
                "tools": [
                    {"name": "get_work_sequence", "arguments": {"query": "stage11_work_sequence"}},
                    {"name": "create_gpp", "arguments": {"works_seq": [], "timeline": {}}}
                ],
                "roles_involved": ["project_manager"],
                "required_data": ["–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç", "–≤—Ä–µ–º–µ–Ω–Ω–∞—è —à–∫–∞–ª–∞"],
                "next_steps": [
                    "–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç –∏–∑ stage11",
                    "–°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["–∞–Ω–∞–ª–∏–∑", "—Ç–µ–Ω–¥–µ—Ä", "–ø—Ä–æ–µ–∫—Ç–∞"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞/—Ç–µ–Ω–¥–µ—Ä–∞
            return {
                "status": "planning",
                "query_type": "tender_analysis",
                "requires_tools": True,
                "tools": [
                    {"name": "search_rag_database", "arguments": {"query": user_input, "doc_types": ["norms", "ppr", "smeta"]}},
                    {"name": "analyze_tender", "arguments": {"tender_data": {}, "requirements": []}}
                ],
                "roles_involved": ["analyst", "chief_engineer", "project_manager"],
                "required_data": ["–¥–∞–Ω–Ω—ã–µ —Ç–µ–Ω–¥–µ—Ä–∞", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞"],
                "next_steps": [
                    "–°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–Ω–¥–µ—Ä–µ",
                    "–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞",
                    "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["roi", "npv", "irr", "–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å", "—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            return {
                "status": "planning",
                "query_type": "financial_metrics",
                "requires_tools": True,
                "tools": [
                    {"name": "calculate_financial_metrics", "arguments": {"type": "ROI", "profit": 300000000, "cost": 200000000}}
                ],
                "roles_involved": ["analyst"],
                "required_data": ["—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏"],
                "next_steps": [
                    "–†–∞—Å—á–µ—Ç —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫",
                    "–ê–Ω–∞–ª–∏–∑ —Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["bim", "ifc", "–∞–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏", "–∞–Ω–∞–ª–∏–∑ bim"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ –∞–Ω–∞–ª–∏–∑ BIM –º–æ–¥–µ–ª–∏
            return {
                "status": "planning",
                "query_type": "bim_analysis",
                "requires_tools": True,
                "tools": [
                    {"name": "analyze_bentley_model", "arguments": {"ifc_path": "sample.ifc", "analysis_type": "clash"}}
                ],
                "roles_involved": ["chief_engineer", "analyst"],
                "required_data": ["IFC —Ñ–∞–π–ª –º–æ–¥–µ–ª–∏", "—Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞"],
                "next_steps": [
                    "–ó–∞–≥—Ä—É–∑–∫–∞ IFC –º–æ–¥–µ–ª–∏",
                    "–ê–Ω–∞–ª–∏–∑ –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–æ–ª–ª–∏–∑–∏–∏",
                    "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –æ –Ω–∞—Ä—É—à–µ–Ω–∏—è—Ö"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["autocad", "dwg", "—ç–∫—Å–ø–æ—Ä—Ç –≤ autocad", "—ç–∫—Å–ø–æ—Ä—Ç dwg"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ —ç–∫—Å–ø–æ—Ä—Ç –≤ AutoCAD
            return {
                "status": "planning",
                "query_type": "autocad_export",
                "requires_tools": True,
                "tools": [
                    {"name": "autocad_export", "arguments": {"dwg_data": {}, "works_seq": []}}
                ],
                "roles_involved": ["chief_engineer"],
                "required_data": ["–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç", "–ø–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–æ—Ä—Ç–∞"],
                "next_steps": [
                    "–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç",
                    "–°–æ–∑–¥–∞–Ω–∏–µ DWG —Ñ–∞–π–ª–∞",
                    "–≠–∫—Å–ø–æ—Ä—Ç —á–µ—Ä—Ç–µ–∂–∞"
                ]
            }
        elif any(keyword in user_input.lower() for keyword in ["monte carlo", "–º–æ–Ω—Ç–µ-–∫–∞—Ä–ª–æ", "—Ä–∏—Å–∫", "–∞–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤", "—Å–∏–º—É–ª—è—Ü–∏—è"]):
            # –ó–∞–ø—Ä–æ—Å –Ω–∞ Monte Carlo —Å–∏–º—É–ª—è—Ü–∏—é
            return {
                "status": "planning",
                "query_type": "monte_carlo_simulation",
                "requires_tools": True,
                "tools": [
                    {"name": "monte_carlo_sim", "arguments": {"project_data": {"base_cost": 200000000, "profit": 300000000, "vars": {"cost": 0.2, "time": 0.15, "roi": 0.1}}}}
                ],
                "roles_involved": ["analyst"],
                "required_data": ["–±–∞–∑–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å", "–ø—Ä–∏–±—ã–ª—å", "–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∞—Ä–∏–∞—Ü–∏–∏"],
                "next_steps": [
                    "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏",
                    "–ó–∞–ø—É—Å–∫ Monte Carlo –∞–Ω–∞–ª–∏–∑–∞",
                    "–ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∏—Å–∫–æ–≤"
                ]
            }
        else:
            # –û–±—â–∏–π –∑–∞–ø—Ä–æ—Å
            return {
                "status": "planning",
                "query_type": "general",
                "requires_tools": True,
                "tools": [
                    {"name": "search_rag_database", "arguments": {"query": user_input, "doc_types": ["norms", "ppr", "smeta", "rd"]}}
                ],
                "roles_involved": ["coordinator"],
                "required_data": ["–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"],
                "next_steps": [
                    "–ü–æ–∏—Å–∫ –æ–±—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"
                ]
            }
    
    def execute_tools(self, plan: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–ª–∞–Ω–∞.
        
        Args:
            plan: JSON-–ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        """
        tool_results = []
        
        if "tools" in plan and plan["tools"]:
            print(f"–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ {len(plan['tools'])} –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤...")
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–∞–∂–¥—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∏–∑ –ø–ª–∞–Ω–∞
            for i, tool_info in enumerate(plan["tools"]):
                if isinstance(tool_info, dict) and "name" in tool_info:
                    tool_name = tool_info["name"]
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º "arguments" –≤–º–µ—Å—Ç–æ "params" —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –ø–ª–∞–Ω–∞
                    tool_params = tool_info.get("arguments", {})
                    print(f"  ‚Üí –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ {tool_name} —Å –∞—Ä–≥—É–º–µ–Ω—Ç–∞–º–∏: {tool_params}")
                    
                    try:
                        result = self.tools_system.execute_tool(tool_name, **tool_params)
                        print(f"  ‚Üê –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
                        
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                        if hasattr(result, 'data'):
                            result_data = result.data
                        elif hasattr(result, 'is_success') and result.is_success():
                            result_data = getattr(result, 'data', str(result))
                        else:
                            result_data = str(result)
                        
                        tool_results.append({
                            "tool_name": tool_name,
                            "result": result_data,
                            "status": "success" if hasattr(result, 'is_success') and result.is_success() else "error"
                        })
                    except Exception as e:
                        print(f"  ‚Üê –û—à–∏–±–∫–∞: {e}")
                        tool_results.append({
                            "tool_name": tool_name,
                            "result": str(e),
                            "status": "error"
                        })
        else:
            print("–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –Ω–µ —Ç—Ä–µ–±—É—é—Ç—Å—è –∏–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –ø–ª–∞–Ω–µ")
        
        return tool_results
    
    def _coordinate_with_specialists(self, plan: Dict[str, Any], tool_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è —Å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            plan: JSON-–ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
            tool_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
            
        Returns:
            –û—Ç–≤–µ—Ç—ã —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤
        """
        specialist_responses = []
        
        if "roles_involved" in plan and plan["roles_involved"]:
            for role in plan["roles_involved"]:
                if role != "coordinator":  # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç —Å–∞–º —Å–µ–±–µ
                    try:
                        # –ü–æ–ª—É—á–∏—Ç—å –∫–ª–∏–µ–Ω—Ç–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–æ–ª–∏
                        model_client = self.model_manager.get_model_client(role)
                        if model_client:
                            # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –ø—Ä–æ–º—Ç –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞
                            specialist_prompt = self._create_specialist_prompt(role, plan, tool_results)
                            
                            # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É
                            messages = [
                                {
                                    "role": "system",
                                    "content": self.model_manager.get_capabilities_prompt(role)
                                },
                                {
                                    "role": "user",
                                    "content": specialist_prompt
                                }
                            ]
                            
                            response_content = self.model_manager.query(role, messages)
                            response = {
                                "role": role,
                                "response": response_content,
                                "tool_results": tool_results
                            }
                            specialist_responses.append(response)
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ {role}: {e}")
        
        return specialist_responses
    
    def _create_specialist_prompt(self, role: str, plan: Dict[str, Any], tool_results: List[Dict[str, Any]]) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º—Ç–∞ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            role: –†–æ–ª—å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞
            plan: –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
            tool_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
            
        Returns:
            –ü—Ä–æ–º—Ç –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞
        """
        prompt = f"–í—ã - {role}. –í–∞–º –Ω—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å —Å–ª–µ–¥—É—é—â—É—é –∑–∞–¥–∞—á—É:\n\n"
        prompt += f"–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {plan.get('query_type', '–æ–±—â–∏–π –∑–∞–ø—Ä–æ—Å')}\n\n"
        
        if tool_results:
            prompt += "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤:\n"
            for result in tool_results:
                if result.get("status") == "success":
                    prompt += f"- {result.get('tool_name', '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç')}: {result.get('result', '–†–µ–∑—É–ª—å—Ç–∞—Ç')}\n"
                else:
                    prompt += f"- {result.get('tool_name', '–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç')}: –û–®–ò–ë–ö–ê - {result.get('result', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}\n"
            prompt += "\n"
        
        prompt += "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ."
        return prompt
    
    def synthesize_response(self, user_input: str, tool_results: List[Dict[str, Any]], 
                          specialist_responses: Optional[List[Dict[str, Any]]] = None) -> str:
        """
        –°–ò–ù–¢–ï–ó –§–ò–ù–ê–õ–¨–ù–û–ì–û –û–¢–í–ï–¢–ê –ù–ê –û–°–ù–û–í–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í.
        """
        print(f"–°–∏–Ω—Ç–µ–∑ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å: {user_input}")

        # 1) –ü—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º RAG, –µ—Å–ª–∏ –µ—Å—Ç—å
        try:
            for result in (tool_results or []):
                if result.get("tool_name") == "search_rag_database" and result.get("status") == "success":
                    data = result.get("result") or {}
                    if isinstance(data, dict):
                        rag_results = data.get("results", []) or (data.get("data", {}) if isinstance(data.get("data"), dict) else {}).get("results", [])
                        if rag_results:
                            first = rag_results[0]
                            content = (first.get("content") if isinstance(first, dict) else None) or first.get("chunk") or "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"
                            meta = (first.get("metadata") if isinstance(first, dict) else {}) or {}
                            source = meta.get("source") or meta.get("doc") or meta.get("title") or "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫"
                            return f"–ù–∞–π–¥–µ–Ω–æ: {content} [–ò—Å—Ç–æ—á–Ω–∏–∫: {source}]"
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–≥–æ RAG-–æ—Ç–≤–µ—Ç–∞: {e}")

        # 2) –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ/—Å–∞–º–æ–ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
        try:
            low = (user_input or '').lower()
            if any(kw in low for kw in ["–ø—Ä–∏–≤–µ—Ç", "–∫—Ç–æ —Ç—ã", "—á—Ç–æ —Ç—ã", "–∫–∞–∫ –¥–µ–ª–∞", "–ø—Ä–µ–¥—Å—Ç–∞–≤—å—Å—è"]) or ("–∫–∞–∫" in low and ("–∑–æ–≤—É—Ç" in low or "–Ω–∞–∑—ã–≤–∞—Ç—å" in low or "—Ç–µ–±—è" in low)):
                return (
                    "–ü—Ä–∏–≤–µ—Ç! –Ø ‚Äî Bldr2, –≤–∞—à AI‚Äë–∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –≤ —Å–∏—Å—Ç–µ–º–µ Bldr Empire. "
                    "–ò—â—É –Ω–æ—Ä–º—ã (–°–ü, –ì–û–°–¢, –°–ù–∏–ü), –≥–µ–Ω–µ—Ä–∏—Ä—É—é —Å–º–µ—Ç—ã –∏ –æ—Ç—á—ë—Ç—ã, —Å–æ–∑–¥–∞—é —á–µ–∫‚Äë–ª–∏—Å—Ç—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏. "
                    "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∑–∞–¥–∞—á—É ‚Äî —è —Å–æ–±–µ—Ä—É –¥–∞–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –∏ –≤–µ—Ä–Ω—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—á–∞—â–µ –≤—Å–µ–≥–æ –≤ –≤–∏–¥–µ —Ñ–∞–π–ª–∞)."
                )
        except Exception:
            pass

        # 3) –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é: –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –ø–æ–ª–µ–∑–Ω–æ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        return "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É—Ç–æ—á–Ω–∏—Ç—å."
    
    def clean_response(self, response: str) -> str:
        """
        –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –¥–µ—Ç–∞–ª–µ–π –∏ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.
        
        Args:
            response: –ò—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        # –£–¥–∞–ª–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏ –∏ –º—ã—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã
        lines = response.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –¥–µ—Ç–∞–ª—è–º–∏
            if not any(keyword in line.lower() for keyword in ["–º—ã—à–ª–µ–Ω–∏–µ", "—Ä–∞–∑–º—ã—à–ª–µ–Ω–∏–µ", "process", "thinking"]):
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def process_request(self, user_input: str) -> str:
        """
        –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: –∞–Ω–∞–ª–∏–∑ ‚Üí –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ ‚Üí —Å–∏–Ω—Ç–µ–∑ –æ—Ç–≤–µ—Ç–∞.
        
        Args:
            user_input: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            
        Returns:
            –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {user_input}")
        
        # 1. –ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞
        plan = self.analyze_request(user_input)
        print(f"–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω –ø–ª–∞–Ω: {json.dumps(plan, ensure_ascii=False, indent=2)}")
        
        # –ï–¥–∏–Ω—ã–π –ø—É—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ - –¥–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª–∏ —Å–∞–º–æ–π —Ä–µ—à–∞—Ç—å

        # 2. –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        tool_results = self.execute_tools(plan)
        print(f"–í—ã–ø–æ–ª–Ω–µ–Ω—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: {len(tool_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        # 3. –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è —Å —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞–º–∏ (–µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è)
        specialist_responses = []
        if plan.get("requires_specialists", True):  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏–≤–ª–µ–∫–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤
            specialist_responses = self._coordinate_with_specialists(plan, tool_results)
            print(f"–ü–æ–ª—É—á–µ–Ω—ã –æ—Ç–≤–µ—Ç—ã —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤: {len(specialist_responses)} –æ—Ç–≤–µ—Ç–æ–≤")
        else:
            print("–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –ø–æ —Ä–µ—à–µ–Ω–∏—é –º–æ–¥–µ–ª–∏")
        
        # 4. –°–∏–Ω—Ç–µ–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        final_response = self.synthesize_response(user_input, tool_results, specialist_responses)
        
        # 5. –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
        cleaned_response = self.clean_response(final_response)
        
        # 6. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ (–∫—Ä–æ–º–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–∞)
        try:
            self.model_manager.force_cleanup()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ø–∞–º—è—Ç–∏: {e}")
        
        return cleaned_response

    def process_photo(self, image_input: str) -> str:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Qwen2.5-vl-7b –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
        
        Args:
            base64_image: –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ base64
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            import os
            import base64
            import tempfile

            # Determine if input is a base64 string or file path
            is_existing_path = isinstance(image_input, str) and os.path.exists(image_input)
            temp_path = image_input
            base64_str = None

            if not is_existing_path:
                # Assume base64 string without data URL header
                base64_str = image_input
                # Create temp file for tool processing
                fd, tmpfile = tempfile.mkstemp(suffix=".jpg")
                os.close(fd)
                with open(tmpfile, "wb") as f:
                    try:
                        f.write(base64.b64decode(base64_str))
                    except Exception:
                        # If header present like data:image/jpeg;base64,
                        b64 = base64_str.split(",", 1)[-1]
                        f.write(base64.b64decode(b64))
                temp_path = tmpfile
            else:
                # If we only have a path, try to read and create base64 for VL model
                try:
                    with open(image_input, "rb") as f:
                        base64_str = base64.b64encode(f.read()).decode("utf-8")
                except Exception:
                    base64_str = None

            # Decide VL capability
            try:
                from core.config import MODELS_CONFIG
                coord_model = MODELS_CONFIG.get("coordinator", {}).get("model", "")
                is_vl = isinstance(coord_model, str) and ("-vl-" in coord_model.lower() or "vl" in coord_model.lower())
            except Exception:
                is_vl = False

            # 1) Run tool analysis on file path
            tool_kwargs = {
                "image_path": temp_path,
                "analysis_type": "objects" if is_vl else "basic",
                "ocr_lang": "rus",
                "detect_objects": True,
                "extract_dimensions": True if is_vl else False,
            }
            tool_result = None
            if getattr(self, "tools_system", None):
                tool_result = self.tools_system.execute_tool("analyze_image", **tool_kwargs)

            # 2) Run VL model analysis with base64 data URL if available
            vl_summary = None
            if is_vl and base64_str:
                try:
                    # LM Studio OpenAI-compatible often expects plain strings; pass text + image URL as string
                    prompt_text = (
                        "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ–æ—Ç–æ –∏ –∫—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏, —á—Ç–æ –Ω–∞ –Ω—ë–º, –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã/–æ–±—ä–µ–∫—Ç—ã.\n"
                        "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: data:image/jpeg;base64," + base64_str
                    )
                    vl_resp = self.model_manager.query("coordinator", [
                        {"role": "user", "content": prompt_text}
                    ])
                    # Clamp overly short/bad replies
                    vl_summary = str(vl_resp).strip()
                except Exception as vl_e:
                    vl_summary = f"VL-–∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {vl_e}"

            # Build combined response
            parts = []
            if vl_summary:
                parts.append(f"[–ö–û–û–†–î–ò–ù–ê–¢–û–†-VL] {vl_summary}")

            if tool_result is not None:
                if hasattr(tool_result, "is_success") and tool_result.is_success():
                    data = getattr(tool_result, "data", {}) or {}
                    if isinstance(data, dict):
                        brief = data.get("summary") or data.get("result") or str(data)[:500]
                    else:
                        brief = str(data)
                    parts.append(f"[–ò–ù–°–¢–†–£–ú–ï–ù–¢ analyze_image] {brief}")
                else:
                    err = getattr(tool_result, "error", None)
                    parts.append(f"[–ò–ù–°–¢–†–£–ú–ï–ù–¢ analyze_image] –û—à–∏–±–∫–∞: {err or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'}")

            # Cleanup temp file if we created it
            try:
                if not is_existing_path and temp_path and os.path.exists(temp_path):
                    os.remove(temp_path)
            except Exception:
                pass

            if parts:
                return "\n\n".join(parts)
            return "–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω, –Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–µ—Ç"

        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏: {str(e)}"
    
    def process_document_search(self, query: str) -> str:
        """
        –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ RAG —Å–∏—Å—Ç–µ–º–µ.
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ–∏—Å–∫–∞ –≤ RAG
            result = self.tools_system.execute_tool(
                "search_rag_database",
                query=query,
                doc_types=["norms", "ppr", "smeta", "rd", "educational"]
            )
            
            if hasattr(result, "is_success") and result.is_success():
                data = getattr(result, "data", {}) or {}
                results = data.get("results", []) if isinstance(data, dict) else []
                if results:
                    response = "–ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:\n"
                    for i, doc in enumerate(results[:5], 1):  # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å 5 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                        chunk = doc.get("chunk", "")[:200] + "..." if len(doc.get("chunk", "")) > 200 else doc.get("chunk", "")
                        response += f"{i}. {chunk}\n"
                    return response
                else:
                    return "–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            else:
                err = getattr(result, "error", None)
                return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {err or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'}"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}"
    
    def create_and_send_file(self, file_type: str, content: Dict[str, Any]) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∞–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π docx/openpyxl –∏ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –≤ Telegram.
        
        Args:
            file_type: –¢–∏–ø —Ñ–∞–π–ª–∞ (docx, xlsx)
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ—Ç–ø—Ä–∞–≤–∫–∏
        """
        try:
            if file_type == "docx":
                # –°–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
                result = self.tools_system.execute_tool("create_document", **content)
                if hasattr(result, "is_success") and result.is_success():
                    data = getattr(result, "data", {}) or {}
                    file_path = data.get("file_path", "") if isinstance(data, dict) else str(data)
                    return f"–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: {file_path}"
                else:
                    err = getattr(result, "error", None)
                    return f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {err or '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'}"
            else:
                return f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_type}"
        except Exception as e:
            return f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞: {str(e)}"
    
    def _handle_voice_request(self, audio_data: bytes) -> str:
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç—É–ª–∑—ã —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ (Whisper)
        –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä–æ–º.

        Args:
            audio_data: –±–∞–π—Ç—ã –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞ (ogg/opus –∏ –¥—Ä.)
        Returns:
            –ß–µ–ª–æ–≤–µ–∫–æ-—á–∏—Ç–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ–º –∏ –∞–Ω–∞–ª–∏–∑–æ–º
        """
        import tempfile, os
        temp_audio_path = None
        try:
            # Save to temp file
            fd, temp_audio_path = tempfile.mkstemp(suffix='.ogg')
            os.close(fd)
            with open(temp_audio_path, 'wb') as f:
                f.write(audio_data)

            # Use tools system to transcribe
            if not getattr(self, 'tools_system', None):
                return "üé§ –ü–æ–ª—É—á–∏–ª –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –Ω–æ —Å–∏—Å—Ç–µ–º–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏."

            result = self.tools_system.execute_tool("transcribe_audio", audio_path=temp_audio_path, language="ru")
            status = result.get("status")
            if status != "success":
                err = result.get("error")
                return f"üé§ –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å: {err or '–æ—à–∏–±–∫–∞ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏'}"

            transcription = (result.get("text") or "").strip()
            if not transcription:
                return "üé§ –ü–æ–ª—É—á–∏–ª –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ç–µ–∫—Å—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–ø–∏—Å–∞—Ç—å —Å–Ω–æ–≤–∞."

            # Analyze transcription with coordinator
            enhanced_prompt = (
                "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: '"
                + transcription + "'. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–ø—Ä–æ—Å –∏ –¥–∞–π –æ—Ç–≤–µ—Ç."
            )
            analysis = self.process_request(enhanced_prompt)
            return f"üé§ –ì–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ\n\n–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è: \"{transcription}\"\n\n{analysis}"
        except Exception as e:
            return f"üé§ –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}"
        finally:
            try:
                if temp_audio_path and os.path.exists(temp_audio_path):
                    os.remove(temp_audio_path)
            except Exception:
                pass
    
    def _format_plan_response(self, plan: Dict[str, Any]) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ –≤ —Å—Ç—Ä–æ–≥–∏–π JSON –±–µ–∑ —Ç–µ–∫—Å—Ç–∞.
        
        Args:
            plan: JSON –ø–ª–∞–Ω
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞
        """
        # –í–µ—Ä–Ω—É—Ç—å —Å—Ç—Ä–æ–≥–∏–π JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        return json.dumps(plan, ensure_ascii=False, indent=2)