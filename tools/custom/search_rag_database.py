# namespace:core_rag
from typing import Any, Dict, List
import time
import re
import os
from core.tools.base_tool import ToolManifest, ToolInterface, ToolParam, ToolParamType

# –ö–∞—Å—Ç–æ–º–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–ª—è –ª—É—á—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫
class ToolValidationError(Exception):
    """–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
    pass

class ToolDependencyError(Exception):
    """–û—à–∏–±–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ (RAG, –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö)"""
    pass

class ToolProcessingError(Exception):
    """–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    pass

# –°–æ–∑–¥–∞–µ–º Pydantic –º–æ–¥–µ–ª–∏ –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
coordinator_interface = ToolInterface(
    purpose="–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏ —Å–≤—è–∑–µ–π",
    input_requirements={},  # –ë—É–¥–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –∏–∑ manifest.params
    execution_flow=[
        "–ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç —Ç–µ–º–∞—Ç–∏–∫–∏ –∏ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
        "–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π", 
        "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏"
    ],
    output_format={
        "structure": {
            "status": "success/error",
            "data": {
                "results": "array - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞",
                "total_found": "number - –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
                "query": "string - –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å",
                "search_stats": "object - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∏—Å–∫–∞"
            },
            "execution_time": "float in seconds"
        },
        "result_fields": {
            "rank": "integer - –ø–æ–∑–∏—Ü–∏—è –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö",
            "score": "string - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å (0.000-1.000)",
            "title": "string - –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
            "content": "string - —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞",
            "source": "string - –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É",
            "doc_type": "string - —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞",
            "metadata": "object - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"
        }
    },
    usage_guidelines={
        "for_coordinator": [
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π",
            "–ü–µ—Ä–µ–¥–∞–≤–∞–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º",
            "–ù–∞—Å—Ç—Ä–æ–π—Ç–µ doc_types –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∑–∞–¥–∞—á–∏",
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ threshold –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
        ],
        "for_models": [
            "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞",
            "–ö–∞–∂–¥—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç",
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ metadata –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"
        ]
    },
    integration_notes={
        "dependencies": ["RAG trainer", "Qdrant database", "Neo4j database"],
        "performance": "–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: 1-3 —Å–µ–∫—É–Ω–¥—ã",
        "reliability": "–í—ã—Å–æ–∫–∞—è - –µ—Å—Ç—å fallback –º–µ—Ö–∞–Ω–∏–∑–º—ã",
        "scalability": "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ 50 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞ –∑–∞–ø—Ä–æ—Å"
    }
)

manifest = ToolManifest(
    name="search_rag_database",
    version="1.0.0",
    title="üîç –£–º–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π",
    description="–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º SBERT, Qdrant –∏ Neo4j. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ç–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Ä–µ–∂–∏–º—ã –ø–æ–∏—Å–∫–∞.",
    category="core_rag",
    ui_placement="dashboard",
    enabled=True,
    system=True,  # –°–∏—Å—Ç–µ–º–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
    entrypoint="tools.custom.search_rag_database_v3:execute",
    params=[
        ToolParam(
            name="query",
            type=ToolParamType.STRING,
            required=True,
            description="–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ",
            ui={
                "placeholder": "–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞...",
                "maxLength": 500
            }
        ),
        ToolParam(
            name="doc_types",
            type=ToolParamType.ARRAY,
            required=False,
            default=["norms"],
            description="–¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞",
            enum=[
                {"value": "norms", "label": "–ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"},
                {"value": "contracts", "label": "–î–æ–≥–æ–≤–æ—Ä—ã"},
                {"value": "specifications", "label": "–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏"},
                {"value": "reports", "label": "–û—Ç—á–µ—Ç—ã"},
                {"value": "standards", "label": "–°—Ç–∞–Ω–¥–∞—Ä—Ç—ã"}
            ],
            ui={
                "multiple": True,
                "searchable": True
            }
        ),
        ToolParam(
            name="k",
            type=ToolParamType.NUMBER,
            required=False,
            default=10,
            description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
            ui={
                "min": 1,
                "max": 50,
                "step": 1
            }
        ),
        ToolParam(
            name="threshold",
            type=ToolParamType.NUMBER,
            required=False,
            default=0.3,
            description="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏",
            ui={
                "min": 0.0,
                "max": 1.0,
                "step": 0.01,
                "slider": True
            }
        ),
        ToolParam(
            name="use_sbert",
            type=ToolParamType.BOOLEAN,
            required=False,
            default=True,
            description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SBERT –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞",
            ui={
                "switch": True
            }
        ),
        ToolParam(
            name="include_metadata",
            type=ToolParamType.BOOLEAN,
            required=False,
            default=True,
            description="–í–∫–ª—é—á–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
            ui={
                "switch": True
            }
        ),
        ToolParam(
            name="search_mode",
            type=ToolParamType.ENUM,
            required=False,
            default="semantic",
            description="–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞",
            enum=[
                {"value": "semantic", "label": "–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π"},
                {"value": "keyword", "label": "–ü–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"},
                {"value": "hybrid", "label": "–ì–∏–±—Ä–∏–¥–Ω—ã–π"},
                {"value": "exact", "label": "–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ"}
            ]
        )
    ],
    outputs=["results", "total_found", "query", "search_stats"],
    permissions=["read:rag", "read:qdrant", "read:neo4j"],
    tags=["search", "rag", "semantic", "enterprise", "system"],
    result_display={
        "type": "advanced_table",
        "title": "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞",
        "description": "–ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏",
        "features": {
            "sortable": True,
            "filterable": True,
            "exportable": True,
            "highlight": True
        },
        "columns": [
            {"key": "rank", "title": "‚Ññ", "width": "60px"},
            {"key": "score", "title": "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å", "width": "120px"},
            {"key": "title", "title": "–ù–∞–∑–≤–∞–Ω–∏–µ", "width": "200px"},
            {"key": "content", "title": "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ", "width": "400px"},
            {"key": "source", "title": "–ò—Å—Ç–æ—á–Ω–∏–∫", "width": "150px"},
            {"key": "doc_type", "title": "–¢–∏–ø", "width": "100px"}
        ]
    },
    documentation={
        "examples": [
            {
                "title": "–ü–æ–∏—Å–∫ –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤",
                "query": "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –±–µ—Ç–æ–Ω—É",
                "doc_types": ["norms"],
                "search_mode": "semantic"
            },
            {
                "title": "–ü–æ–∏—Å–∫ –≤ –¥–æ–≥–æ–≤–æ—Ä–∞—Ö",
                "query": "—Å—Ä–æ–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ä–∞–±–æ—Ç",
                "doc_types": ["contracts"],
                "threshold": 0.5
            }
        ],
        "tips": [
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤",
            "–ù–∞—Å—Ç—Ä–æ–π—Ç–µ doc_types –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏",
            "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ threshold –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
        ]
    },
    coordinator_interface=coordinator_interface
)

def execute(**kwargs) -> Dict[str, Any]:
    """Execute enterprise-level RAG search with advanced error handling."""
    start_time = time.time()
    
    try:
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–æ–π, –ø–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞–ø—Ä—è–º—É—é
        query = kwargs['query']
        doc_types = kwargs.get('doc_types', ['norms'])
        k = kwargs.get('k', 10)
        threshold = kwargs.get('threshold', 0.3)
        use_sbert = kwargs.get('use_sbert', True)
        include_metadata = kwargs.get('include_metadata', True)
        search_mode = kwargs.get('search_mode', 'semantic')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        if not _check_dependencies():
            raise ToolDependencyError("RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö.")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        search_results = _perform_search(
            query, doc_types, k, threshold, use_sbert, include_metadata, search_mode
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        search_stats = _generate_search_stats(query, search_results, search_mode, use_sbert)
        
        execution_time = time.time() - start_time
        
        return {
            'status': 'success',
            'data': {
                'results': search_results['results'],
                'total_found': search_results['total_found'],
                'query': query,
                'search_stats': search_stats
            },
            'execution_time': execution_time,
            'result_type': 'advanced_table',
            'result_title': f'üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞: "{query}"',
            'result_table': search_results['results'],
            'metadata': {
                'search_mode': search_mode,
                'doc_types_searched': doc_types,
                'threshold_used': threshold,
                'sbert_enabled': use_sbert
            }
        }
        
    except ToolValidationError as e:
        return {
            'status': 'error',
            'error': str(e),
            'error_category': 'validation',
            'execution_time': time.time() - start_time
        }
    except ToolDependencyError as e:
        return {
            'status': 'error',
            'error': str(e),
            'error_category': 'dependency',
            'execution_time': time.time() - start_time
        }
    except ToolProcessingError as e:
        return {
            'status': 'error',
            'error': str(e),
            'error_category': 'processing',
            'execution_time': time.time() - start_time
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': f'–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {str(e)}',
            'error_category': 'unknown',
            'execution_time': time.time() - start_time
        }


def _check_dependencies() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º RAG trainer
        from core.unified_tools_system import execute_tool as unified_exec
        return True
    except ImportError:
        return False


def _perform_search(query: str, doc_types: List[str], k: int, threshold: float,
                   use_sbert: bool, include_metadata: bool, search_mode: str) -> Dict[str, Any]:
    """–í—ã–ø–æ–ª–Ω—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–π RAG –ø–æ–∏—Å–∫ —Å —Ä–µ–∞–ª—å–Ω—ã–º fallback."""
    try:
        # !!! –†–ï–ê–õ–¨–ù–´–ô RAG –ü–û–ò–°–ö: –ü—ã—Ç–∞–µ–º—Å—è —á–µ—Ä–µ–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö !!!
        return _real_rag_search(query, doc_types, k, threshold, use_sbert, include_metadata, search_mode)
            
    except Exception as e:
        # !!! –†–ï–ê–õ–¨–ù–´–ô FALLBACK: –ü–æ–∏—Å–∫ –ø–æ —Ä–µ–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º! !!!
        try:
            return _real_file_search(query, doc_types, k)
        except Exception as fallback_error:
            raise ToolProcessingError(f"RAG –ø–æ–∏—Å–∫ –Ω–µ —É–¥–∞–ª—Å—è: {str(e)}. Fallback –ø–æ–∏—Å–∫ —Ç–æ–∂–µ –Ω–µ —É–¥–∞–ª—Å—è: {str(fallback_error)}")


def _real_rag_search(query: str, doc_types: List[str], k: int, threshold: float,
                    use_sbert: bool, include_metadata: bool, search_mode: str) -> Dict[str, Any]:
    """–†–µ–∞–ª—å–Ω—ã–π RAG –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Neo4j –∏ Qdrant."""
    try:
        # !!! –†–ï–ê–õ–¨–ù–´–ô –ü–û–ò–°–ö: –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–∞–º –¥–∞–Ω–Ω—ã—Ö !!!
        from enterprise_rag_trainer_full import EnterpriseRAGTrainer
        
        # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä RAG trainer
        trainer = EnterpriseRAGTrainer()
        
        # !!! –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–±–∏—Ä–∞–µ–º —Ñ–∏–ª—å—Ç—Ä –ø–æ doc_types, —Ç–∞–∫ –∫–∞–∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ !!!
        search_results = trainer.query_with_filters(
            question=query,
            k=k,
            doc_types=None,  # !!! –£–ë–ò–†–ê–ï–ú –§–ò–õ–¨–¢–† !!!
            threshold=threshold
        )
        
        # !!! –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏–∑ query_with_filters !!!
        formatted_results = []
        for i, result in enumerate(search_results.get('results', []), 1):
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            chunk = result.get('chunk', '')
            meta = result.get('meta', {})
            score = result.get('score', 0.0)
            
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
            file_path = meta.get('file_path', '')
            title = meta.get('title', '')
            
            # !!! –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –°–ü –∏–∑ –ö–ê–ù–û–ù–ò–ß–ï–°–ö–ò–• –ú–ï–¢–ê–î–ê–ù–ù–´–• !!!
            sp_number = _extract_sp_number(file_path, chunk, title, meta)
            
            # !!! –û–¢–õ–ê–î–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è !!!
            print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢ –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –°–ü: '{sp_number}'")
            
            # –°–æ–∑–¥–∞–µ–º –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if sp_number:
                display_title = f"–°–ü {sp_number}"
            elif title:
                display_title = title
            elif file_path:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
                from pathlib import Path
                file_name = Path(file_path).stem
                display_title = file_name.replace('_', ' ').replace('-', ' ')
            else:
                display_title = '–î–æ–∫—É–º–µ–Ω—Ç'
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
            doc_type = _determine_doc_type(file_path, chunk, meta)
            
            formatted_results.append({
                'rank': i,
                'score': f"{score:.3f}",
                'title': display_title,
                'content': chunk,
                'source': file_path if file_path else '–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π',
                'doc_type': doc_type,
                'metadata': meta,
                'sp_number': sp_number  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–º–µ—Ä –°–ü
            })
        
        return {
            'results': formatted_results,
            'total_found': len(formatted_results)
        }
        
    except Exception as e:
        raise ToolProcessingError(f"–û—à–∏–±–∫–∞ RAG –ø–æ–∏—Å–∫–∞: {str(e)}")


def _format_search_results(data: Dict[str, Any], query: str) -> Dict[str, Any]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    results = data.get('results', [])
    total_found = data.get('total_found', len(results))
    
    formatted_results = []
    for i, result in enumerate(results, 1):
            # !!! –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –î–æ–±–∞–≤–ª—è–µ–º doc_id –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤! !!!
            metadata = result.get('metadata', {}) if result.get('metadata') else {}
            doc_id = metadata.get('doc_id', '')
            doc_title = metadata.get('doc_title', result.get('title', '–î–æ–∫—É–º–µ–Ω—Ç'))
            
            formatted_results.append({
                'rank': i,
                'score': f"{result.get('score', result.get('relevance', 0.8)):.3f}",
                'title': result.get('title', result.get('file_path', '–î–æ–∫—É–º–µ–Ω—Ç')),
                'content': result.get('content', result.get('chunk', '')),
                'source': result.get('source', result.get('file_path', '–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π')),
                'doc_type': result.get('doc_type', 'unknown'),
                'metadata': metadata,
                # !!! –î–û–ë–ê–í–õ–Ø–ï–ú doc_id –ò –°–°–´–õ–ö–£ –î–õ–Ø –°–ö–ê–ß–ò–í–ê–ù–ò–Ø! !!!
                'doc_id': doc_id,
                'doc_title': doc_title,
                'download_url': f'/api/files/download?doc_id={doc_id}' if doc_id else None
            })
    
    return {
        'results': formatted_results,
        'total_found': total_found
    }


def _extract_sp_number(file_path: str, chunk: str, title: str, metadata: Dict = None) -> str:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä –°–ü –∏–∑ –ö–ê–ù–û–ù–ò–ß–ï–°–ö–ò–• –ú–ï–¢–ê–î–ê–ù–ù–´–• (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç) –∏–ª–∏ –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    import re
    
    # !!! –û–¢–õ–ê–î–ö–ê: –õ–æ–≥–∏—Ä—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ !!!
    print(f"üîç DEBUG _extract_sp_number:")
    print(f"  file_path: {file_path}")
    print(f"  title: {title}")
    print(f"  metadata: {metadata}")
    print(f"  chunk preview: {chunk[:100] if chunk else 'None'}...")
    
    # !!! –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ò—â–µ–º –≤ –ö–ê–ù–û–ù–ò–ß–ï–°–ö–ò–• –ú–ï–¢–ê–î–ê–ù–ù–´–• !!!
    if metadata:
        # –ò—â–µ–º –≤ canonical_id (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
        canonical_id = metadata.get('canonical_id', '')
        if canonical_id:
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–æ–º–µ—Ä–∞ –°–ü –∏–∑ canonical_id
            sp_patterns = [
                r'–°–ü\s+(\d+(?:\.\d+)*(?:\.\d+)*)',  # –°–ü 123.456.789
                r'–°–ü\s+(\d+)',  # –°–ü 123
                r'(\d+\.\d+\.\d+)',  # 123.456.789
                r'(\d+\.\d+)',  # 123.456
            ]
            for pattern in sp_patterns:
                match = re.search(pattern, canonical_id)
                if match:
                    return match.group(1)
        
        # –ò—â–µ–º –≤ doc_numbers
        doc_numbers = metadata.get('doc_numbers', [])
        if doc_numbers and isinstance(doc_numbers, list):
            for doc_num in doc_numbers:
                for pattern in sp_patterns:
                    match = re.search(pattern, str(doc_num))
                    if match:
                        return match.group(1)
        
        # –ò—â–µ–º –≤ primary_doc_name
        primary_doc_name = metadata.get('primary_doc_name', '')
        if primary_doc_name:
            for pattern in sp_patterns:
                match = re.search(pattern, primary_doc_name)
                if match:
                    return match.group(1)
    
    # !!! –ü–†–ò–û–†–ò–¢–ï–¢ 2: Fallback - –∏—â–µ–º –≤ –¥—Ä—É–≥–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö !!!
    sp_patterns = [
        r'–°–ü\s+(\d+(?:\.\d+)*(?:\.\d+)*)',  # –°–ü 123.456.789
        r'–°–ü\s+(\d+)',  # –°–ü 123
        r'(\d+\.\d+\.\d+)',  # 123.456.789
        r'(\d+\.\d+)',  # 123.456
    ]
    
    # –ò—â–µ–º –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ
    if title:
        for pattern in sp_patterns:
            match = re.search(pattern, title)
            if match:
                return match.group(1)
    
    # –ò—â–µ–º –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤)
    if chunk:
        content_preview = chunk[:500]
        for pattern in sp_patterns:
            match = re.search(pattern, content_preview)
            if match:
                print(f"‚úÖ –ù–ê–ô–î–ï–ù –°–ü –≤ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º: {match.group(1)}")
                return match.group(1)
    
    print(f"‚ùå –°–ü –ù–ï –ù–ê–ô–î–ï–ù")
    return ""

def _determine_doc_type(file_path: str, chunk: str, meta: dict) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—É—Ç–∏, —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
    import re
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    if meta.get('doc_type'):
        return meta.get('doc_type')
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ñ–∞–π–ª–æ–≤—ã–π –ø—É—Ç—å
    if file_path:
        file_lower = file_path.lower()
        if '—Å–ø' in file_lower or '—Å–Ω–∏–ø' in file_lower:
            return 'norms'
        elif '–≥–æ—Å—Ç' in file_lower:
            return 'gost'
        elif '–∏–∑–º' in file_lower or '–∏–∑–º–µ–Ω–µ–Ω–∏–µ' in file_lower:
            return 'amendment'
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
    if chunk:
        content_lower = chunk.lower()
        if any(keyword in content_lower for keyword in ['—Å–ø ', '—Å–Ω–∏–ø ', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã']):
            return 'norms'
        elif any(keyword in content_lower for keyword in ['–≥–æ—Å—Ç', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç']):
            return 'gost'
        elif any(keyword in content_lower for keyword in ['–∏–∑–º–µ–Ω–µ–Ω–∏–µ', '–ø–æ–ø—Ä–∞–≤–∫–∞']):
            return 'amendment'
    
    return 'document'

def _real_file_search(query: str, doc_types: List[str], k: int) -> Dict[str, Any]:
    """–†–ï–ê–õ–¨–ù–´–ô –ø–æ–∏—Å–∫ –ø–æ —Ñ–∞–π–ª–∞–º –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
    try:
        # !!! –†–ï–ê–õ–¨–ù–´–ô –ü–û–ò–°–ö: –ò—â–µ–º –≤ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö! !!!
        import os
        import glob
        from pathlib import Path
        
        # –ò—â–µ–º –≤ –±–∞–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        base_dir = "I:/docs/downloaded"  # –†–µ–∞–ª—å–Ω–∞—è –±–∞–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if not os.path.exists(base_dir):
            raise ToolProcessingError(f"–ë–∞–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {base_dir}")
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keywords = query.lower().split()
        results = []
        
        # –ü–æ–∏—Å–∫ –≤ PDF —Ñ–∞–π–ª–∞—Ö
        pdf_files = glob.glob(os.path.join(base_dir, "**/*.pdf"), recursive=True)
        for pdf_file in pdf_files[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            try:
                # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
                filename = os.path.basename(pdf_file).lower()
                if any(keyword in filename for keyword in keywords):
                    results.append({
                        'rank': len(results) + 1,
                        'score': 0.9,
                        'title': os.path.basename(pdf_file),
                        'content': f"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {os.path.basename(pdf_file)}",
                        'source': pdf_file,
                        'doc_type': 'pdf',
                        'metadata': {'file_path': pdf_file, 'real_search': True}
                    })
            except Exception:
                continue
        
        # –ü–æ–∏—Å–∫ –≤ DOCX —Ñ–∞–π–ª–∞—Ö
        docx_files = glob.glob(os.path.join(base_dir, "**/*.docx"), recursive=True)
        for docx_file in docx_files[:50]:
            try:
                filename = os.path.basename(docx_file).lower()
                if any(keyword in filename for keyword in keywords):
                    results.append({
                        'rank': len(results) + 1,
                        'score': 0.8,
                        'title': os.path.basename(docx_file),
                        'content': f"–ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª: {os.path.basename(docx_file)}",
                        'source': docx_file,
                        'doc_type': 'docx',
                        'metadata': {'file_path': docx_file, 'real_search': True}
                    })
            except Exception:
                continue
        
        return {
            'results': results[:k],
            'total_found': len(results)
        }
        
    except Exception as e:
        raise ToolProcessingError(f"–†–µ–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Ñ–∞–π–ª–∞–º –Ω–µ —É–¥–∞–ª—Å—è: {str(e)}")


def _generate_search_stats(query: str, search_results: Dict[str, Any], 
                          search_mode: str, use_sbert: bool) -> Dict[str, Any]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–∏—Å–∫–∞."""
    results = search_results.get('results', [])
    
    if not results:
        return {
            'query_length': len(query),
            'search_mode': search_mode,
            'sbert_enabled': use_sbert,
            'results_found': 0,
            'avg_relevance': 0.0
        }
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
    scores = []
    for result in results:
        try:
            score = float(result.get('score', 0))
            scores.append(score)
        except (ValueError, TypeError):
            continue
    
    avg_relevance = sum(scores) / len(scores) if scores else 0.0
    
    return {
        'query_length': len(query),
        'search_mode': search_mode,
        'sbert_enabled': use_sbert,
        'results_found': len(results),
        'avg_relevance': avg_relevance,
        'relevance_range': {
            'min': min(scores) if scores else 0.0,
            'max': max(scores) if scores else 0.0
        }
    }
