"""
üá∑üá∫ –õ–æ–∫–∞–ª—å–Ω—ã–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π LLM –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è RAG –ø–∞–π–ø–ª–∞–π–Ω–∞
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏: RuGPT, YaLM, T-Lite
"""

import logging
import torch
import json
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    pipeline,
    BitsAndBytesConfig
)

logger = logging.getLogger(__name__)

@dataclass
class LocalLLMConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM"""
    model_name: str = "ai-forever/rugpt3large_based_on_gpt2"  # RuGPT-3.5
    device: str = "auto"  # auto, cuda, cpu
    max_length: int = 4096  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    temperature: float = 0.1
    do_sample: bool = True
    use_quantization: bool = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    cache_dir: str = "./models_cache"

class LocalRussianLLMProcessor:
    """
    –õ–æ–∫–∞–ª—å–Ω—ã–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π LLM –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç RuGPT, YaLM, T-Lite –∏ –¥—Ä—É–≥–∏–µ —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ –º–æ–¥–µ–ª–∏
    """
    
    def __init__(self, config: LocalLLMConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self._initialize_model()
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            self.logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {self.config.model_name}")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            quantization_config = None
            if self.config.use_quantization and torch.cuda.is_available():
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0,
                    llm_int8_has_fp16_weight=False,
                )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_name,
                cache_dir=self.config.cache_dir,
                trust_remote_code=True
            )
            
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_name,
                cache_dir=self.config.cache_dir,
                quantization_config=quantization_config,
                device_map=self.config.device,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True
            )
            
            # –°–æ–∑–¥–∞–µ–º pipeline –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=self.config.device,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
            )
            
            self.logger.info(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.config.model_name}")
            self.logger.info(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.model.device}")
            self.logger.info(f"üíæ –ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB" if torch.cuda.is_available() else "CPU —Ä–µ–∂–∏–º")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def analyze_document_structure(self, content: str, vlm_metadata: Dict) -> Dict:
        """
        –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM
        
        Args:
            content: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            vlm_metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç VLM –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM
            prompt = self._prepare_analysis_prompt(content, vlm_metadata)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å –ª–æ–∫–∞–ª—å–Ω—ã–º LLM
            analysis_result = self._analyze_with_local_llm(prompt)
            
            return {
                'local_llm_available': True,
                'analysis': analysis_result,
                'model': self.config.model_name,
                'device': str(self.model.device),
                'context_length': len(prompt),
                'processing_time': analysis_result.get('processing_time', 0)
            }
            
        except Exception as e:
            self.logger.error(f"Local LLM analysis failed: {e}")
            return {
                'local_llm_available': False,
                'error': str(e),
                'fallback_to_vlm': True
            }
    
    def _prepare_analysis_prompt(self, content: str, vlm_metadata: Dict) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç VLM
        vlm_tables = vlm_metadata.get('tables', [])
        vlm_sections = vlm_metadata.get('sections', [])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        max_content_length = self.config.max_length - 500  # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
        truncated_content = content[:max_content_length] if len(content) > max_content_length else content
        
        prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç –∏ –∏–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.

–î–û–ö–£–ú–ï–ù–¢:
{truncated_content}

VLM –ê–ù–ê–õ–ò–ó (–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π):
- –¢–∞–±–ª–∏—Ü—ã: {len(vlm_tables)}
- –°–µ–∫—Ü–∏–∏: {len(vlm_sections)}

–ó–ê–î–ê–ß–ò:
1. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–°–ù–∏–ü, –ì–û–°–¢, –ü–ü–†, —Å–º–µ—Ç–∞, –ø—Ä–æ–µ–∫—Ç)
2. –ò–∑–≤–ª–µ–∫–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –∏ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
3. –ù–∞–π–¥–∏ —Ç–∞–±–ª–∏—Ü—ã –∏ –∏—Ö —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
4. –í—ã–¥–µ–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –Ω–æ—Ä–º—ã
5. –û–ø—Ä–µ–¥–µ–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å –∏ –æ–±–ª–∞—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏—è

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
    "document_type": "—Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞",
    "sections": [
        {{
            "title": "–Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞",
            "content": "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ",
            "level": 1
        }}
    ],
    "tables": [
        {{
            "title": "–Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã",
            "data": "–¥–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã"
        }}
    ],
    "requirements": [
        "–∫–ª—é—á–µ–≤–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ 1",
        "–∫–ª—é—á–µ–≤–æ–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ 2"
    ],
    "scope": "–æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"
}}

–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        
        return prompt
    
    def _analyze_with_local_llm(self, prompt: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º LLM"""
        start_time = time.time()
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –ø–æ–º–æ—â—å—é pipeline
            response = self.pipeline(
                prompt,
                max_length=min(len(prompt) + 1000, self.config.max_length),
                temperature=self.config.temperature,
                do_sample=self.config.do_sample,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                return_full_text=False
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            generated_text = response[0]['generated_text'].strip()
            
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ JSON –≤ –æ—Ç–≤–µ—Ç–µ
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = generated_text[json_start:json_end]
                try:
                    analysis = json.loads(json_text)
                    processing_time = time.time() - start_time
                    analysis['processing_time'] = processing_time
                    return analysis
                except json.JSONDecodeError:
                    self.logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM")
                    return {"error": "JSON parsing failed", "raw_response": generated_text}
            else:
                self.logger.warning("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ LLM")
                return {"error": "No JSON found", "raw_response": generated_text}
                
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM: {e}")
            return {"error": str(e), "processing_time": time.time() - start_time}
    
    def get_model_info(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        return {
            'model_name': self.config.model_name,
            'device': str(self.model.device) if self.model else 'not_loaded',
            'max_length': self.config.max_length,
            'quantization': self.config.use_quantization,
            'cache_dir': self.config.cache_dir
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è RuGPT-3.5
    config = LocalLLMConfig(
        model_name="ai-forever/rugpt3large_based_on_gpt2",
        device="auto",
        max_length=2048,
        use_quantization=True
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞
    processor = LocalRussianLLMProcessor(config)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    test_content = "–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã."
    vlm_metadata = {"tables": [], "sections": []}
    
    result = processor.analyze_document_structure(test_content, vlm_metadata)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞: {result}")
    print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏: {processor.get_model_info()}")
