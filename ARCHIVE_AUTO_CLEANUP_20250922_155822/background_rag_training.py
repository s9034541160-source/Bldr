#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ BACKGROUND RAG TRAINING SCRIPT
=================================
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –ø–æ–ª–Ω–æ–≥–æ RAG-–æ–±—É—á–µ–Ω–∏—è –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
–Ω–∞ –≤—Å–µ—Ö 1168 —Ñ–∞–π–ª–∞—Ö —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º

–í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
‚úÖ –û–±—É—á–µ–Ω–∏–µ –≤ —Ñ–æ–Ω–µ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã —á–∞–Ω–∫–∏–Ω–≥–∞  
‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏ –æ—à–∏–±–æ–∫
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ —Å–±–æ—è—Ö
‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –æ—Ç—á–µ—Ç—ã
"""

import os
import sys
import time
import json
import logging
import threading
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp
from queue import Queue
import signal

# Import process tracking system
from core.process_tracker import get_process_tracker, ProcessType, ProcessStatus
from core.retry_system import get_retry_system

# Get process tracker and retry system instances
process_tracker = get_process_tracker()
retry_system = get_retry_system()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–≥–æ —Ä–µ–∂–∏–º–∞
log_dir = Path("C:/Bldr/logs")
log_dir.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / f"rag_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç –Ω–∞—à–µ–≥–æ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ RAG —Ç—Ä–µ–Ω–µ—Ä–∞
try:
    from working_frontend_rag_integration import create_working_rag_trainer, WorkingEnhancedRAGTrainer
    ENHANCED_RAG_AVAILABLE = True
    logger.info("‚úÖ Enhanced RAG trainer imported successfully")
except ImportError as e:
    logger.error(f"‚ùå Failed to import enhanced RAG trainer: {e}")
    ENHANCED_RAG_AVAILABLE = False
    sys.exit(1)

class BackgroundRAGTrainer:
    """
    üöÄ –§–æ–Ω–æ–≤—ã–π RAG —Ç—Ä–µ–Ω–µ—Ä —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º
    """
    
    def __init__(self, base_dir: str = "I:/docs", max_workers: int = 4):
        self.base_dir = Path(base_dir)
        self.max_workers = max_workers
        self.progress_file = Path("C:/Bldr/training_progress.json")
        self.stats_file = Path("C:/Bldr/training_stats.json")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        self.stats = {
            'start_time': None,
            'end_time': None,
            'files_found': 0,
            'files_processed': 0,
            'files_failed': 0,
            'chunks_created': 0,
            'processing_errors': [],
            'current_file': '',
            'progress_percent': 0.0,
            'estimated_time_left': 'unknown',
            'average_time_per_file': 0.0
        }
        
        # –û—á–µ—Ä–µ–¥—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.file_queue = Queue()
        self.processed_files = set()
        self.failed_files = set()
        
        # –§–ª–∞–≥–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
        self.running = False
        self.paused = False
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä
        logger.info("üöÄ Initializing enhanced RAG trainer...")
        self.rag_trainer = create_working_rag_trainer(use_intelligent_chunking=True)
        
        # Process tracking
        self.process_id = f"rag_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        logger.info(f"‚úÖ Background RAG Trainer initialized")
        logger.info(f"üìÅ Base directory: {self.base_dir}")
        logger.info(f"üë• Max workers: {self.max_workers}")
    
    def discover_files(self) -> List[Path]:
        """–ü–æ–∏—Å–∫ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        logger.info("üîç Discovering files for training...")
        
        extensions = ['*.pdf', '*.docx', '*.doc', '*.txt', '*.rtf']
        all_files = []
        
        for ext in extensions:
            try:
                files = list(self.base_dir.rglob(ext))
                all_files.extend(files)
                logger.info(f"  üìÑ Found {len(files)} {ext} files")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error searching for {ext}: {e}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ–∞–π–ª—ã
        unique_files = []
        seen_names = set()
        
        for file_path in all_files:
            if file_path.name.lower() not in seen_names and file_path.exists():
                unique_files.append(file_path)
                seen_names.add(file_path.name.lower())
        
        logger.info(f"üìä Total unique files discovered: {len(unique_files)}")
        self.stats['files_found'] = len(unique_files)
        
        return unique_files
    
    def load_progress(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞"""
        
        if self.progress_file.exists():
            try:
                with open(self.progress_file, 'r', encoding='utf-8') as f:
                    progress = json.load(f)
                    
                self.processed_files = set(progress.get('processed_files', []))
                self.failed_files = set(progress.get('failed_files', []))
                
                logger.info(f"üìà Loaded progress: {len(self.processed_files)} processed, {len(self.failed_files)} failed")
                return progress
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to load progress: {e}")
        
        return {}
    
    def save_progress(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        
        progress = {
            'processed_files': list(self.processed_files),
            'failed_files': list(self.failed_files),
            'stats': self.stats,
            'timestamp': datetime.now().isoformat()
        }
        
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå Failed to save progress: {e}")
    
    def save_stats(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        
        try:
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå Failed to save stats: {e}")
    
    def process_single_file(self, file_path: Path) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º"""
        
        start_time = time.time()
        result = {
            'file_path': str(file_path),
            'success': False,
            'chunks_created': 0,
            'processing_time': 0,
            'error': None,
            'file_size': 0
        }
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            if file_path.stat().st_size > 50 * 1024 * 1024:  # 50MB
                raise ValueError(f"File too large: {file_path.stat().st_size / 1024 / 1024:.1f}MB")
            
            result['file_size'] = file_path.stat().st_size
            
            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            if file_path.suffix.lower() == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–µ —á—Ç–µ–Ω–∏–µ
                # –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ PDF/DOCX —á–µ—Ä–µ–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
                try:
                    if file_path.suffix.lower() == '.pdf':
                        # –ò–º–ø–æ—Ä—Ç PDF –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
                        try:
                            import PyPDF2
                            with open(file_path, 'rb') as pdf_file:
                                pdf_reader = PyPDF2.PdfReader(pdf_file)
                                content = ''
                                for page in pdf_reader.pages:
                                    content += page.extract_text() + '\n'
                        except ImportError:
                            logger.warning("PyPDF2 not available for PDF processing")
                            raise ValueError("PDF processing library not available")
                    elif file_path.suffix.lower() in ['.docx', '.doc']:
                        # –ò–º–ø–æ—Ä—Ç DOCX –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
                        try:
                            from docx import Document
                            doc = Document(str(file_path))
                            content = '\n'.join([paragraph.text for paragraph in doc.paragraphs])
                        except ImportError:
                            logger.warning("python-docx not available for DOCX processing")
                            raise ValueError("DOCX processing library not available")
                    else:
                        # –î–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
                        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                except Exception as e:
                    # Fallback –¥–ª—è –±–∏–Ω–∞—Ä–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                    logger.warning(f"‚ö†Ô∏è Skipping file {file_path}: {str(e)}")
                    raise ValueError(f"File format not supported or processing error: {str(e)}")
            
            if not content.strip():
                raise ValueError("Empty file content")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å –ø–æ–º–æ—â—å—é —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ RAG —Ç—Ä–µ–Ω–µ—Ä–∞
            document_result = self.rag_trainer.process_single_document(content, str(file_path))
            
            chunks_count = len(document_result.get('chunks', []))
            result['chunks_created'] = chunks_count
            result['success'] = True
            
            logger.info(f"‚úÖ Processed {file_path.name}: {chunks_count} chunks")
            
        except Exception as e:
            result['error'] = str(e)
            logger.error(f"‚ùå Failed to process {file_path.name}: {e}")
        
        result['processing_time'] = time.time() - start_time
        return result
    
    def update_stats(self, result: Dict[str, Any]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        if result['success']:
            self.stats['files_processed'] += 1
            self.stats['chunks_created'] += result['chunks_created']
            self.processed_files.add(result['file_path'])
        else:
            self.stats['files_failed'] += 1
            self.failed_files.add(result['file_path'])
            self.stats['processing_errors'].append({
                'file': result['file_path'],
                'error': result['error'],
                'timestamp': datetime.now().isoformat()
            })
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        total_files = self.stats['files_found']
        processed_total = self.stats['files_processed'] + self.stats['files_failed']
        
        if total_files > 0:
            self.stats['progress_percent'] = (processed_total / total_files) * 100
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if self.stats['files_processed'] > 0:
            elapsed_time = time.time() - self.stats['start_time']
            self.stats['average_time_per_file'] = elapsed_time / processed_total
            
            # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
            files_left = total_files - processed_total
            estimated_seconds = files_left * self.stats['average_time_per_file']
            self.stats['estimated_time_left'] = str(timedelta(seconds=int(estimated_seconds)))
    
    def print_progress(self):
        """–í—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        
        progress = self.stats['progress_percent']
        processed = self.stats['files_processed']
        failed = self.stats['files_failed']
        total = self.stats['files_found']
        chunks = self.stats['chunks_created']
        time_left = self.stats['estimated_time_left']
        
        print(f"\nüìä RAG Training Progress:")
        print(f"  üìà Progress: {progress:.1f}% ({processed + failed}/{total})")
        print(f"  ‚úÖ Processed: {processed} files")
        print(f"  ‚ùå Failed: {failed} files")
        print(f"  üß© Chunks created: {chunks}")
        print(f"  ‚è±Ô∏è Time left: {time_left}")
        print(f"  üìÑ Current: {self.stats.get('current_file', 'N/A')}")
    
    def start_training(self, resume: bool = True):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –≤ —Ñ–æ–Ω–µ"""
        
        logger.info("üöÄ Starting background RAG training...")
        
        # Register process with process tracker
        process_tracker.start_process(
            process_id=self.process_id,
            process_type=ProcessType.RAG_TRAINING,
            name="RAG Training Process",
            description="Full RAG training on all documents",
            metadata={
                "base_dir": str(self.base_dir),
                "max_workers": self.max_workers,
                "resume": resume
            }
        )
        
        # Update process tracker status
        process_tracker.update_process(
            self.process_id,
            status=ProcessStatus.RUNNING,
            progress=0,
            metadata_update={"stage": "initializing"}
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if resume:
            self.load_progress()
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã
        all_files = self.discover_files()
        
        if not all_files:
            logger.error("‚ùå No files found for training!")
            process_tracker.update_process(
                self.process_id,
                status=ProcessStatus.FAILED,
                metadata_update={"stage": "error", "error": "No files found for training"}
            )
            return
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã –ø—Ä–∏ –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏
        if resume:
            files_to_process = [f for f in all_files if str(f) not in self.processed_files]
            logger.info(f"üìÅ Resuming training: {len(files_to_process)} files left to process")
        else:
            files_to_process = all_files
            self.processed_files.clear()
            self.failed_files.clear()
        
        if not files_to_process:
            logger.info("‚úÖ All files already processed!")
            process_tracker.update_process(
                self.process_id,
                status=ProcessStatus.COMPLETED,
                progress=100,
                metadata_update={"stage": "completed", "message": "All files already processed"}
            )
            return
        
        self.stats['start_time'] = time.time()
        self.stats['files_found'] = len(all_files)
        self.running = True
        
        # Update process tracker
        process_tracker.update_process(
            self.process_id,
            progress=5,
            metadata_update={"stage": "processing", "files_to_process": len(files_to_process)}
        )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –ø–æ—Ç–æ–∫ –¥–ª—è –ø–µ—á–∞—Ç–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        progress_thread = threading.Thread(target=self._progress_monitor, daemon=True)
        progress_thread.start()
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –≤ –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–º —Ä–µ–∂–∏–º–µ
        try:
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏ –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É
                future_to_file = {
                    executor.submit(self.process_single_file, file_path): file_path 
                    for file_path in files_to_process
                }
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
                for future in as_completed(future_to_file):
                    if not self.running:
                        break
                    
                    file_path = future_to_file[future]
                    self.stats['current_file'] = file_path.name
                    
                    try:
                        result = future.result(timeout=1800)  # 30 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
                        self.update_stats(result)
                        
                        # Update process tracker with progress
                        total_files = self.stats['files_found']
                        processed_total = self.stats['files_processed'] + self.stats['files_failed']
                        progress_percent = (processed_total / total_files) * 100 if total_files > 0 else 0
                        
                        process_tracker.update_process(
                            self.process_id,
                            progress=int(progress_percent),
                            metadata_update={
                                "stage": "processing",
                                "current_file": file_path.name,
                                "files_processed": self.stats['files_processed'],
                                "files_failed": self.stats['files_failed']
                            }
                        )
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 10 —Ñ–∞–π–ª–æ–≤
                        if (self.stats['files_processed'] + self.stats['files_failed']) % 10 == 0:
                            self.save_progress()
                            self.save_stats()
                        
                    except Exception as e:
                        logger.error(f"‚ùå Task failed for {file_path.name}: {e}")
                        self.stats['files_failed'] += 1
                        self.failed_files.add(str(file_path))
                        
                        # Update process tracker with error
                        process_tracker.update_process(
                            self.process_id,
                            metadata_update={
                                "stage": "processing",
                                "error_files": len(self.failed_files),
                                "last_error": str(e)
                            }
                        )
        
        except KeyboardInterrupt:
            logger.info("‚ö†Ô∏è Training interrupted by user")
            process_tracker.update_process(
                self.process_id,
                status=ProcessStatus.CANCELLED,
                metadata_update={"stage": "cancelled", "message": "Training interrupted by user"}
            )
        except Exception as e:
            logger.error(f"‚ùå Training failed with error: {e}")
            process_tracker.update_process(
                self.process_id,
                status=ProcessStatus.FAILED,
                metadata_update={"stage": "error", "error": str(e)}
            )
        
        finally:
            self.running = False
            self.stats['end_time'] = time.time()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.save_progress()
            self.save_stats()
            
            # Update process tracker with final status
            if self.stats['files_failed'] == 0:
                process_tracker.update_process(
                    self.process_id,
                    status=ProcessStatus.COMPLETED,
                    progress=100,
                    metadata_update={"stage": "completed", "message": "Training completed successfully"}
                )
            else:
                process_tracker.update_process(
                    self.process_id,
                    status=ProcessStatus.COMPLETED,
                    progress=100,
                    metadata_update={
                        "stage": "completed_with_errors",
                        "message": f"Training completed with {self.stats['files_failed']} failed files",
                        "files_processed": self.stats['files_processed'],
                        "files_failed": self.stats['files_failed']
                    }
                )
            
            # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            self._print_final_report()
    
    def _progress_monitor(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
        
        while self.running:
            self.print_progress()
            time.sleep(30)  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
    
    def _print_final_report(self):
        """–í—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ–±—É—á–µ–Ω–∏—è"""
        
        total_time = self.stats['end_time'] - self.stats['start_time']
        
        logger.info("üéØ RAG Training Completed!")
        logger.info("=" * 50)
        logger.info(f"üìä Final Statistics:")
        logger.info(f"  üìÅ Files found: {self.stats['files_found']}")
        logger.info(f"  ‚úÖ Successfully processed: {self.stats['files_processed']}")
        logger.info(f"  ‚ùå Failed: {self.stats['files_failed']}")
        logger.info(f"  üß© Total chunks created: {self.stats['chunks_created']}")
        logger.info(f"  ‚è±Ô∏è Total time: {timedelta(seconds=int(total_time))}")
        logger.info(f"  üìà Success rate: {(self.stats['files_processed'] / self.stats['files_found'] * 100):.1f}%")
        
        if self.stats['files_failed'] > 0:
            logger.info(f"‚ö†Ô∏è Failed files:")
            for error in self.stats['processing_errors'][-10:]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –æ—à–∏–±–æ–∫
                logger.info(f"    - {Path(error['file']).name}: {error['error']}")
    
    def _signal_handler(self, signum, frame):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
        logger.info(f"üì® Received signal {signum}, gracefully shutting down...")
        self.running = False

def start_background_training(base_dir: str = "I:/docs", max_workers: int = 4, resume: bool = True):
    """
    üöÄ –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤–æ–≥–æ RAG –æ–±—É—á–µ–Ω–∏—è
    
    Args:
        base_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        max_workers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤
        resume: –í–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –æ–±—É—á–µ–Ω–∏–µ –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å
    """
    trainer = BackgroundRAGTrainer(base_dir=base_dir, max_workers=max_workers)
    trainer.start_training(resume=resume)

if __name__ == "__main__":
    # Parse command line arguments
    import argparse
    parser = argparse.ArgumentParser(description="Background RAG Training")
    parser.add_argument("--base-dir", default="I:/docs", help="Base directory with documents")
    parser.add_argument("--max-workers", type=int, default=4, help="Maximum number of worker threads")
    parser.add_argument("--no-resume", action="store_true", help="Do not resume from previous progress")
    
    args = parser.parse_args()
    
    # Start training
    start_background_training(
        base_dir=args.base_dir,
        max_workers=args.max_workers,
        resume=not args.no_resume
    )