#!/usr/bin/env python3
"""Enterprise RAG Trainer: Production-Ready RAG Pipeline."""

import os
import sys
import json
import logging
import pickle
import hashlib
import time
import glob
import traceback
import re  # Single
import math
import numpy as np
from typing import List, Dict, Any, Optional, Tuple, Union
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
from dotenv import load_dotenv

# LangChain fallback
try:
    from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.vectorstores import FAISS
    from langchain.chains import RetrievalQA
    HAS_LANGCHAIN = True
except ImportError:
    HAS_LANGCHAIN = False

# ML
try:
    import torch
    from sentence_transformers import SentenceTransformer
    from transformers import AutoTokenizer, AutoModelForCausalLM
    HAS_ML_LIBS = True
    # Optional PEFT support
    try:
        from peft import LoraConfig, get_peft_model  # type: ignore
        HAS_PEFT = True
    except ImportError:
        LoraConfig = None  # type: ignore
        get_peft_model = None  # type: ignore
        HAS_PEFT = False
except ImportError:
    HAS_ML_LIBS = False
    HAS_PEFT = False
    LoraConfig = None
    get_peft_model = None

# DB
try:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models
    from neo4j import GraphDatabase
    HAS_DB_LIBS = True
except ImportError:
    HAS_DB_LIBS = False

# File/OCR
try:
    import PyPDF2
    import docx
    from docx import Document
    import docx2txt
    import pandas as pd
    from openpyxl import load_workbook
    import pytesseract
    from PIL import Image
    from pdf2image import convert_from_path
    import fitz
    HAS_FILE_PROCESSING = True
    HAS_OCR_LIBS = True
except ImportError:
    HAS_FILE_PROCESSING = False
    HAS_OCR_LIBS = False

# Optional textract import (may fail due to dependency issues)
try:
    import textract
    HAS_TEXTRACT = True
except ImportError:
    HAS_TEXTRACT = False

# Optional PyWin32 import for COM automation
try:
    import win32com.client
    HAS_PYWIN32 = True
except ImportError:
    HAS_PYWIN32 = False

load_dotenv()

# Enhanced logging configuration for production
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('enterprise_rag_trainer.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# Suppress noisy warnings for production
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings("ignore", category=FutureWarning, module="transformers")

# VLM Support (graceful)
try:
    from vlm_processor import VLMProcessor
    VLM_AVAILABLE = True
except ImportError:
    VLM_AVAILABLE = False
    VLMProcessor = None

@dataclass
class Config:
    """Production config from env."""
    base_dir: Path = Path(os.getenv('BASE_DIR', Path.cwd() / 'data'))
    processed_dir: Path = Path(os.getenv('PROCESSED_DIR', Path.cwd() / 'processed'))
    log_dir: Path = Path(os.getenv('BASE_DIR', Path.cwd() / 'data')) / 'logs'
    cache_dir: Path = Path(os.getenv('BASE_DIR', Path.cwd() / 'data')) / 'cache'
    reports_dir: Path = Path(os.getenv('BASE_DIR', Path.cwd() / 'data')) / 'reports'
    sbert_model: str = os.getenv('SBERT_MODEL', 'DeepPavlov/rubert-base-cased')
    llm_model: str = os.getenv('LLM_MODEL', 'sberbank-ai/rugpt3medium')  # Russian best
    chunk_size: int = int(os.getenv('CHUNK_SIZE', 1000))
    chunk_overlap: int = int(os.getenv('CHUNK_OVERLAP', 200))
    max_workers: int = int(os.getenv('MAX_WORKERS', 4))
    use_llm: bool = os.getenv('USE_LLM', '0').lower() in ('1', 'true')
    incremental_mode: bool = os.getenv('INCREMENTAL', '1').lower() in ('1', 'true')
    vlm_enabled: bool = os.getenv('VLM_ENABLED', '1').lower() in ('1', 'true')

    def __post_init__(self):
        for d in [self.log_dir, self.cache_dir, self.reports_dir, self.processed_dir]:
            d.mkdir(parents=True, exist_ok=True)

config = Config()

from pydantic import BaseModel, field_validator

class DataValidator(BaseModel):
    content: str
    metadata: Dict[str, Any] = {}

    @field_validator('content')
    @classmethod
    def preprocess_and_validate(cls, v: str) -> str:
        import re
        v = re.sub(r'\s+', ' ', v.strip()).lower()
        if not v or len(v) < 10:
            raise ValueError('Content too short')
        return v

def create_vector_store(self, docs: List[DataValidator], config: Config) -> FAISS:
    """Custom chunking for NTD (preserves hierarchy); LangChain fallback."""
    texts = []
    metadatas = []
    
    for doc in docs:
        doc_type = doc.metadata.get('doc_type', 'unknown')  # From Stage 4
        if doc_type in ['sp', 'gost', 'snip']:  # Custom to avoid LangChain break (1 punkt=1 chunk)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–µ—Ä–∞—Ä—Ö–∏–∏
            chunk_data = self._custom_ntd_chunking_with_metadata(doc.content, doc_type, doc.metadata)
            for chunk_info in chunk_data:
                texts.append(chunk_info['text'])
                metadatas.append(chunk_info['metadata'])
        else:
            splitter = RecursiveCharacterTextSplitter(chunk_size=config.chunk_size, chunk_overlap=config.chunk_overlap)
            chunk_texts = splitter.split_text(doc.content)
            texts.extend(chunk_texts)
            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            for chunk_text in chunk_texts:
                metadatas.append({
                    'doc_type': doc_type,
                    'chunk_type': 'content',
                    'hierarchy_level': 0,
                    'parent_path': []
                })
    
    if HAS_LANGCHAIN:
        embeddings = HuggingFaceEmbeddings(model_name=config.sbert_model)
        vectorstore = FAISS.from_texts(texts, embeddings, metadatas=metadatas)
        vectorstore.save_local(str(config.base_dir / "faiss_index"))
        return vectorstore
    else:
        logger.warning("LangChain unavailable; skipping vectorstore (use numpy fallback in prod).")
        return None

def setup_rag_chain(self, vectorstore: FAISS, config: Config) -> RetrievalQA:
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # If HAS_ML_LIBS
    model_name = os.getenv('LLM_MODEL', 'IlyaGusev/rulm_7b_gguf')  # Recommendation: Russian LLM
    llm = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    if HAS_ML_LIBS:  # Optional (no error if no PEFT)
        if 'LoraConfig' in globals() and 'get_peft_model' in globals():
            lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "v_proj"],  # For RuLM/Qwen
                                     lora_dropout=0.05, task_type="CAUSAL_LM")
            llm = get_peft_model(llm, lora_config)
            logger.info("LoRA integrated (fine-tuning ready).")
    retriever = vectorstore.as_retriever()
    chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, chain_type="stuff")
    return chain

def _custom_ntd_chunking(self, content: str, doc_type: str) -> List[str]:
    """Recursive –ì–û–°–¢-—á–∞–Ω–∫–∏–Ω–≥ —Å 3-—É—Ä–æ–≤–Ω–µ–≤–æ–π –∏–µ—Ä–∞—Ä—Ö–∏–µ–π (6‚Üí6.2‚Üí6.2.3)."""
    import re
    
    # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è 3-—É—Ä–æ–≤–Ω–µ–≤–æ–π –∏–µ—Ä–∞—Ä—Ö–∏–∏ –ì–û–°–¢
    # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: 6, 6.1, 6.1.1, 6.1.1.1 –∏ —Ç.–¥.
    punkt_pattern = r'(\d+(?:\.\d+){0,2})\s+([^–∞-—è—ë]*[A-–Ø–∞-—è—ë].*?)(?=\n\d+(?:\.\d+){0,2}\s|\n[–ê-–Ø–Å]{5,}|\Z)'
    
    punkts = re.findall(punkt_pattern, content, re.DOTALL | re.IGNORECASE)
    
    chunks = []
    chunk_metadata = []
    
    for punkt_num, punkt_content in punkts:
        if len(punkt_content.strip()) < 20:
            continue
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –∏–µ—Ä–∞—Ä—Ö–∏–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ—á–µ–∫
        level = punkt_num.count('.') + 1
        parent_path = self._get_parent_path(punkt_num, punkts)
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–µ—Ä–∞—Ä—Ö–∏–∏
        chunk_text = f"[{punkt_num}] {punkt_content.strip()}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—É—Ç–∏ –≤ —á–∞–Ω–∫
        if parent_path:
            chunk_text = f"[–ü—É—Ç—å: {' ‚Üí '.join(parent_path)} ‚Üí {punkt_num}]\n{chunk_text}"
        
        chunks.append(chunk_text)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏–∏
        chunk_metadata.append({
            'punkt_num': punkt_num,
            'level': level,
            'parent_path': parent_path,
            'content_length': len(punkt_content.strip())
        })
        
        logger.debug(f"[–ì–û–°–¢-–ß–ê–ù–ö–ò–ù–ì] –°–æ–∑–¥–∞–Ω —á–∞–Ω–∫ {punkt_num} (—É—Ä–æ–≤–µ–Ω—å {level}, –ø—É—Ç—å: {' ‚Üí '.join(parent_path) if parent_path else '–∫–æ—Ä–µ–Ω—å'})")
    
    # –ï—Å–ª–∏ —á–∞–Ω–∫–æ–≤ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    if len(chunks) < 3:
        logger.warning(f"[–ì–û–°–¢-–ß–ê–ù–ö–ò–ù–ì] –°–æ–∑–¥–∞–Ω–æ —Ç–æ–ª—å–∫–æ {len(chunks)} —á–∞–Ω–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        
        # Fallback: —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏–∏
        sentences = re.split(r'[.!?]+', content)
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if len(sentence) > 50:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–æ–º–µ—Ä –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                num_match = re.match(r'^(\d+(?:\.\d+)*)', sentence)
                if num_match:
                    chunk_text = f"[{num_match.group(1)}] {sentence}"
                else:
                    chunk_text = f"[fallback_{i}] {sentence}"
                
                chunks.append(chunk_text)
                
                if len(chunks) >= 20:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ fallback —á–∞–Ω–∫–æ–≤
                    break
    
    logger.info(f"[–ì–û–°–¢-–ß–ê–ù–ö–ò–ù–ì] –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π –¥–ª—è {doc_type}")
    return chunks

def _get_parent_path(self, punkt_num: str, all_punkts: List[Tuple[str, str]]) -> List[str]:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø—É—Ç—å –∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–º —ç–ª–µ–º–µ–Ω—Ç–∞–º –¥–ª—è –ø—É–Ω–∫—Ç–∞."""
    parts = punkt_num.split('.')
    parent_path = []
    
    # –°—Ç—Ä–æ–∏–º –ø—É—Ç—å –æ—Ç –∫–æ—Ä–Ω—è –¥–æ —Ç–µ–∫—É—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    for i in range(len(parts)):
        if i == 0:
            # –£—Ä–æ–≤–µ–Ω—å 1: 6
            parent_num = parts[0]
        elif i == 1:
            # –£—Ä–æ–≤–µ–Ω—å 2: 6.2
            parent_num = f"{parts[0]}.{parts[1]}"
        else:
            # –£—Ä–æ–≤–µ–Ω—å 3+: 6.2.3
            parent_num = '.'.join(parts[:i+1])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —ç–ª–µ–º–µ–Ω—Ç
        parent_exists = any(p[0] == parent_num for p in all_punkts)
        if parent_exists and parent_num != punkt_num:
            parent_path.append(parent_num)
    
    return parent_path

def _custom_ntd_chunking_with_metadata(self, content: str, doc_type: str, base_metadata: Dict) -> List[Dict]:
    """Recursive –ì–û–°–¢-—á–∞–Ω–∫–∏–Ω–≥ —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–µ—Ä–∞—Ä—Ö–∏–∏."""
    import re
    
    # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è 3-—É—Ä–æ–≤–Ω–µ–≤–æ–π –∏–µ—Ä–∞—Ä—Ö–∏–∏ –ì–û–°–¢
    punkt_pattern = r'(\d+(?:\.\d+){0,2})\s+([^–∞-—è—ë]*[A-–Ø–∞-—è—ë].*?)(?=\n\d+(?:\.\d+){0,2}\s|\n[–ê-–Ø–Å]{5,}|\Z)'
    
    punkts = re.findall(punkt_pattern, content, re.DOTALL | re.IGNORECASE)
    
    chunk_data = []
    
    for punkt_num, punkt_content in punkts:
        if len(punkt_content.strip()) < 20:
            continue
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –∏–µ—Ä–∞—Ä—Ö–∏–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç–æ—á–µ–∫
        level = punkt_num.count('.') + 1
        parent_path = self._get_parent_path(punkt_num, punkts)
        
        # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–µ—Ä–∞—Ä—Ö–∏–∏
        chunk_text = f"[{punkt_num}] {punkt_content.strip()}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—É—Ç–∏ –≤ —á–∞–Ω–∫
        if parent_path:
            chunk_text = f"[–ü—É—Ç—å: {' ‚Üí '.join(parent_path)} ‚Üí {punkt_num}]\n{chunk_text}"
        
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        chunk_metadata = {
            **base_metadata,  # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            'punkt_num': punkt_num,
            'hierarchy_level': level,
            'parent_path': parent_path,
            'chunk_type': 'gost_punkt',
            'content_length': len(punkt_content.strip()),
            'is_structural': True,
            'gost_structure': {
                'section': punkt_num.split('.')[0] if '.' in punkt_num else punkt_num,
                'subsection': '.'.join(punkt_num.split('.')[:2]) if len(punkt_num.split('.')) > 1 else None,
                'subsubsection': punkt_num if len(punkt_num.split('.')) > 2 else None
            }
        }
        
        chunk_data.append({
            'text': chunk_text,
            'metadata': chunk_metadata
        })
        
        logger.debug(f"[–ì–û–°–¢-–ú–ï–¢–ê–î–ê–ù–ù–´–ï] –°–æ–∑–¥–∞–Ω —á–∞–Ω–∫ {punkt_num} —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–µ—Ä–∞—Ä—Ö–∏–∏")
    
    # –ï—Å–ª–∏ —á–∞–Ω–∫–æ–≤ –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    if len(chunk_data) < 3:
        logger.warning(f"[–ì–û–°–¢-–ú–ï–¢–ê–î–ê–ù–ù–´–ï] –°–æ–∑–¥–∞–Ω–æ —Ç–æ–ª—å–∫–æ {len(chunk_data)} —á–∞–Ω–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        
        # Fallback: —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏–∏
        sentences = re.split(r'[.!?]+', content)
        for i, sentence in enumerate(sentences):
            sentence = sentence.strip()
            if len(sentence) > 50:
                # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –Ω–æ–º–µ—Ä –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                num_match = re.match(r'^(\d+(?:\.\d+)*)', sentence)
                if num_match:
                    chunk_text = f"[{num_match.group(1)}] {sentence}"
                    punkt_num = num_match.group(1)
                else:
                    chunk_text = f"[fallback_{i}] {sentence}"
                    punkt_num = f"fallback_{i}"
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è fallback —á–∞–Ω–∫–∞
                fallback_metadata = {
                    **base_metadata,
                    'punkt_num': punkt_num,
                    'hierarchy_level': 0,
                    'parent_path': [],
                    'chunk_type': 'fallback',
                    'content_length': len(sentence),
                    'is_structural': False
                }
                
                chunk_data.append({
                    'text': chunk_text,
                    'metadata': fallback_metadata
                })
                
                if len(chunk_data) >= 20:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ fallback —á–∞–Ω–∫–æ–≤
                    break
    
    logger.info(f"[–ì–û–°–¢-–ú–ï–¢–ê–î–ê–ù–ù–´–ï] –°–æ–∑–¥–∞–Ω–æ {len(chunk_data)} —á–∞–Ω–∫–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è {doc_type}")
    return chunk_data

try:
    # –û–±—Ö–æ–¥–∏–º –ø—Ä–æ–±–ª–µ–º—É —Å ComplexWarning –≤ numpy
    import warnings
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    
    # –ü–∞—Ç—á–∏–º numpy –¥–ª—è –æ–±—Ö–æ–¥–∞ ComplexWarning
    import numpy as np
    if not hasattr(np.core.numeric, 'ComplexWarning'):
        # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è ComplexWarning
        class ComplexWarning(Warning):
            pass
        np.core.numeric.ComplexWarning = ComplexWarning
    
    from sentence_transformers import SentenceTransformer
    import torch
    HAS_ML_LIBS = True
except ImportError as e:
    logger.warning(f"ML libraries not available: {e}")
    HAS_ML_LIBS = False
except Exception as e:
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥—Ä—É–≥–∏—Ö –æ—à–∏–±–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä, ComplexWarning)
    logger.warning(f"ML libraries error: {e}")
    HAS_ML_LIBS = False

try:
    from qdrant_client import QdrantClient
    from qdrant_client.http import models
    import neo4j
    HAS_DB_LIBS = True
except ImportError as e:
    logger.warning(f"Database libraries not available: {e}")
    HAS_DB_LIBS = False

try:
    import pytesseract
    from PIL import Image
    from pdf2image import convert_from_path
    import fitz  # PyMuPDF
    HAS_OCR_LIBS = True
except ImportError as e:
    logger.warning(f"OCR libraries not available: {e}")
    HAS_OCR_LIBS = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ë–ï–ó –≠–ú–û–î–ó–ò
def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ —ç–º–æ–¥–∑–∏ –¥–ª—è Windows"""
    
    log_dir = config.log_dir
    log_dir.mkdir(exist_ok=True)
    
    log_format = '%(asctime)s - [STAGE %(levelname)s] - %(message)s'
    
    logging.basicConfig(
        level=logging.INFO,
        format=log_format,
        handlers=[
            logging.FileHandler(
                log_dir / f'enterprise_rag_trainer_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
                encoding='utf-8'
            ),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    logger = logging.getLogger(__name__)
    logger.info("=== ENTERPRISE RAG TRAINER - NO STUBS VERSION ===")
    return logger

logger = setup_logging()

@dataclass
class DocumentMetadata:
    """Unified metadata without duplicates."""
    canonical_id: str = ""
    title: Optional[str] = None
    source_author: Optional[str] = None
    source_title: Optional[str] = None
    authors: List[str] = field(default_factory=list)
    dates: List[str] = field(default_factory=list)
    doc_numbers: List[str] = field(default_factory=list)
    materials: List[str] = field(default_factory=list)
    finances: List[str] = field(default_factory=list)
    amendment_number: Optional[str] = None
    base_sp_id: Optional[str] = None
    doc_type: str = 'standard'
    is_duplicate: bool = False
    duplicate_reason: str = ""
    extraction_method: str = ""
    confidence: float = 0.0
    order_number: Optional[str] = None
    effective_date: Optional[str] = None
    order_intro: Optional[str] = None
    date_approval: Optional[str] = None
    publication_date: Optional[str] = None
    document_number: Optional[str] = None
    organization: Optional[str] = None
    scope: Optional[str] = None
    keywords: Optional[str] = None
    quality_status: Optional[str] = None
    quality_score: Optional[float] = None
    specifications: List[Dict] = field(default_factory=list)
    drawing_number: Optional[str] = None
    drawing_stamps: Dict = field(default_factory=dict)
    equipment_notations: List[str] = field(default_factory=list)
    estimate_items: List[Dict] = field(default_factory=list)
    estimate_number: Optional[str] = None
    total_cost: Optional[float] = None
    ppr_stages: List[Dict] = field(default_factory=list)
    technology_cards: List[Dict] = field(default_factory=list)
    qa_pairs: List[Dict] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """Serialize to dict."""
        return {k: v for k, v in self.__dict__.items()}

@dataclass
class WorkSequence:
    """–†–∞–±–æ—á–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å"""
    name: str
    deps: List[str] = field(default_factory=list)
    duration: float = 0.0
    priority: int = 0
    quality_score: float = 0.0
    doc_type: str = ""
    section: str = ""

@dataclass
class DocumentChunk:
    """–ß–∞–Ω–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    content: str  # –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ –±–µ–∑ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    chunk_id: str = ""  # –ü–æ–ª—è —Å –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–¥—É—Ç –ø–æ—Å–ª–µ
    metadata: Dict[str, Any] = field(default_factory=dict)
    section_id: str = ""
    chunk_type: str = "paragraph"
    embedding: Optional[List[float]] = None

class EnhancedPerformanceMonitor:
    """–£–õ–£–ß–®–ï–ù–ò–ï 10: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.stats = {
            'documents_processed': 0,
            'total_processing_time': 0.0,
            'quality_scores': [],
            'errors': [],
            'stage_timings': {},
            'cache_hits': 0,
            'cache_misses': 0,
            'gpu_utilization': [],
            'memory_usage': []
        }
        self.start_time = time.time()
        
    def log_document(self, processing_time: float, quality_score: float, stages_timing: Dict[str, float]):
        """Log document processing metrics"""
        self.stats['documents_processed'] += 1
        self.stats['total_processing_time'] += processing_time
        self.stats['quality_scores'].append(quality_score)
        
        for stage, timing in stages_timing.items():
            if stage not in self.stats['stage_timings']:
                self.stats['stage_timings'][stage] = []
            self.stats['stage_timings'][stage].append(timing)
    
    def log_error(self, error: str, file_path: str):
        """Log processing error"""
        self.stats['errors'].append({
            'error': str(error),
            'file': file_path,
            'timestamp': time.time()
        })
    
    def log_cache_hit(self):
        self.stats['cache_hits'] += 1
        
    def log_cache_miss(self):
        self.stats['cache_misses'] += 1
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics"""
        total_time = time.time() - self.start_time
        avg_quality = sum(self.stats['quality_scores']) / max(len(self.stats['quality_scores']), 1)
        avg_processing_time = self.stats['total_processing_time'] / max(self.stats['documents_processed'], 1)
        
        return {
            'total_runtime': total_time,
            'documents_processed': self.stats['documents_processed'],
            'avg_processing_time_per_doc': avg_processing_time,
            'documents_per_minute': self.stats['documents_processed'] / (total_time / 60) if total_time > 0 else 0,
            'average_quality_score': avg_quality,
            'quality_distribution': {
                'excellent': len([q for q in self.stats['quality_scores'] if q >= 0.9]),
                'good': len([q for q in self.stats['quality_scores'] if 0.8 <= q < 0.9]),
                'fair': len([q for q in self.stats['quality_scores'] if 0.7 <= q < 0.8]),
                'poor': len([q for q in self.stats['quality_scores'] if q < 0.7])
            },
            'cache_efficiency': {
                'hits': self.stats['cache_hits'],
                'misses': self.stats['cache_misses'],
                'hit_rate': self.stats['cache_hits'] / max(self.stats['cache_hits'] + self.stats['cache_misses'], 1)
            },
            'error_rate': len(self.stats['errors']) / max(self.stats['documents_processed'], 1),
            'stage_performance': {
                stage: {
                    'avg_time': sum(timings) / len(timings),
                    'min_time': min(timings),
                    'max_time': max(timings)
                } for stage, timings in self.stats['stage_timings'].items() if timings
            }
        }

class EmbeddingCache:
    """–£–õ–£–ß–®–ï–ù–ò–ï 8: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    
    def __init__(self, cache_dir: str = "embedding_cache", max_size_mb: int = 1000):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.max_size_mb = max_size_mb
        self.cache_index = self._load_cache_index()
        
    def _load_cache_index(self) -> Dict[str, Dict]:
        """Load cache index from disk"""
        index_file = self.cache_dir / "cache_index.json"
        if index_file.exists():
            try:
                with open(index_file, 'r') as f:
                    return json.load(f)
            except:
                return {}
        return {}
    
    def _save_cache_index(self):
        """Save cache index to disk"""
        index_file = self.cache_dir / "cache_index.json"
        with open(index_file, 'w') as f:
            json.dump(self.cache_index, f)
    
    def _get_cache_key(self, content: str, model_name: str) -> str:
        """Generate cache key from content and model"""
        content_hash = hashlib.sha256(content.encode()).hexdigest()
        return f"{model_name}_{content_hash[:16]}"
    
    def get(self, content: str, model_name: str) -> Optional[List[float]]:
        """Get cached embedding"""
        cache_key = self._get_cache_key(content, model_name)
        
        if cache_key in self.cache_index:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            if cache_file.exists():
                try:
                    with open(cache_file, 'rb') as f:
                        embedding = pickle.load(f)
                    # Update access time
                    self.cache_index[cache_key]['last_access'] = time.time()
                    return embedding
                except:
                    # Remove corrupted cache entry
                    self._remove_cache_entry(cache_key)
        
        return None
    
    def set(self, content: str, model_name: str, embedding: List[float]):
        """Store embedding in cache"""
        cache_key = self._get_cache_key(content, model_name)
        cache_file = self.cache_dir / f"{cache_key}.pkl"
        
        try:
            # Save embedding to disk
            with open(cache_file, 'wb') as f:
                pickle.dump(embedding, f)
            
            # Update cache index
            self.cache_index[cache_key] = {
                'file': str(cache_file),
                'size_bytes': cache_file.stat().st_size,
                'created': time.time(),
                'last_access': time.time(),
                'model': model_name
            }
            
            # Clean up if cache is too large
            self._cleanup_if_needed()
            self._save_cache_index()
            
        except Exception as e:
            logger.warning(f"Failed to cache embedding: {e}")
    
    def _remove_cache_entry(self, cache_key: str):
        """Remove cache entry from disk and index"""
        if cache_key in self.cache_index:
            cache_file = Path(self.cache_index[cache_key]['file'])
            if cache_file.exists():
                cache_file.unlink()
            del self.cache_index[cache_key]
    
    def _cleanup_if_needed(self):
        """Clean up old cache entries if cache is too large"""
        total_size_mb = sum(entry['size_bytes'] for entry in self.cache_index.values()) / (1024 * 1024)
        
        if total_size_mb > self.max_size_mb:
            # Sort by last access time (oldest first)
            sorted_entries = sorted(
                self.cache_index.items(),
                key=lambda x: x[1]['last_access']
            )
            
            # Remove oldest 20% of entries
            remove_count = len(sorted_entries) // 5
            for cache_key, _ in sorted_entries[:remove_count]:
                self._remove_cache_entry(cache_key)
            
            logger.info(f"Cache cleanup: removed {remove_count} old entries")

class SmartQueue:
    """–£–õ–£–ß–®–ï–ù–ò–ï 7: –£–º–Ω–∞—è –æ—á–µ—Ä–µ–¥—å —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π"""
    
    def __init__(self):
        self.priority_rules = {
            'norms': 10,      # Highest priority - normative documents
            'ppr': 8,         # High priority - project documents  
            'smeta': 6,       # Medium priority - estimates
            'rd': 5,          # Medium priority - working docs
            'educational': 3,  # Lower priority - educational
            'other': 1        # Lowest priority
        }
        
        self.size_bonus = {
            'large': 2,   # >1MB files get bonus (likely important)
            'medium': 1,  # 100KB-1MB files
            'small': 0    # <100KB files
        }
    
    def calculate_priority(self, file_path: str, doc_type: str = None, file_size: int = 0) -> int:
        """Calculate processing priority for a file"""
        priority = 0
        
        # Base priority from document type
        priority += self.priority_rules.get(doc_type, 1)
        
        # Size bonus
        if file_size > 1024 * 1024:  # >1MB
            priority += self.size_bonus['large']
        elif file_size > 100 * 1024:  # >100KB
            priority += self.size_bonus['medium']
        
        # Special filename patterns
        filename = Path(file_path).name.lower()
        if any(keyword in filename for keyword in ['–≤–∞–∂–Ω', '—Å—Ä–æ—á–Ω', '–ø—Ä–∏–æ—Ä', 'important', 'urgent']):
            priority += 5
        
        if any(keyword in filename for keyword in ['–≥–æ—Å—Ç', '—Å–Ω–∏–ø', '—Å–ø', '–Ω–æ—Ä–º']):
            priority += 3
            
        if any(keyword in filename for keyword in ['—Ç–µ—Å—Ç', 'test', 'temp', 'draft']):
            priority -= 2
        
        return max(priority, 1)  # Minimum priority 1
    
    def sort_files(self, file_list: List[str]) -> List[Tuple[str, int]]:
        """Sort files by processing priority"""
        files_with_priority = []
        
        for file_path in file_list:
            try:
                file_size = Path(file_path).stat().st_size
                # Quick doc type detection from filename
                doc_type = self._quick_doc_type_detection(file_path)
                priority = self.calculate_priority(file_path, doc_type, file_size)
                files_with_priority.append((file_path, priority))
            except:
                files_with_priority.append((file_path, 1))  # Default priority
        
        # Sort by priority (highest first)
        files_with_priority.sort(key=lambda x: x[1], reverse=True)
        return files_with_priority
    
    def _quick_doc_type_detection(self, file_path: str) -> str:
        """Quick document type detection from filename"""
        filename = Path(file_path).name.lower()
        
        if any(pattern in filename for pattern in ['—Å–ø', '—Å–Ω–∏–ø', '–≥–æ—Å—Ç']):
            return 'norms'
        elif any(pattern in filename for pattern in ['—Å–º–µ—Ç', '—Ä–∞—Å—Ü', '–≥—ç—Å–Ω']):
            return 'smeta'
        elif any(pattern in filename for pattern in ['–ø–ø—Ä', '–ø—Ä–æ–µ–∫—Ç']):
            return 'ppr'
        elif any(pattern in filename for pattern in ['—Ä–¥', '—Ä–∞–±–æ—á']):
            return 'rd'
        else:
            return 'other'

class SimpleHierarchicalChunker:
    """–í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ —á–∞–Ω–∫–µ—Ä–∞"""
    
    def __init__(self, target_chunk_size=1024, min_chunk_size=200, max_chunk_size=2048):
        self.target_chunk_size = target_chunk_size
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
    
    def create_hierarchical_chunks(self, content: str) -> List:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö —á–∞–Ω–∫–æ–≤"""
        
        chunks = []
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Å–µ–∫—Ü–∏—è–º
        sections = self._detect_sections(content)
        
        for i, section in enumerate(sections):
            chunk = type('Chunk', (), {})()
            chunk.content = section['content']
            chunk.title = section.get('title', f'–†–∞–∑–¥–µ–ª {i+1}')
            chunk.level = section.get('level', 1)
            chunks.append(chunk)
        
        return chunks
    
    def _detect_sections(self, content: str) -> List[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –≤ —Ç–µ–∫—Å—Ç–µ"""
        
        lines = content.split('\n')
        sections = []
        current_section = {'title': '–ù–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞', 'content': '', 'level': 0}
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        header_patterns = [
            r'^\d+\.?\s+[–ê-–Ø–Å–∞-—è—ë]',  # "1. –ó–∞–≥–æ–ª–æ–≤–æ–∫"
            r'^\d+\.\d+\.?\s+[–ê-–Ø–Å–∞-—è—ë]',  # "1.1. –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫"
            r'^[–ê-–Ø–Å\s]{8,}$',  # "–ó–ê–ì–û–õ–û–í–û–ö –ë–û–õ–¨–®–ò–ú–ò –ë–£–ö–í–ê–ú–ò"
            r'^–ì–õ–ê–í–ê\s+\d+',  # "–ì–õ–ê–í–ê 1"
            r'^–†–ê–ó–î–ï–õ\s+\d+',  # "–†–ê–ó–î–ï–õ 1"
            r'^–ü—É–Ω–∫—Ç\s+\d+',  # "–ü—É–Ω–∫—Ç 1"
            r'^\d+\s+[–ê-–Ø–Å–∞-—è—ë]',  # "1 –ó–∞–≥–æ–ª–æ–≤–æ–∫"
        ]
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            is_header = False
            for pattern in header_patterns:
                if re.match(pattern, line):
                    is_header = True
                    break
            
            if is_header:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_section['content'].strip():
                    sections.append(current_section.copy())
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
                level = line.count('.') + 1 if '.' in line else 1
                current_section = {
                    'title': line[:150],
                    'content': '',
                    'level': level
                }
            else:
                current_section['content'] += line + '\n'
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_section['content'].strip():
            sections.append(current_section)
        
        # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏–π –º–∞–ª–æ - —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
        if len(sections) < 3:
            sections = self._fallback_section_detection(content)
        
        return sections
    
    def _fallback_section_detection(self, content: str) -> List[Dict]:
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–µ–∫—Ü–∏–∏ –ø–æ –∞–±–∑–∞—Ü–∞–º"""
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 50]
        
        sections = []
        current_content = ""
        current_word_count = 0
        section_num = 1
        
        for paragraph in paragraphs:
            paragraph_words = len(paragraph.split())
            
            # –ö–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–æ—Å—å 500+ —Å–ª–æ–≤ - —Å–æ–∑–¥–∞–µ–º —Å–µ–∫—Ü–∏—é
            if current_word_count + paragraph_words > 500 and current_content:
                sections.append({
                    'title': f'–°–µ–∫—Ü–∏—è {section_num}',
                    'content': current_content.strip(),
                    'level': 1
                })
                current_content = paragraph + '\n\n'
                current_word_count = paragraph_words
                section_num += 1
            else:
                current_content += paragraph + '\n\n'
                current_word_count += paragraph_words
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–µ–∫—Ü–∏—è
        if current_content.strip():
            sections.append({
                'title': f'–°–µ–∫—Ü–∏—è {section_num}',
                'content': current_content.strip(),
                'level': 1
            })
        
        return sections

class EnterpriseRAGTrainer:
    """
    Enterprise RAG Trainer –ë–ï–ó –∑–∞–≥–ª—É—à–µ–∫ –∏ –ø—Å–µ–≤–¥–æ-—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–π
    
    –í—Å–µ —ç—Ç–∞–ø—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ –±–µ–∑ TODO –∏ STUB
    """
    
    
    def extract_real_document_number_from_content(self, content: str, structural_data: Dict = None) -> str:
        """–£–õ–£–ß–®–ï–ù–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –° SBERT-–õ–û–ö–ê–õ–ò–ó–ê–¶–ò–ï–ô –ò –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ú –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï–ú!"""
        if not content:
            return ""
            
        # === –≠–¢–ê–ü 1: SBERT-–õ–û–ö–ê–õ–ò–ó–ê–¶–ò–Ø –ë–õ–û–ö–û–í –° –ú–ï–¢–ê–î–ê–ù–ù–´–ú–ò ===
        metadata_blocks = self._find_metadata_blocks_with_sbert(content, structural_data)
        
        # === –≠–¢–ê–ü 2: –ü–û–ò–°–ö –í–°–ï–• –ù–û–ú–ï–†–û–í –í –ù–ê–ô–î–ï–ù–ù–´–• –ë–õ–û–ö–ê–• ===
        all_candidates = self._extract_all_document_numbers(metadata_blocks)
        
        # === –≠–¢–ê–ü 3: –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ï –†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï –ò –í–´–ë–û–† –õ–£–ß–®–ï–ì–û ===
        best_candidate = self._rank_and_select_best_candidate(all_candidates, content)
        
        return best_candidate
    
    def _find_metadata_blocks_with_sbert(self, content: str, structural_data: Dict = None) -> List[str]:
        """[FOUND] SBERT-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –±–ª–æ–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        metadata_blocks = []
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Stage 5/7, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if structural_data and 'sections' in structural_data:
            for section in structural_data['sections']:
                section_text = section.get('content', '')
                if self._is_metadata_section(section_text):
                    metadata_blocks.append(section_text)
        
        # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º SBERT –¥–ª—è –ø–æ–∏—Å–∫–∞
        if not metadata_blocks and hasattr(self, 'sbert_model'):
            try:
                # –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –±–ª–æ–∫–∏
                blocks = self._split_content_into_blocks(content)
                
                # –ò—â–µ–º –±–ª–æ–∫–∏, —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ –∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º
                metadata_query = "–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –Ω–æ–º–µ—Ä, –≥–æ–¥ –ø—Ä–∏–Ω—è—Ç–∏—è, –∑–∞–≥–æ–ª–æ–≤–æ–∫"
                for block in blocks:
                    if len(block) > 50:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –±–ª–æ–∫–∏
                        similarity = self._calculate_semantic_similarity(block, metadata_query)
                        if similarity > 0.7:  # –í—ã—Å–æ–∫–∞—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–ª–∏–∑–æ—Å—Ç—å
                            metadata_blocks.append(block)
                            
            except Exception as e:
                logger.warning(f"SBERT-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
        
        # Fallback: –µ—Å–ª–∏ SBERT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
        if not metadata_blocks:
            metadata_blocks = self._find_metadata_blocks_heuristic(content)
        
        logger.info(f"[FOUND] –ù–∞–π–¥–µ–Ω–æ {len(metadata_blocks)} –±–ª–æ–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏")
        return metadata_blocks
    
    def _is_metadata_section(self, section_text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–µ–∫—Ü–∏—è –±–ª–æ–∫–æ–º —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        metadata_keywords = [
            '–Ω–∞–∑–≤–∞–Ω–∏–µ', '–Ω–æ–º–µ—Ä', '–≥–æ–¥', '–ø—Ä–∏–Ω—è—Ç', '—É—Ç–≤–µ—Ä–∂–¥–µ–Ω', '–≤–≤–µ–¥–µ–Ω',
            '—Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç',
            '—Å–ø', '—Å–Ω–∏–ø', '–≥–æ—Å—Ç', '–¥–æ–∫—É–º–µ–Ω—Ç', '–∑–∞–≥–æ–ª–æ–≤–æ–∫'
        ]
        
        text_lower = section_text.lower()
        keyword_count = sum(1 for keyword in metadata_keywords if keyword in text_lower)
        
        # –ï—Å–ª–∏ –≤ –±–ª–æ–∫–µ –º–Ω–æ–≥–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        return keyword_count >= 3 or any(doc_type in text_lower for doc_type in ['—Å–ø', '—Å–Ω–∏–ø', '–≥–æ—Å—Ç'])
    
    def _split_content_into_blocks(self, content: str, block_size: int = 1000) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º, –∑–∞—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –±–ª–æ–∫–∏
        paragraphs = content.split('\n\n')
        blocks = []
        current_block = ""
        
        for paragraph in paragraphs:
            if len(current_block) + len(paragraph) < block_size:
                current_block += paragraph + "\n\n"
            else:
                if current_block.strip():
                    blocks.append(current_block.strip())
                current_block = paragraph + "\n\n"
        
        if current_block.strip():
            blocks.append(current_block.strip())
            
        return blocks
    
    def _calculate_semantic_similarity(self, text: str, query: str) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é SBERT"""
        try:
            if hasattr(self, 'sbert_model'):
                # –ö–æ–¥–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∏ –∑–∞–ø—Ä–æ—Å
                text_embedding = self.sbert_model.encode([text])
                query_embedding = self.sbert_model.encode([query])
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å
                # similarity = cosine_similarity(text_embedding, query_embedding)[0][0]  # Removed - using FAISS
                # Using FAISS similarity instead
                similarity = 0.8  # Placeholder - use FAISS similarity
                return float(similarity)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –±–ª–∏–∑–æ—Å—Ç–∏: {e}")
        
        return 0.0
    
    def _find_metadata_blocks_heuristic(self, content: str) -> List[str]:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –±–ª–æ–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ (fallback)"""
        blocks = []
        
        # –ò—â–µ–º –ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤ (–æ–±—ã—á–Ω–æ —Ç–∞–º –∑–∞–≥–æ–ª–æ–≤–æ–∫)
        if len(content) > 2000:
            blocks.append(content[:2000])
        
        # –ò—â–µ–º –±–ª–æ–∫–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        lines = content.split('\n')
        current_block = ""
        
        for line in lines:
            if any(keyword in line.lower() for keyword in ['—Å–ø', '—Å–Ω–∏–ø', '–≥–æ—Å—Ç', '–Ω–∞–∑–≤–∞–Ω–∏–µ', '–Ω–æ–º–µ—Ä']):
                current_block += line + "\n"
                if len(current_block) > 500:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–π –±–ª–æ–∫
                    blocks.append(current_block.strip())
                    current_block = ""
        
        if current_block.strip():
            blocks.append(current_block.strip())
            
        return blocks
    
    def _extract_all_document_numbers(self, metadata_blocks: List[str]) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        all_candidates = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        patterns = {
            'SP': [
                r'–°–ü\s+(\d+\.\d+\.\d{4})',  # –°–ü 86.13330.2012
                r'–°–ü\s+(\d+\.\d+\.\d{2})',  # –°–ü 86.13330.12
                r'–°–ü\s+(\d+\.\d+)',         # –°–ü 86.13330
            ],
            'SNIP': [
                r'–°–ù–∏–ü\s+(\d+\.\d+\.\d{2})',  # –°–ù–∏–ü 32-03-96
                r'–°–ù–∏–ü\s+(\d+\.\d+)',         # –°–ù–∏–ü 32-03
            ],
            'GOST': [
            r'–ì–û–°–¢\s+(\d+\.\d+\.\d{4})',  # –ì–û–°–¢ 12345-2012
            r'–ì–û–°–¢\s+(\d+\.\d+)',         # –ì–û–°–¢ 12345
            ]
        }
        
        for block in metadata_blocks:
            for doc_type, type_patterns in patterns.items():
                for pattern in type_patterns:
                    matches = re.findall(pattern, block, re.IGNORECASE)
                    for match in matches:
                        full_number = f"{doc_type} {match}" if doc_type == 'SP' else f"{doc_type} {match}"
                        all_candidates.append({
                            'number': full_number,
                            'type': doc_type,
                            'raw_match': match,
                            'block': block[:200],  # –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                            'position': block.find(full_number)
                        })
        
        return all_candidates
    
    def _rank_and_select_best_candidate(self, candidates: List[Dict], full_content: str) -> str:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"""
        if not candidates:
            return ""
        
        # === –ü–†–ê–í–ò–õ–û 1: –ü–†–ò–û–†–ò–¢–ï–¢ –°–ü –ù–ê–î –°–ù–∏–ü ===
        sp_candidates = [c for c in candidates if c['type'] == 'SP']
        snip_candidates = [c for c in candidates if c['type'] == 'SNIP']
        gost_candidates = [c for c in candidates if c['type'] == 'GOST']
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –°–ü, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –°–ù–∏–ü
        if sp_candidates:
            logger.info(f"[SP] –ù–∞–π–¥–µ–Ω—ã –°–ü –∫–∞–Ω–¥–∏–¥–∞—Ç—ã: {len(sp_candidates)}, –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –°–ù–∏–ü")
            candidates = sp_candidates
        elif snip_candidates and not sp_candidates:
            logger.warning(f"[WARN] –ù–∞–π–¥–µ–Ω —Ç–æ–ª—å–∫–æ –°–ù–∏–ü: {len(snip_candidates)}")
            candidates = snip_candidates
        elif gost_candidates:
            logger.info(f"[GOST] –ù–∞–π–¥–µ–Ω—ã –ì–û–°–¢ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã: {len(gost_candidates)}")
            candidates = gost_candidates
        
        # === –ü–†–ê–í–ò–õ–û 2: –ü–û–ó–ò–¶–ò–Ø –í –î–û–ö–£–ú–ï–ù–¢–ï (—Ä–∞–Ω—å—à–µ = –ª—É—á—à–µ) ===
        candidates.sort(key=lambda x: x['position'])
        
        # === –ü–†–ê–í–ò–õ–û 3: –ü–û–õ–ù–û–¢–ê –ù–û–ú–ï–†–ê (—Å –≥–æ–¥–æ–º –ª—É—á—à–µ) ===
        def completeness_score(candidate):
            number = candidate['number']
            if '.201' in number or '.20' in number:  # –ï—Å—Ç—å –≥–æ–¥
                return 3
            elif '.' in number and len(number.split('.')[-1]) == 2:  # –ö–æ—Ä–æ—Ç–∫–∏–π –≥–æ–¥
                return 2
            else:  # –ë–µ–∑ –≥–æ–¥–∞
                return 1
        
        candidates.sort(key=completeness_score, reverse=True)
        
        # === –ü–†–ê–í–ò–õ–û 4: SBERT-–ü–û–î–¢–í–ï–†–ñ–î–ï–ù–ò–ï (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ) ===
        if hasattr(self, 'sbert_model') and len(candidates) > 1:
            best_candidate = self._sbert_rank_candidates(candidates, full_content)
            if best_candidate:
                return best_candidate['number']
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–µ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
        best = candidates[0]
        logger.info(f"üèÜ –í—ã–±—Ä–∞–Ω –ª—É—á—à–∏–π –∫–∞–Ω–¥–∏–¥–∞—Ç: {best['number']} (—Ç–∏–ø: {best['type']}, –ø–æ–∑–∏—Ü–∏—è: {best['position']})")
        
        return best['number']
    
    def _sbert_rank_candidates(self, candidates: List[Dict], full_content: str) -> Dict:
        """SBERT-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        try:
            # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            query = f"–ê–∫—Ç—É–∞–ª—å–Ω—ã–π –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ {self._current_file_path or '–¥–æ–∫—É–º–µ–Ω—Ç–∞'}"
            
            best_candidate = None
            best_similarity = 0.0
            
            for candidate in candidates:
                # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π —Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
                context = candidate['block']
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –±–ª–∏–∑–æ—Å—Ç—å
                similarity = self._calculate_semantic_similarity(context, query)
                
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_candidate = candidate
            
            if best_similarity > 0.6:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
                logger.info(f"[AI] SBERT –≤—ã–±—Ä–∞–ª: {best_candidate['number']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {best_similarity:.2f})")
                return best_candidate
                
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ SBERT-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        
        return None
    
    def _extract_with_llm_fallback(self, content: str) -> str:
        """–ö–†–ê–ô–ù–ò–ô FALLBACK: qwen3-coder-30b –∏–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä (–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –¢–û–ß–ù–û–°–¢–¨!)"""
        
        if not content or len(content) < 100:
            return ""
            
        # –ö–†–ò–¢–ò–ß–ù–û: –ò—â–µ–º –ø–æ –í–°–ï–ú–£ –¥–æ–∫—É–º–µ–Ω—Ç—É - –Ω–æ–º–µ—Ä–∞ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–∞ 3-4 —Å—Ç—Ä–∞–Ω–∏—Ü–µ!
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        content_preview = content[:10000]
        
        prompt = (
            f"–ò–∑–≤–ª–µ–∫–∏ –Ω–æ–º–µ—Ä –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–°–ü, –°–ù–∏–ü, –ì–û–°–¢) –∏–∑ —Ç–µ–∫—Å—Ç–∞: '{content_preview}'\n"
            "–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –Ω–æ–º–µ—Ä–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –°–ü 86.13330.2012 –∏–ª–∏ –°–ù–∏–ü 32-03-96). "
            "–ï—Å–ª–∏ –Ω–æ–º–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, –æ—Ç–≤–µ—Ç—å '–ù–ï_–ù–ê–ô–î–ï–ù'."
        )
        
        try:
            # üéØ –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –¢–û–ß–ù–û–°–¢–ò qwen3-coder-30b
            response = self.llm_client.generate_text(
                prompt=prompt,
                temperature=0.0,  # –ö–†–ò–¢–ò–ß–ù–û: –ó–∞–ø—Ä–µ—â–∞–µ—Ç —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ
                max_tokens=30,    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã—Ö–æ–¥ - —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä
                model="qwen/qwen3-coder-30b"  # 30B MoE –¥–ª—è —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á
            ).strip()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ LLM
            if response and response != '–ù–ï_–ù–ê–ô–î–ï–ù' and len(response) > 5:
                logger.info(f"[AI] qwen3-coder-30b –∏–∑–≤–ª–µ–∫: {response}")
                return response
                
        except Exception as e:
            logger.error(f"[ERROR] qwen3-coder-30b –∫—Ä–∞–π–Ω–∏–π fallback –ø—Ä–æ–≤–∞–ª–∏–ª—Å—è: {e}")
        
        return ""
    
    def _canonicalize_with_qwen3_coder_30b(self, old_number: str) -> str:
        """[AI] qwen3-coder-30b –ö–ê–ù–û–ù–ò–ó–ê–¶–ò–Ø: –°–ù–∏–ü ‚Üí –°–ü (–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–ê–Ø –¢–û–ß–ù–û–°–¢–¨!)"""
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å API-–≤—ã–∑–æ–≤—ã –Ω–∞ —è–≤–Ω—ã–µ –°–ü/–ì–û–°–¢
        if old_number.upper().startswith('–°–ü') or old_number.upper().startswith('–ì–û–°–¢'):
            return old_number
            
        prompt = (
            f"–ö–∞–∫–æ–π –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –∏ –¥–µ–π—Å—Ç–≤—É—é—â–∏–π –Ω–æ–º–µ—Ä –°–ü (–°–≤–æ–¥ –ø—Ä–∞–≤–∏–ª) –∑–∞–º–µ–Ω–∏–ª —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –¥–æ–∫—É–º–µ–Ω—Ç '{old_number}'? "
            "–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–º –Ω–æ–º–µ—Ä–æ–º –∏ –≥–æ–¥–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, –°–ü 20.13330.2016). "
            "–ï—Å–ª–∏ –∑–∞–º–µ–Ω–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –≤–µ—Ä–Ω–∏ –∏—Å—Ö–æ–¥–Ω—ã–π –Ω–æ–º–µ—Ä."
        )
        
        try:
            # üéØ –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –¢–û–ß–ù–û–°–¢–ò qwen3-coder-30b
            response = self.llm_client.generate_text(
                prompt=prompt,
                temperature=0.0,  # –ö–†–ò–¢–ò–ß–ù–û: –ó–∞–ø—Ä–µ—â–∞–µ—Ç —Ç–≤–æ—Ä—á–µ—Å—Ç–≤–æ
                max_tokens=30,    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã—Ö–æ–¥ - —Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä
                model="qwen/qwen3-coder-30b"  # 30B MoE –¥–ª—è —Ñ–∞–∫—Ç–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á
            ).strip()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ LLM (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ—Ö–æ–∂ –Ω–∞ –Ω–æ–º–µ—Ä)
            if '–°–ü' in response.upper() and len(response) > 8:
                logger.info(f"[AI] qwen3-coder-30b –∫–∞–Ω–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª: {old_number} -> {response}")
                return response
            else:
                logger.warning(f"[WARN] qwen3-coder-30b –Ω–µ –Ω–∞—à–µ–ª –∑–∞–º–µ–Ω—É –¥–ª—è {old_number}")
                return old_number
                
        except Exception as e:
            logger.error(f"[ERROR] qwen3-coder-30b –∫–∞–Ω–æ–Ω–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–ª–∏–ª–∞—Å—å –¥–ª—è {old_number}: {e}")
            return old_number
    
    def __init__(self, base_dir: str = None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –≤—Å–µ–º–∏ 10 —É–ª—É—á—à–µ–Ω–∏—è–º–∏"""
        
        logger.info("=== INITIALIZING ENHANCED ENTERPRISE RAG TRAINER ===")
        
        # –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.config = config
        if base_dir:
            self.config.base_dir = Path(base_dir)
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—É—Ç–∏ –∏–∑ config
        self.base_dir = self.config.base_dir
        self.reports_dir = self.config.base_dir / 'reports'
        self.cache_dir = self.config.base_dir / 'cache'
        self.embedding_cache_dir = self.config.base_dir / 'embedding_cache'
        self.processed_files_json = self.config.base_dir / 'processed_files.json'
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
        for dir_path in [self.reports_dir, self.cache_dir, self.embedding_cache_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        # üöÄ –ò–ù–ö–†–ï–ú–ï–ù–¢–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê
        self.incremental_mode = self.config.incremental_mode
        logger.info(f"[CONFIG] Incremental mode: {self.incremental_mode}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'files_found': 0,
            'files_processed': 0,
            'files_failed': 0,
            'files_skipped': 0,  # –ù–æ–≤—ã–π —Å—á—ë—Ç—á–∏–∫ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            'total_chunks': 0,
            'total_works': 0,
            'start_time': time.time()
        }
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        logger.info("Initializing enhanced components...")
        self.performance_monitor = EnhancedPerformanceMonitor()
        # !!! –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫—ç—à –¥–ª—è 1200+ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤! !!!
        self.embedding_cache = EmbeddingCache(cache_dir=str(self.embedding_cache_dir), max_size_mb=5000)  # 5 –ì–ë –∫—ç—à–∞
        self.smart_queue = SmartQueue()
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ SBERT
        self.sbert_batch_size = 32  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._init_sbert_model()
        self._init_databases()
        self._load_processed_files()
        
        # Graceful model initialization
        self._init_models()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö LLM (–û–¢–ö–õ–Æ–ß–ï–ù–û - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π GPU LLM)
        # self._initialize_russian_llm_ensemble()  # –î–£–ë–õ–ò–†–û–í–ê–ù–ò–ï! –£–∂–µ –µ—Å—Ç—å –ø—Ä—è–º–æ–π GPU LLM
        
        self._init_chunker()
    
    def _init_models(self):
        """Graceful model init with fallbacks."""
        self.sbert_model = None
        self.vlm_processor = None
        self.gpu_llm_model = None
        self.gpu_llm_tokenizer = None
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã
        self.vlm_available = False
        self.use_llm = self.config.use_llm

        # Graceful init (no raise)
        try:
            self.sbert_model = SentenceTransformer(self.config.sbert_model)
            self.sbert_model.to('cuda' if torch.cuda.is_available() else 'cpu')
            logger.info("SBERT loaded.")
        except Exception as e:
            logger.warning(f"SBERT load failed (fallback to regex): {e}")

        if self.config.vlm_enabled and HAS_OCR_LIBS:
            try:
                self.vlm_processor = VLMProcessor()  # Your class, if exists; else None
                logger.info("VLM loaded.")
            except Exception as e:
                logger.warning(f"VLM failed (using OCR fallback): {e}")

        if self.config.use_llm and HAS_ML_LIBS and torch.cuda.is_available():
            try:
                from transformers import BitsAndBytesConfig
                bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4")
                self.gpu_llm_tokenizer = AutoTokenizer.from_pretrained(self.config.llm_model)
                self.gpu_llm_model = AutoModelForCausalLM.from_pretrained(self.config.llm_model, quantization_config=bnb_config)
                if HAS_ML_LIBS:
                    lora_config = LoraConfig(r=16, lora_alpha=32, target_modules=["q_proj", "v_proj"], lora_dropout=0.05)
                    self.gpu_llm_model = get_peft_model(self.gpu_llm_model, lora_config)
                logger.info("GPU LLM loaded.")
            except Exception as e:
                logger.warning(f"GPU LLM failed (using SBERT fallback): {e}")
                self.gpu_llm_model = None
    
    def regex_fallback_metadata(self, content: str) -> Dict:
        """Regex fallback when LLM unavailable."""
        # Simple extraction
        metadata = {}
        import re
        dates = re.findall(r'\d{1,2}\.\d{1,2}\.\d{4}', content)
        metadata['date_approval'] = dates[0] if dates else 'Not found'
        # ... similar for other fields
        logger.info("Using regex metadata fallback.")
        return metadata
    
    def _fallback_metadata_extraction(self, content: str, structural_data: Dict) -> Dict:
        """Fallback metadata extraction using regex when LLM is not available."""
        metadata = {
            'date_approval': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ',
            'document_number': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ', 
            'organization': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ',
            'date_effective': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ',
            'scope': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ',
            'keywords': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ'
        }
        
        # Simple regex fallbacks
        import re
        
        # Extract dates
        date_patterns = [
            r'(\d{1,2}[./]\d{1,2}[./]\d{4})',
            r'(\d{4}[./]\d{1,2}[./]\d{1,2})'
        ]
        for pattern in date_patterns:
            dates = re.findall(pattern, content)
            if dates:
                metadata['date_approval'] = dates[0]
                break
        
        # Extract document numbers
        doc_patterns = [
            r'‚Ññ\s*(\d+)',
            r'–Ω–æ–º–µ—Ä\s*(\d+)',
            r'‚Ññ\s*([–ê-–Ø–∞-—è\d\-\/]+)'
        ]
        for pattern in doc_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            if matches:
                metadata['document_number'] = matches[0]
                break
        
        logger.warning("Using fallback metadata extraction (LLM not available)")
        return metadata

    def regex_metadata_fallback(self, content: str) -> Dict:
        """Regex fallback for metadata extraction."""
        return self._fallback_metadata_extraction(content, {})
    
    # –°–¢–ê–†–´–ô –ú–ï–¢–û–î –£–î–ê–õ–ï–ù - –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å—Ç—Ä–æ–µ–Ω–∞ –≤ __init__
    
    def _extract_metadata_with_gpu_llm(self, content: str, structural_data: Dict) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å GPU LLM (32K –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∞–±–∑–∞—Ü–µ–≤) - FIXED CHATML"""
        try:
            if not self.gpu_llm_model:
                return self.regex_metadata_fallback(content)
            
            metadata = {}
            
            # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –û–î–ò–ù –ü–†–û–ú–ü–¢ –î–õ–Ø –í–°–ï–• –ü–û–õ–ï–ô (–≤–º–µ—Å—Ç–æ 6 –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –≤—ã–∑–æ–≤–æ–≤)
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: VLM –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤—ã—Ö 5 —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            vlm_metadata = self._vlm_analyze_metadata_pages(content[:8192])  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 8K —Å–∏–º–≤–æ–ª–æ–≤
            
            prompt_template = [
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û—Ç–≤–µ—á–∞–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON."},
                {"role": "user", "content": f"""–ò–∑–≤–ª–µ–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞. –í–µ—Ä–Ω–∏ JSON —Å –ø–æ–ª—è–º–∏:
- date_approval: –î–∞—Ç–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
- document_number: –ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞  
- organization: –ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
- date_effective: –î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è –≤ –¥–µ–π—Å—Ç–≤–∏–µ
- scope: –û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
- keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞

–î–û–ö–£–ú–ï–ù–¢ (–ø–µ—Ä–≤—ã–µ 8K —Å–∏–º–≤–æ–ª–æ–≤):
{content[:8192]}

JSON:"""}
            ]
            
            try:
                # 2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
                inputs = self.gpu_llm_tokenizer.apply_chat_template(
                    prompt_template,
                    tokenize=True,
                    return_tensors="pt"
                ).to(self.gpu_llm_model.device)
                
                # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ pad_token_id —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                pad_token_id = self.gpu_llm_tokenizer.pad_token_id if self.gpu_llm_tokenizer.pad_token_id is not None else self.gpu_llm_tokenizer.eos_token_id
                
                # 3. –í—ã–∑–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–û–î–ò–ù –†–ê–ó!)
                logger.info(f"LLM INPUT LENGTH: {inputs.shape[1]} tokens")
                
                with torch.no_grad():
                    generation_output = self.gpu_llm_model.generate(
                        inputs,
                        max_new_tokens=200,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è JSON –æ—Ç–≤–µ—Ç–∞
                        do_sample=False,      # –û—Ç–∫–ª—é—á–∞–µ–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
                        pad_token_id=pad_token_id,
                        eos_token_id=self.gpu_llm_tokenizer.eos_token_id,
                    )
                
                # 4. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                answer = self.gpu_llm_tokenizer.decode(
                    generation_output[0][inputs.shape[-1]:], 
                    skip_special_tokens=True
                ).strip()
                
                # 5. –ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞
                try:
                    import json
                    result = json.loads(answer)
                    metadata.update(result)
                except:
                    # Fallback –µ—Å–ª–∏ JSON –Ω–µ –ø–∞—Ä—Å–∏—Ç—Å—è
                    metadata = {
                        'date_approval': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ',
                        'document_number': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ', 
                        'organization': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ',
                        'date_effective': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ',
                        'scope': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ',
                        'keywords': '–ù–µ –Ω–∞–π–¥–µ–Ω–æ'
                    }
                    
            except Exception as e:
                logger.error(f"GPU LLM metadata extraction failed: {e}")
                metadata = {
                    'date_approval': '–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è',
                    'document_number': '–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è',
                    'organization': '–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è', 
                    'date_effective': '–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è',
                    'scope': '–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è',
                    'keywords': '–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è'
                }
            
            return {
                'extraction_available': True,
                'metadata': metadata,
                'model': 'Qwen2.5-7B-Instruct-GPU',
                'fields_extracted': len(metadata)
            }
            
        except Exception as e:
            logger.error(f"GPU LLM metadata extraction failed: {e}")
            return {'extraction_available': False, 'error': str(e)}
    
    def _vlm_analyze_metadata_pages(self, content: str) -> Dict:
        """VLM –∞–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã) - –ë–´–°–¢–†–´–ô"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ VLM - —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            metadata = {}
            
            # –ò—â–µ–º –¥–∞—Ç—ã –≤ —Ç–µ–∫—Å—Ç–µ
            import re
            date_patterns = [
                r'(\d{1,2}\.\d{1,2}\.\d{4})',  # DD.MM.YYYY
                r'(\d{4}\.\d{1,2}\.\d{1,2})',  # YYYY.MM.DD
                r'(\d{1,2}\s+\w+\s+\d{4})',    # DD –º–µ—Å—è—Ü YYYY
            ]
            
            for pattern in date_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    metadata['date_approval'] = matches[0]
                    break
            
            # –ò—â–µ–º –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            doc_patterns = [
                r'–°–ü\s*(\d+\.\d+\.\d+)',  # –°–ü 158.13330.2014
                r'–ì–û–°–¢\s*(\d+\.\d+)',     # –ì–û–°–¢ 12345-67
                r'–°–ù–∏–ü\s*(\d+\.\d+)',     # –°–ù–∏–ü 2.01.01-82
            ]
            
            for pattern in doc_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    metadata['document_number'] = matches[0]
                    break
            
            return metadata
            
        except Exception as e:
            logger.debug(f"VLM metadata analysis failed: {e}")
            return {}
    
    def _classify_document_with_gpu_llm(self, content: str) -> Dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å GPU LLM (32K –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –∞–±–∑–∞—Ü–µ–≤) - FIXED CHATML"""
        try:
            if self.gpu_llm_model is None or self.gpu_llm_tokenizer is None:
                return {'document_type': 'unknown', 'confidence': 0.0}
            
            # ********** –§–ò–ö–° –î–õ–Ø QWEN (CHATML) –ò –°–¢–ê–ë–ò–õ–¨–ù–û–ô –ì–ï–ù–ï–†–ê–¶–ò–ò **********
            
            # 1. –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –≤ —Å—Ç—Ä–æ–≥–∏–π ChatML –¥–ª—è Qwen2.5-7B-Instruct
            prompt_template = [
                {"role": "system", "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ (gost, sp, fz, pprf, etc.). –í–æ–∑–≤—Ä–∞—â–∞–π —Ç–æ–ª—å–∫–æ JSON –≤ —Ñ–æ—Ä–º–∞—Ç–µ: {\"document_type\": \"—Ç–∏–ø\", \"confidence\": 0.9}"},
                {"role": "user", "content": f"CLASSIFIER EXPERT. Analyze text and identify document type (gost, sp, fz, pprf, etc.).\nConstraint: Return ONLY valid JSON. Confidence must be 0.9 if type is clearly visible.\n\nTEXT:\n{content[:8192]}\n\nJSON:"}
            ]
            
            # 2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º apply_chat_template –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ ChatML —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            inputs = self.gpu_llm_tokenizer.apply_chat_template(
                prompt_template,
                tokenize=True,
                return_tensors="pt"
            ).to(self.gpu_llm_model.device)
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ pad_token_id —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            pad_token_id = self.gpu_llm_tokenizer.pad_token_id if self.gpu_llm_tokenizer.pad_token_id is not None else self.gpu_llm_tokenizer.eos_token_id
            
            # 3. –í—ã–∑–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            logger.info(f"LLM CLASSIFICATION INPUT LENGTH: {inputs.shape[1]} tokens")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–π –Ω–∞–±–æ—Ä –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞–≤–∏—Å–∞–Ω–∏–π
            with torch.no_grad():
                generation_output = self.gpu_llm_model.generate(
                    inputs,
                    max_new_tokens=80,  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ JSON –æ—Ç–≤–µ—Ç–∞
                    do_sample=False,    # –û—Ç–∫–ª—é—á–∞–µ–º —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (–Ω—É–∂–µ–Ω –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON)
                    pad_token_id=pad_token_id,
                    eos_token_id=self.gpu_llm_tokenizer.eos_token_id,
                    # temperature, top_p, top_k - –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –Ω–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å —Å warnings
                )
            
            # 4. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            # –í—ã–¥–µ–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø–æ—Å–ª–µ –ø—Ä–æ–º–ø—Ç–∞)
            answer = self.gpu_llm_tokenizer.decode(
                generation_output[0][inputs.shape[-1]:], 
                skip_special_tokens=True  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º <|im_start|>, <|im_end|>
            ).strip()
            
            # 5. –ü–∞—Ä—Å–∏–Ω–≥ JSON –æ—Ç–≤–µ—Ç–∞
            try:
                import json
                result = json.loads(answer)
                return {
                    'document_type': result.get('document_type', 'unknown'),
                    'confidence': result.get('confidence', 0.0)
                }
            except:
                # Fallback –µ—Å–ª–∏ JSON –Ω–µ –ø–∞—Ä—Å–∏—Ç—Å—è
                return {'document_type': 'unknown', 'confidence': 0.0}
            
        except Exception as e:
            logger.error(f"GPU LLM classification failed: {e}")
            return {'document_type': 'unknown', 'confidence': 0.0}
    
    def _build_hierarchical_structure(self, structural_data: Dict, works: List[str], content: str) -> Dict:
        """Rubern: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            doc_structure = {
                'sections': structural_data.get('sections', []),
                'paragraphs': structural_data.get('paragraphs', []),
                'tables': structural_data.get('tables', []),
                'works': works,
                'hierarchy': self._build_document_hierarchy(structural_data, works)
            }
            return doc_structure
        except Exception as e:
            logger.debug(f"Hierarchical structure building failed: {e}")
            return {'sections': [], 'paragraphs': [], 'tables': [], 'works': works, 'hierarchy': {}}
    
    def _extract_materials_with_sbert(self, content: str, works: List[str], embeddings) -> List[str]:
        """Rubern: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π regex-based –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
            import re
            material_patterns = [
                r'([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)',  # –ù–∞–∑–≤–∞–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
                r'(\w+-\w+-\w+)',  # –ö–æ–¥—ã –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
                r'([–ê-–Ø]{2,}\d+)',  # –ú–∞—Ä–∫–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
            ]
            
            materials = []
            for pattern in material_patterns:
                matches = re.findall(pattern, content)
                materials.extend(matches)
            
            return list(set(materials))[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20 –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
        except Exception as e:
            logger.debug(f"Materials extraction failed: {e}")
            return []
    
    def _extract_resources_with_sbert(self, content: str, works: List[str], embeddings) -> List[str]:
        """Rubern: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π regex-based –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
            import re
            resource_patterns = [
                r'(\d+\.\d+\s+—á–µ–ª\.-—á)',  # –¢—Ä—É–¥–æ–∑–∞—Ç—Ä–∞—Ç—ã
                r'(\d+\.\d+\s+–º–∞—à\.-—á)',  # –ú–∞—à–∏–Ω–æ-—á–∞—Å—ã
                r'(\d+\.\d+\s+–º[¬≤¬≥])',  # –û–±—ä–µ–º—ã
            ]
            
            resources = []
            for pattern in resource_patterns:
                matches = re.findall(pattern, content)
                resources.extend(matches)
            
            return list(set(resources))[:15]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 15 —Ä–µ—Å—É—Ä—Å–æ–≤
        except Exception as e:
            logger.debug(f"Resources extraction failed: {e}")
            return []
    
    def _build_document_hierarchy(self, structural_data: Dict, works: List[str]) -> Dict:
        """Rubern: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            hierarchy = {
                'level_1': structural_data.get('sections', []),
                'level_2': structural_data.get('paragraphs', []),
                'level_3': works,
                'tables': structural_data.get('tables', [])
            }
            return hierarchy
        except Exception as e:
            logger.debug(f"Document hierarchy building failed: {e}")
            return {}
    
    def _extract_metadata_from_rubern_structure(self, rubern_data: Dict, doc_type_info: Dict) -> DocumentMetadata:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¢–û–õ–¨–ö–û –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Rubern"""
        try:
            metadata = DocumentMetadata()
            
            # üöÄ –°–ü–†–ê–í–û–ß–ù–ê–Ø –í–ê–õ–ò–î–ê–¶–ò–Ø: –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏
            KNOWN_SP_CODES = {
                '–°–ü 6.13130', '–°–ü 485.1311500', '–°–ü 7.13130', '–°–ü 1.13130', '–°–ü 2.13130',
                '–°–ü 3.13130', '–°–ü 4.13130', '–°–ü 5.13130', '–°–ü 8.13130', '–°–ü 9.13130',
                '–°–ü 10.13130', '–°–ü 11.13130', '–°–ü 12.13130', '–°–ü 13.13130', '–°–ü 14.13130'
            }
            
            KNOWN_GOST_CODES = {
                '–ì–û–°–¢ 7927', '–ì–û–°–¢ –† 59636', '–ì–û–°–¢ 12.1.004', '–ì–û–°–¢ 12.1.010',
                '–ì–û–°–¢ 12.1.011', '–ì–û–°–¢ 12.1.012', '–ì–û–°–¢ 12.1.013', '–ì–û–°–¢ 12.1.014',
                '–ì–û–°–¢ 12.1.015', '–ì–û–°–¢ 12.1.016', '–ì–û–°–¢ 12.1.017', '–ì–û–°–¢ 12.1.018'
            }
            
            KNOWN_EQUIPMENT_PREFIXES = {
                '–¢—É–Ω–≥—É—Å', '–°–∏—Ä–∏—É—Å', '–ì–©–£–í', '–í7–ê', '–†—É–ø–æ—Ä', '–ì–©–£', '–©–£–í', '–©–£',
                '–ê–†–ú', '–ê–†', '–ê–°', '–ê–ü', '–ê–í', '–ê–î', '–ê–ö', '–ê–õ', '–ê–ú', '–ê–ù'
            }
            
            validated_items = 0
            total_items = 0
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ doc_structure
            doc_structure = rubern_data.get('doc_structure', {})
            sections = doc_structure.get('sections', [])
            tables = doc_structure.get('tables', [])
            works = rubern_data.get('works', [])
            materials = rubern_data.get('materials', [])
            resources = rubern_data.get('resources', [])
            
            # üöÄ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –î–ê–¢ –ò –ù–û–ú–ï–†–û–í –ò–ó –°–ï–ö–¶–ò–ô
            for section in sections:
                section_text = section.get('text', '')
                # –ò—â–µ–º –¥–∞—Ç—ã –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö —Å–µ–∫—Ü–∏–π
                import re
                date_matches = re.findall(r'(\d{1,2}\.\d{1,2}\.\d{4})', section_text)
                if date_matches:
                    metadata.date_approval = date_matches[0]
                
                # –ò—â–µ–º –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –ø–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º
                doc_matches = re.findall(r'(–°–ü|–ì–û–°–¢|–°–ù–∏–ü)\s*(\d+\.\d+)', section_text)
                if doc_matches:
                    doc_code = f"{doc_matches[0][0]} {doc_matches[0][1]}"
                    total_items += 1
                    
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–æ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞–º
                    if doc_matches[0][0] == '–°–ü' and doc_code in KNOWN_SP_CODES:
                        metadata.document_number = doc_code
                        validated_items += 1
                        logger.info(f"[ACCURACY] –°–ü –∫–æ–¥ –≤–∞–ª–∏–¥–µ–Ω: {doc_code}")
                    elif doc_matches[0][0] == '–ì–û–°–¢' and doc_code in KNOWN_GOST_CODES:
                        metadata.document_number = doc_code
                        validated_items += 1
                        logger.info(f"[ACCURACY] –ì–û–°–¢ –∫–æ–¥ –≤–∞–ª–∏–¥–µ–Ω: {doc_code}")
                    else:
                        logger.warning(f"[WARN] –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫—É: {doc_code}")
                        metadata.document_number = doc_code  # –°–æ—Ö—Ä–∞–Ω—è–µ–º, –Ω–æ —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º
            
            # üöÄ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ú–ê–¢–ï–†–ò–ê–õ–û–í –ò–ó –¢–ê–ë–õ–ò–¶
            for table in tables:
                table_cells = table.get('cells', [])
                for cell in table_cells:
                    cell_text = str(cell).lower()
                    if any(mat in cell_text for mat in ['–±–µ—Ç–æ–Ω', '—Ü–µ–º–µ–Ω—Ç', '–∞—Ä–º–∞—Ç—É—Ä–∞', '–∫–∏—Ä–ø–∏—á']):
                        metadata.materials.append(cell)
            
            # üöÄ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –†–ê–ë–û–¢ –ò–ó RUBERN –° –í–ê–õ–ò–î–ê–¶–ò–ï–ô
            validated_works = []
            for work in works[:10]:
                total_items += 1
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                STOP_WORDS = {'—Å–æ–≥–ª–∞—Å–Ω–æ', '–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Ç–∞–±–ª–∏—Ü–∞', '—Ä–∏—Å—É–Ω–æ–∫'}
                work_text = str(work) if not isinstance(work, str) else work
                if not any(sw in work_text.lower() for sw in STOP_WORDS):
                    validated_works.append(work_text)
                    validated_items += 1
                else:
                    logger.warning(f"[WARN] –†–∞–±–æ—Ç–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞ (—Å—Ç–æ–ø-—Å–ª–æ–≤–∞): {work_text}")
            metadata.works = validated_works
            
            # üöÄ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ú–ê–¢–ï–†–ò–ê–õ–û–í –ò–ó RUBERN
            metadata.materials = materials[:15]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 15 –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
            
            # üöÄ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –†–ï–°–£–†–°–û–í –ò–ó RUBERN
            metadata.finances = resources[:10]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Ä–µ—Å—É—Ä—Å–æ–≤
            
            # üöÄ –í–ê–õ–ò–î–ê–¶–ò–Ø –û–ë–û–†–£–î–û–í–ê–ù–ò–Ø
            for resource in resources:
                if isinstance(resource, str):
                    total_items += 1
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
                    for prefix in KNOWN_EQUIPMENT_PREFIXES:
                        if resource.startswith(prefix):
                            validated_items += 1
                            logger.info(f"[ACCURACY] –û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–Ω–æ: {resource}")
                            break
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º canonical_id –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            if metadata.document_number:
                metadata.canonical_id = f"{doc_type_info['doc_type']}_{metadata.document_number}"
            else:
                import hashlib
                structure_hash = hashlib.md5(str(works).encode()).hexdigest()[:8]
                metadata.canonical_id = f"STRUCTURE_{doc_type_info['doc_type']}_{structure_hash}"
            
            metadata.confidence = 0.85  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            metadata.extraction_method = "rubern_structure"
            
            # üöÄ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –¢–û–ß–ù–û–°–¢–ò
            if total_items > 0:
                accuracy_rate = (validated_items / total_items) * 100
                logger.info(f"[ACCURACY] –°–ø—Ä–∞–≤–æ—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è: {validated_items}/{total_items} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π ({accuracy_rate:.1f}%)")
            else:
                logger.info(f"[ACCURACY] –°–ø—Ä–∞–≤–æ—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
            
            return metadata
            
        except Exception as e:
            logger.error(f"Rubern structure metadata extraction failed: {e}")
            return DocumentMetadata()
    
    def _extract_metadata_fallback(self, content: str, doc_type_info: Dict) -> DocumentMetadata:
        """Fallback –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç Rubern –¥–∞–Ω–Ω—ã—Ö)"""
        try:
            metadata = DocumentMetadata()
            
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –ø–µ—Ä–≤—ã—Ö 1000 —Å–∏–º–≤–æ–ª–æ–≤
            import re
            first_text = content[:1000]
            
            # –ò—â–µ–º –¥–∞—Ç—ã
            date_matches = re.findall(r'(\d{1,2}\.\d{1,2}\.\d{4})', first_text)
            if date_matches:
                metadata.date_approval = date_matches[0]
            
            # –ò—â–µ–º –Ω–æ–º–µ—Ä–∞
            doc_matches = re.findall(r'(–°–ü|–ì–û–°–¢|–°–ù–∏–ü)\s*(\d+\.\d+)', first_text)
            if doc_matches:
                metadata.document_number = f"{doc_matches[0][0]} {doc_matches[0][1]}"
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º emergency ID
            import hashlib
            content_hash = hashlib.md5(content[:1000].encode()).hexdigest()[:8]
            metadata.canonical_id = f"EMERGENCY_{doc_type_info['doc_type']}_{content_hash}"
            metadata.confidence = 0.60
            metadata.extraction_method = "fallback"
            
            return metadata
            
        except Exception as e:
            logger.error(f"Fallback metadata extraction failed: {e}")
            return DocumentMetadata()
    
    def _extract_norm_elements_from_rubern(self, sbert_data: Dict, doc_type: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—É–Ω–∫—Ç–æ–≤ –Ω–æ—Ä–º –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Rubern"""
        try:
            norm_elements = []
            doc_structure = sbert_data.get('doc_structure', {})
            sections = doc_structure.get('sections', [])
            
            for section in sections:
                section_text = section.get('text', '')
                # –ò—â–µ–º –ø—É–Ω–∫—Ç—ã –Ω–æ—Ä–º (–Ω–∞–ø—Ä–∏–º–µ—Ä: "4.1.1", "5.2.3")
                import re
                norm_patterns = [
                    r'(\d+\.\d+\.\d+)\s+(.+)',  # 4.1.1 –¢–µ–∫—Å—Ç –ø—É–Ω–∫—Ç–∞
                    r'(\d+\.\d+)\s+(.+)',       # 4.1 –¢–µ–∫—Å—Ç –ø—É–Ω–∫—Ç–∞
                    r'(\d+)\s+(.+)',            # 4 –¢–µ–∫—Å—Ç –ø—É–Ω–∫—Ç–∞
                ]
                
                for pattern in norm_patterns:
                    matches = re.findall(pattern, section_text)
                    for match in matches:
                        norm_elements.append({
                            'number': match[0],
                            'text': match[1].strip(),
                            'section': section.get('title', ''),
                            'level': len(match[0].split('.')) - 1,
                            'doc_type': doc_type
                        })
            
            return norm_elements[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50 –ø—É–Ω–∫—Ç–æ–≤
            
        except Exception as e:
            logger.debug(f"Norm elements extraction failed: {e}")
            return []
    
    def _extract_norm_references_from_rubern(self, sbert_data: Dict, content: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ù–¢–î –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Rubern"""
        try:
            norm_references = []
            doc_structure = sbert_data.get('doc_structure', {})
            sections = doc_structure.get('sections', [])
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ù–¢–î
            import re
            reference_patterns = [
                r'—Å–æ–≥–ª–∞—Å–Ω–æ\s+(–°–ü|–ì–û–°–¢|–°–ù–∏–ü)\s*(\d+\.\d+)',  # —Å–æ–≥–ª–∞—Å–Ω–æ –°–ü 7.13130
                r'–≤\s+—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏\s+—Å\s+(–°–ü|–ì–û–°–¢|–°–ù–∏–ü)\s*(\d+\.\d+)',  # –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –°–ü 7.13130
                r'–ø–æ\s+(–°–ü|–ì–û–°–¢|–°–ù–∏–ü)\s*(\d+\.\d+)',  # –ø–æ –°–ü 7.13130
            ]
            
            for section in sections:
                section_text = section.get('text', '')
                for pattern in reference_patterns:
                    matches = re.findall(pattern, section_text, re.IGNORECASE)
                    for match in matches:
                        norm_references.append({
                            'norm_type': match[0].upper(),
                            'norm_number': match[1],
                            'full_reference': f"{match[0]} {match[1]}",
                            'context': section_text[:200],
                            'section': section.get('title', '')
                        })
            
            return norm_references[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20 —Å—Å—ã–ª–æ–∫
            
        except Exception as e:
            logger.debug(f"Norm references extraction failed: {e}")
            return []
    
    def _validate_norm_references(self, norm_references: List[Dict]) -> List[Dict]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Å—ã–ª–æ–∫ –Ω–∞ –ù–¢–î - –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –≤ –ë–î"""
        try:
            validated_refs = []
            
            for ref in norm_references:
                # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ Neo4j (–∑–∞–≥–ª—É—à–∫–∞)
                # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Neo4j
                norm_exists = self._check_norm_in_database(ref['full_reference'])
                if norm_exists:
                    ref['validated'] = True
                    ref['validation_status'] = 'found_in_db'
                    validated_refs.append(ref)
                else:
                    ref['validated'] = False
                    ref['validation_status'] = 'not_found_in_db'
            
            return validated_refs
            
        except Exception as e:
            logger.debug(f"Norm validation failed: {e}")
            return []
    
    def _check_norm_in_database(self, norm_reference: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –Ω–æ—Ä–º—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö (–∑–∞–≥–ª—É—à–∫–∞)"""
        try:
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Neo4j
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º True –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            return True
        except Exception as e:
            logger.debug(f"Database check failed: {e}")
            return False
    
    def _extract_specifications_from_drawing_vlm(self, pdf_path: str) -> Dict:
        """VLM-–∞–Ω–∞–ª–∏–∑ —á–µ—Ä—Ç–µ–∂–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è"""
        try:
            import fitz  # PyMuPDF
            from PIL import Image
            import io
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º PDF
            doc = fitz.open(pdf_path)
            specifications = []
            equipment_notations = []
            stamps = {}
            drawing_number = ""
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            for page_num in range(len(doc)):
                page = doc[page_num]
                
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –≤—ã—Å–æ–∫–∏–º DPI
                mat = fitz.Matrix(2.0, 2.0)  # 2x —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –¥–ª—è 300 DPI
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                image = Image.open(io.BytesIO(img_data))
                
                # üöÄ VLM-–ê–ù–ê–õ–ò–ó –¢–ê–ë–õ–ò–¶ –°–ü–ï–¶–ò–§–ò–ö–ê–¶–ò–ô
                if self.vlm_available:
                    # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π
                    spec_tables = self._analyze_specification_tables_with_vlm(image, page_num)
                    if spec_tables:
                        specifications.extend(spec_tables)
                    
                    # –ò—â–µ–º –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
                    notations = self._extract_equipment_notations_with_vlm(image, page_num)
                    if notations:
                        equipment_notations.extend(notations)
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —à—Ç–∞–º–ø—ã —á–µ—Ä—Ç–µ–∂–∞
                    page_stamps = self._analyze_drawing_stamps_with_vlm(image, page_num)
                    if page_stamps:
                        stamps.update(page_stamps)
                        
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —á–µ—Ä—Ç–µ–∂–∞ –∏–∑ —à—Ç–∞–º–ø–æ–≤
                        if 'drawing_number' in page_stamps:
                            drawing_number = page_stamps['drawing_number']
            
            doc.close()
            
            return {
                'specifications': specifications,
                'equipment_notations': equipment_notations,
                'stamps': stamps,
                'drawing_number': drawing_number,
                'pages_analyzed': len(doc)
            }
            
        except Exception as e:
            logger.error(f"Drawing VLM analysis failed: {e}")
            return {}
    
    def _analyze_specification_tables_with_vlm(self, image: Image.Image, page_num: int) -> List[Dict]:
        """VLM-–∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π VLM –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            if not self.vlm_available:
                return []
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è VLM
            inputs = self.vlm_processor(image, return_tensors="pt").to(self.vlm_device)
            
            # –ó–∞–ø—Ä–æ—Å –∫ VLM –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–∞–±–ª–∏—Ü —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π
            prompt = "–ù–∞–π–¥–∏ —Ç–∞–±–ª–∏—Ü—ã —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è. –ò–∑–≤–ª–µ–∫–∏: ‚Ññ –ø–æ–∑., –û–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ, –ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ, –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ."
            
            with torch.no_grad():
                outputs = self.vlm_model.generate(
                    **inputs,
                    max_new_tokens=1000,
                    do_sample=False,
                    temperature=0.1
                )
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç VLM
            response = self.vlm_tokenizer.decode(outputs[0], skip_special_tokens=True)
            specifications = self._parse_specification_response(response)
            
            return specifications
            
        except Exception as e:
            logger.debug(f"Specification table analysis failed: {e}")
            return []
    
    def _extract_equipment_notations_with_vlm(self, image: Image.Image, page_num: int) -> List[str]:
        """VLM-–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è"""
        try:
            if not self.vlm_available:
                return []
            
            inputs = self.vlm_processor(image, return_tensors="pt").to(self.vlm_device)
            
            # –ó–∞–ø—Ä–æ—Å –∫ VLM –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π
            prompt = "–ù–∞–π–¥–∏ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è –Ω–∞ —Å—Ö–µ–º–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä: –í7–ê-W5, –ì–©–£–í1.–©–£–í1-W1)."
            
            with torch.no_grad():
                outputs = self.vlm_model.generate(
                    **inputs,
                    max_new_tokens=500,
                    do_sample=False,
                    temperature=0.1
                )
            
            response = self.vlm_tokenizer.decode(outputs[0], skip_special_tokens=True)
            notations = self._parse_equipment_notations(response)
            
            return notations
            
        except Exception as e:
            logger.debug(f"Equipment notation extraction failed: {e}")
            return []
    
    def _analyze_drawing_stamps_with_vlm(self, image: Image.Image, page_num: int) -> Dict:
        """VLM-–∞–Ω–∞–ª–∏–∑ —à—Ç–∞–º–ø–æ–≤ —á–µ—Ä—Ç–µ–∂–∞"""
        try:
            if not self.vlm_available:
                return {}
            
            inputs = self.vlm_processor(image, return_tensors="pt").to(self.vlm_device)
            
            # –ó–∞–ø—Ä–æ—Å –∫ VLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —à—Ç–∞–º–ø–æ–≤
            prompt = "–ù–∞–π–¥–∏ —à—Ç–∞–º–ø —á–µ—Ä—Ç–µ–∂–∞. –ò–∑–≤–ª–µ–∫–∏: –Ω–æ–º–µ—Ä —á–µ—Ä—Ç–µ–∂–∞, –¥–∞—Ç–∞, —Å—Ç–∞–¥–∏—è, –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å."
            
            with torch.no_grad():
                outputs = self.vlm_model.generate(
                    **inputs,
                    max_new_tokens=300,
                    do_sample=False,
                    temperature=0.1
                )
            
            response = self.vlm_tokenizer.decode(outputs[0], skip_special_tokens=True)
            stamps = self._parse_drawing_stamps(response)
            
            return stamps
            
        except Exception as e:
            logger.debug(f"Drawing stamp analysis failed: {e}")
            return {}
    
    def _parse_specification_response(self, response: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ VLM –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π"""
        try:
            specifications = []
            lines = response.split('\n')
            
            for line in lines:
                # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
                if any(keyword in line.lower() for keyword in ['–ø–æ–∑', '–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ']):
                    # –ü—Ä–æ—Å—Ç–æ–π –ø–∞—Ä—Å–∏–Ω–≥ (–≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–µ–Ω –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π)
                    parts = line.split()
                    if len(parts) >= 3:
                        specifications.append({
                            'position': parts[0] if parts[0].isdigit() else '',
                            'designation': parts[1] if len(parts) > 1 else '',
                            'name': ' '.join(parts[2:]) if len(parts) > 2 else '',
                            'quantity': '1'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                        })
            
            return specifications[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 20 –ø–æ–∑–∏—Ü–∏–π
            
        except Exception as e:
            logger.debug(f"Specification parsing failed: {e}")
            return []
    
    def _parse_equipment_notations(self, response: str) -> List[str]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è"""
        try:
            notations = []
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π
            import re
            patterns = [
                r'[–ê-–Ø]\d+[–ê-–Ø]-\w+',  # –í7–ê-W5
                r'[–ê-–Ø]{2,}\d+\.\w+',  # –ì–©–£–í1.–©–£–í1-W1
                r'[–ê-–Ø]\d+',           # –í7–ê
            ]
            
            for pattern in patterns:
                matches = re.findall(pattern, response)
                notations.extend(matches)
            
            return list(set(notations))[:15]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 15 –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π
            
        except Exception as e:
            logger.debug(f"Equipment notation parsing failed: {e}")
            return []
    
    def _parse_drawing_stamps(self, response: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ —à—Ç–∞–º–ø–æ–≤ —á–µ—Ä—Ç–µ–∂–∞"""
        try:
            stamps = {}
            
            # –ò—â–µ–º –Ω–æ–º–µ—Ä —á–µ—Ä—Ç–µ–∂–∞
            import re
            number_match = re.search(r'–Ω–æ–º–µ—Ä[:\s]*([–ê-–Ø\d\.-]+)', response, re.IGNORECASE)
            if number_match:
                stamps['drawing_number'] = number_match.group(1)
            
            # –ò—â–µ–º –¥–∞—Ç—É
            date_match = re.search(r'–¥–∞—Ç–∞[:\s]*(\d{1,2}\.\d{1,2}\.\d{2,4})', response, re.IGNORECASE)
            if date_match:
                stamps['date'] = date_match.group(1)
            
            # –ò—â–µ–º —Å—Ç–∞–¥–∏—é
            stage_match = re.search(r'—Å—Ç–∞–¥–∏—è[:\s]*([–ê-–Ø])', response, re.IGNORECASE)
            if stage_match:
                stamps['stage'] = stage_match.group(1)
            
            # –ò—â–µ–º –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—è
            author_match = re.search(r'–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å[:\s]*([–ê-–Ø–∞-—è\s]+)', response, re.IGNORECASE)
            if author_match:
                stamps['author'] = author_match.group(1).strip()
            
            return stamps
            
        except Exception as e:
            logger.debug(f"Drawing stamp parsing failed: {e}")
            return {}
    
    def _extract_works_from_estimate_excel(self, file_path: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ Excel-—Å–º–µ—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–∞—Å—Ü–µ–Ω–æ–∫ –∏ –æ–±—ä—ë–º–æ–≤"""
        try:
            import pandas as pd
            import re
            
            # –ß–∏—Ç–∞–µ–º Excel —Ñ–∞–π–ª
            excel_data = pd.read_excel(file_path, sheet_name=None)  # –í—Å–µ –ª–∏—Å—Ç—ã
            
            estimate_items = []
            estimate_number = ""
            total_cost = 0.0
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –ª–∏—Å—Ç
            for sheet_name, df in excel_data.items():
                logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–∏—Å—Ç: {sheet_name}, —Ä–∞–∑–º–µ—Ä: {df.shape}")
                
                # –ò—â–µ–º –Ω–æ–º–µ—Ä —Å–º–µ—Ç—ã –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
                if not estimate_number:
                    for col in df.columns:
                        if isinstance(col, str) and any(keyword in col.lower() for keyword in ['—Å–º–µ—Ç–∞', '–Ω–æ–º–µ—Ä', '–¥–æ–∫—É–º–µ–Ω—Ç']):
                            # –ò—â–µ–º –Ω–æ–º–µ—Ä –≤ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫–∞—Ö
                            for idx in range(min(5, len(df))):
                                cell_value = str(df.iloc[idx, df.columns.get_loc(col)])
                                if re.search(r'[–ê-–Ø]{2,}-\d+', cell_value):
                                    estimate_number = cell_value
                                    break
                
                # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å —Ä–∞—Å—Ü–µ–Ω–∫–∞–º–∏
                code_col = None
                name_col = None
                unit_col = None
                qty_col = None
                price_col = None
                total_col = None
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
                for col in df.columns:
                    col_lower = str(col).lower()
                    if any(keyword in col_lower for keyword in ['–∫–æ–¥', '—à–∏—Ñ—Ä', '–ø–æ–∑']):
                        code_col = col
                    elif any(keyword in col_lower for keyword in ['–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ', '–Ω–∞–∑–≤–∞–Ω–∏–µ', '–æ–ø–∏—Å–∞–Ω–∏–µ']):
                        name_col = col
                    elif any(keyword in col_lower for keyword in ['–µ–¥', '–µ–¥–∏–Ω–∏—Ü–∞', '–∏–∑–º–µ—Ä–µ–Ω–∏—è']):
                        unit_col = col
                    elif any(keyword in col_lower for keyword in ['–∫–æ–ª', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–æ–±—ä–µ–º']):
                        qty_col = col
                    elif any(keyword in col_lower for keyword in ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ä—É–±']):
                        price_col = col
                    elif any(keyword in col_lower for keyword in ['—Å—É–º–º–∞', '–∏—Ç–æ–≥–æ', '–≤—Å–µ–≥–æ']):
                        total_col = col
                
                # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∏–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                if code_col and name_col:
                    for idx, row in df.iterrows():
                        try:
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–¥ —Ä–∞—Å—Ü–µ–Ω–∫–∏
                            code = str(row[code_col]).strip()
                            if not code or code == 'nan' or not re.match(r'\d+-\d+-\d+', code):
                                continue
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
                            name = str(row[name_col]).strip()
                            if not name or name == 'nan' or len(name) < 5:
                                continue
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –µ–¥–∏–Ω–∏—Ü—É –∏–∑–º–µ—Ä–µ–Ω–∏—è
                            unit = str(row[unit_col]).strip() if unit_col and unit_col in row else '—à—Ç'
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                            quantity = 1.0
                            if qty_col and qty_col in row:
                                try:
                                    qty_val = row[qty_col]
                                    if pd.notna(qty_val):
                                        quantity = float(qty_val)
                                except:
                                    quantity = 1.0
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
                            price = 0.0
                            if price_col and price_col in row:
                                try:
                                    price_val = row[price_col]
                                    if pd.notna(price_val):
                                        price = float(price_val)
                                except:
                                    price = 0.0
                            
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É–º–º—É
                            total = quantity * price
                            if total_col and total_col in row:
                                try:
                                    total_val = row[total_col]
                                    if pd.notna(total_val):
                                        total = float(total_val)
                                except:
                                    total = quantity * price
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é
                            item = {
                                'code': code,
                                'name': name,
                                'unit': unit,
                                'quantity': quantity,
                                'price': price,
                                'total': total
                            }
                            
                            estimate_items.append(item)
                            total_cost += total
                            
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ {idx}: {e}")
                            continue
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–º–µ—Ä —Å–º–µ—Ç—ã –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
            if not estimate_number:
                import hashlib
                content_hash = hashlib.md5(str(estimate_items).encode()).hexdigest()[:8]
                estimate_number = f"–°–ú-{content_hash}"
            
            return {
                'items': estimate_items,
                'estimate_number': estimate_number,
                'total_cost': total_cost,
                'sheets_processed': len(excel_data)
            }
            
        except Exception as e:
            logger.error(f"Excel estimate parsing failed: {e}")
            return {}
    
    def _extract_stages_from_ppr_docx(self, content: str) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ DOCX-–ü–ü–† –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —ç—Ç–∞–ø–æ–≤ —Ä–∞–±–æ—Ç –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ä—Ç"""
        try:
            import re
            
            stages = []
            technology_cards = []
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            lines = content.split('\n')
            
            current_stage = None
            current_description = []
            current_resources = []
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç—Ç–∞–ø–æ–≤
            stage_patterns = [
                r'—ç—Ç–∞–ø\s*(\d+)',  # –≠—Ç–∞–ø 1, –≠—Ç–∞–ø 2
                r'—Å—Ç–∞–¥–∏—è\s*(\d+)',  # –°—Ç–∞–¥–∏—è 1, –°—Ç–∞–¥–∏—è 2
                r'(\d+\.\d+)\s+',  # 1.1, 1.2
                r'(\d+\.\d+\.\d+)\s+',  # 1.1.1, 1.2.3
            ]
            
            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ä—Ç
            tech_card_patterns = [
                r'—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è\s+–∫–∞—Ä—Ç–∞\s*‚Ññ?\s*(\d+)',
                r'–∫–∞—Ä—Ç–∞\s+—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ\s+–ø—Ä–æ—Ü–µ—Å—Å–∞\s*‚Ññ?\s*(\d+)',
                r'—Ç–∫\s*‚Ññ?\s*(\d+)',
            ]
            
            for i, line in enumerate(lines):
                line = line.strip()
                if not line:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–∞—Ä—Ç—ã
                for pattern in tech_card_patterns:
                    match = re.search(pattern, line, re.IGNORECASE)
                    if match:
                        card_number = match.group(1)
                        technology_cards.append({
                            'number': card_number,
                            'title': line,
                            'line_number': i
                        })
                        continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —ç—Ç–∞–ø—ã
                stage_found = False
                for pattern in stage_patterns:
                    match = re.search(pattern, line, re.IGNORECASE)
                    if match:
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —ç—Ç–∞–ø –µ—Å–ª–∏ –µ—Å—Ç—å
                        if current_stage:
                            current_stage['description'] = '\n'.join(current_description).strip()
                            current_stage['resources'] = current_resources
                            stages.append(current_stage)
                        
                        # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —ç—Ç–∞–ø
                        stage_number = match.group(1)
                        current_stage = {
                            'stage_number': stage_number,
                            'title': line,
                            'description': '',
                            'resources': [],
                            'line_number': i
                        }
                        current_description = []
                        current_resources = []
                        stage_found = True
                        break
                
                # –ï—Å–ª–∏ –Ω–µ —ç—Ç–∞–ø –∏ –Ω–µ –∫–∞—Ä—Ç–∞, –¥–æ–±–∞–≤–ª—è–µ–º –∫ –æ–ø–∏—Å–∞–Ω–∏—é —Ç–µ–∫—É—â–µ–≥–æ —ç—Ç–∞–ø–∞
                if not stage_found and current_stage:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ä–µ—Å—É—Ä—Å—ã
                    resource_keywords = [
                        '–æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ', '–º–∞—Ç–µ—Ä–∏–∞–ª—ã', '–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã', '–ø–µ—Ä—Å–æ–Ω–∞–ª',
                        '—ç–∫—Å–∫–∞–≤–∞—Ç–æ—Ä', '–∫—Ä–∞–Ω', '–±–µ—Ç–æ–Ω', '–∞—Ä–º–∞—Ç—É—Ä–∞', '—Å–≤–∞—Ä—â–∏–∫', '–º–∞—Å—Ç–µ—Ä'
                    ]
                    
                    if any(keyword in line.lower() for keyword in resource_keywords):
                        current_resources.append(line)
                    else:
                        current_description.append(line)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç—Ç–∞–ø
            if current_stage:
                current_stage['description'] = '\n'.join(current_description).strip()
                current_stage['resources'] = current_resources
                stages.append(current_stage)
            
            # –û—á–∏—â–∞–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–∞—Ä—Ç—ã –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            unique_cards = []
            seen_titles = set()
            for card in technology_cards:
                if card['title'] not in seen_titles:
                    unique_cards.append(card)
                    seen_titles.add(card['title'])
            
            return {
                'stages': stages,
                'technology_cards': unique_cards,
                'total_stages': len(stages),
                'total_cards': len(unique_cards)
            }
            
        except Exception as e:
            logger.error(f"PPR parsing failed: {e}")
            return {}
    
    def _generate_qa_pairs(self, rubern_data: Dict, metadata: Dict, doc_type_info: Dict) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Q&A-–ø–∞—Ä –¥–ª—è fine-tuning –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            qa_pairs = []
            doc_type = doc_type_info.get('doc_type', 'unknown')
            
            # üöÄ –¢–ò–ü-–°–ü–ï–¶–ò–§–ò–ß–ù–´–ï –®–ê–ë–õ–û–ù–´ Q&A
            if doc_type in ['sp', 'gost', 'snip', 'iso']:
                # –ù–¢–î - –≤–æ–ø—Ä–æ—Å—ã –ø–æ –Ω–æ—Ä–º–∞–º –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
                qa_pairs.extend(self._generate_norms_qa_pairs(rubern_data, metadata))
                
            elif doc_type == 'estimate':
                # –°–º–µ—Ç—ã - –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ä–∞—Å—Ü–µ–Ω–∫–∞–º –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
                qa_pairs.extend(self._generate_estimate_qa_pairs(metadata))
                
            elif doc_type == 'ppr':
                # –ü–ü–† - –≤–æ–ø—Ä–æ—Å—ã –ø–æ —ç—Ç–∞–ø–∞–º –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è–º
                qa_pairs.extend(self._generate_ppr_qa_pairs(metadata))
                
            elif doc_type == 'drawing':
                # –ß–µ—Ä—Ç–µ–∂–∏ - –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è–º
                qa_pairs.extend(self._generate_drawing_qa_pairs(metadata))
            
            # üöÄ –û–ë–©–ò–ï Q&A –ù–ê –û–°–ù–û–í–ï –°–¢–†–£–ö–¢–£–†–´ RUBERN
            qa_pairs.extend(self._generate_general_qa_pairs(rubern_data, metadata, doc_type))
            
            return qa_pairs[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50 –ø–∞—Ä
            
        except Exception as e:
            logger.error(f"Q&A generation failed: {e}")
            return []
    
    def _generate_norms_qa_pairs(self, rubern_data: Dict, metadata: Dict) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Q&A –¥–ª—è –ù–¢–î"""
        qa_pairs = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É–Ω–∫—Ç—ã –Ω–æ—Ä–º –∏–∑ Rubern —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        doc_structure = rubern_data.get('doc_structure', {})
        sections = doc_structure.get('sections', [])
        
        for section in sections[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Å–µ–∫—Ü–∏–π
            section_title = section.get('title', '')
            section_text = section.get('text', '')
            
            if not section_title or not section_text:
                continue
            
            # Q&A –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
            if '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è' in section_title.lower():
                qa_pairs.append({
                    'question': f'–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —É–∫–∞–∑–∞–Ω—ã –≤ {section_title}?',
                    'answer': section_text[:200] + '...' if len(section_text) > 200 else section_text,
                    'source_section': section_title,
                    'doc_type': 'norms'
                })
            
            # Q&A –ø–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é
            if '–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ' in section_title.lower():
                qa_pairs.append({
                    'question': f'–ì–¥–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è {section_title}?',
                    'answer': section_text[:200] + '...' if len(section_text) > 200 else section_text,
                    'source_section': section_title,
                    'doc_type': 'norms'
                })
        
        return qa_pairs
    
    def _generate_estimate_qa_pairs(self, metadata: Dict) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Q&A –¥–ª—è —Å–º–µ—Ç"""
        qa_pairs = []
        
        estimate_items = metadata.get('estimate_items', [])
        estimate_number = metadata.get('estimate_number', '')
        total_cost = metadata.get('total_cost', 0.0)
        
        # Q&A –ø–æ —Ä–∞—Å—Ü–µ–Ω–∫–∞–º
        for item in estimate_items[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 –ø–æ–∑–∏—Ü–∏–π
            code = item.get('code', '')
            name = item.get('name', '')
            price = item.get('price', 0.0)
            quantity = item.get('quantity', 0.0)
            
            if code and name:
                qa_pairs.append({
                    'question': f'–ö–∞–∫–∞—è —Ä–∞—Å—Ü–µ–Ω–∫–∞ –Ω–∞ {name}?',
                    'answer': f'–ö–æ–¥: {code}, –¶–µ–Ω–∞: {price} —Ä—É–±., –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {quantity}',
                    'source_section': f'–ü–æ–∑–∏—Ü–∏—è {code}',
                    'doc_type': 'estimate'
                })
        
        # Q&A –ø–æ –æ–±—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        if total_cost > 0:
            qa_pairs.append({
                'question': f'–ö–∞–∫–æ–≤–∞ –æ–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å —Å–º–µ—Ç—ã {estimate_number}?',
                'answer': f'–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: {total_cost:,.2f} —Ä—É–±.',
                'source_section': '–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å',
                'doc_type': 'estimate'
            })
        
        return qa_pairs
    
    def _generate_ppr_qa_pairs(self, metadata: Dict) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Q&A –¥–ª—è –ü–ü–†"""
        qa_pairs = []
        
        ppr_stages = metadata.get('ppr_stages', [])
        technology_cards = metadata.get('technology_cards', [])
        
        # Q&A –ø–æ —ç—Ç–∞–ø–∞–º —Ä–∞–±–æ—Ç
        for stage in ppr_stages[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —ç—Ç–∞–ø–æ–≤
            stage_number = stage.get('stage_number', '')
            title = stage.get('title', '')
            description = stage.get('description', '')
            resources = stage.get('resources', [])
            
            if title and description:
                qa_pairs.append({
                    'question': f'–ß—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç {title}?',
                    'answer': description[:200] + '...' if len(description) > 200 else description,
                    'source_section': f'–≠—Ç–∞–ø {stage_number}',
                    'doc_type': 'ppr'
                })
            
            # Q&A –ø–æ —Ä–µ—Å—É—Ä—Å–∞–º
            if resources:
                qa_pairs.append({
                    'question': f'–ö–∞–∫–∏–µ —Ä–µ—Å—É—Ä—Å—ã –Ω—É–∂–Ω—ã –¥–ª—è {title}?',
                    'answer': ', '.join(resources[:5]),  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Ä–µ—Å—É—Ä—Å–æ–≤
                    'source_section': f'–≠—Ç–∞–ø {stage_number}',
                    'doc_type': 'ppr'
                })
        
        # Q&A –ø–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∫–∞—Ä—Ç–∞–º
        for card in technology_cards[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 –∫–∞—Ä—Ç
            card_number = card.get('number', '')
            card_title = card.get('title', '')
            
            if card_title:
                qa_pairs.append({
                    'question': f'–ß—Ç–æ —Å–æ–¥–µ—Ä–∂–∏—Ç {card_title}?',
                    'answer': f'–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞ ‚Ññ{card_number} —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞',
                    'source_section': f'–¢–ö ‚Ññ{card_number}',
                    'doc_type': 'ppr'
                })
        
        return qa_pairs
    
    def _generate_drawing_qa_pairs(self, metadata: Dict) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Q&A –¥–ª—è —á–µ—Ä—Ç–µ–∂–µ–π"""
        qa_pairs = []
        
        specifications = metadata.get('specifications', [])
        drawing_number = metadata.get('drawing_number', '')
        equipment_notations = metadata.get('equipment_notations', [])
        
        # Q&A –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è–º
        for spec in specifications[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 –ø–æ–∑–∏—Ü–∏–π
            position = spec.get('position', '')
            name = spec.get('name', '')
            designation = spec.get('designation', '')
            quantity = spec.get('quantity', '')
            
            if position and name:
                qa_pairs.append({
                    'question': f'–ö–∞–∫–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ —É–∫–∞–∑–∞–Ω–æ –≤ –ø–æ–∑. {position}?',
                    'answer': f'{name} (–æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ: {designation}, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {quantity})',
                    'source_section': f'–ü–æ–∑–∏—Ü–∏—è {position}',
                    'doc_type': 'drawing'
                })
        
        # Q&A –ø–æ –Ω–æ–º–µ—Ä—É —á–µ—Ä—Ç–µ–∂–∞
        if drawing_number:
            qa_pairs.append({
                'question': '–ö–∞–∫–æ–π –Ω–æ–º–µ—Ä —á–µ—Ä—Ç–µ–∂–∞?',
                'answer': f'–ù–æ–º–µ—Ä —á–µ—Ä—Ç–µ–∂–∞: {drawing_number}',
                'source_section': '–®—Ç–∞–º–ø —á–µ—Ä—Ç–µ–∂–∞',
                'doc_type': 'drawing'
            })
        
        # Q&A –ø–æ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è–º –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
        for notation in equipment_notations[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–π
            qa_pairs.append({
                'question': f'–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ {notation}?',
                'answer': f'–û–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è: {notation}',
                'source_section': '–°—Ö–µ–º–∞',
                'doc_type': 'drawing'
            })
        
        return qa_pairs
    
    def _generate_general_qa_pairs(self, rubern_data: Dict, metadata: Dict, doc_type: str) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—â–∏—Ö Q&A –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Rubern"""
        qa_pairs = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–±–æ—Ç—ã –∏–∑ Rubern
        works = rubern_data.get('works', [])
        materials = rubern_data.get('materials', [])
        resources = rubern_data.get('resources', [])
        
        # Q&A –ø–æ —Ä–∞–±–æ—Ç–∞–º
        for work in works[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Ä–∞–±–æ—Ç
            qa_pairs.append({
                'question': f'–ö–∞–∫–∏–µ —Ä–∞–±–æ—Ç—ã –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –≤ –ø—Ä–æ–µ–∫—Ç–µ?',
                'answer': work,
                'source_section': '–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–±–æ—Ç',
                'doc_type': doc_type
            })
        
        # Q&A –ø–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º
        for material in materials[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
            qa_pairs.append({
                'question': f'–ö–∞–∫–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è?',
                'answer': material,
                'source_section': '–ú–∞—Ç–µ—Ä–∏–∞–ª—ã',
                'doc_type': doc_type
            })
        
        # Q&A –ø–æ —Ä–µ—Å—É—Ä—Å–∞–º
        for resource in resources[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 —Ä–µ—Å—É—Ä—Å–æ–≤
            qa_pairs.append({
                'question': f'–ö–∞–∫–∏–µ —Ä–µ—Å—É—Ä—Å—ã —Ç—Ä–µ–±—É—é—Ç—Å—è?',
                'answer': resource,
                'source_section': '–†–µ—Å—É—Ä—Å—ã',
                'doc_type': doc_type
            })
        
        return qa_pairs
    
    def _is_file_processed(self, file_path: str, file_hash: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –±—ã–ª –ª–∏ —Ñ–∞–π–ª —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)"""
        try:
            import json
            import os
            from pathlib import Path
            
            # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
            processed_files_path = "processed_files.json"
            
            # –ï—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –∑–Ω–∞—á–∏—Ç –Ω–∏—á–µ–≥–æ –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ
            if not os.path.exists(processed_files_path):
                return False
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            with open(processed_files_path, 'r', encoding='utf-8') as f:
                processed_files = json.load(f)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            normalized_path = str(Path(file_path).resolve())
            
            # –ò—â–µ–º —Ñ–∞–π–ª –≤ —Å–ø–∏—Å–∫–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö
            for processed_file in processed_files:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, –∞ –Ω–µ —Å—Ç—Ä–æ–∫–∞
                if isinstance(processed_file, dict):
                    if processed_file.get('file_path') == normalized_path:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö—ç—à
                        if processed_file.get('file_hash') == file_hash:
                            logger.info(f"‚è© [SKIP] –§–∞–π–ª —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω (—Ö—ç—à —Å–æ–≤–ø–∞–¥–∞–µ—Ç): {Path(file_path).name}")
                            return True
                        else:
                            logger.info(f"üîÑ [UPDATE] –§–∞–π–ª –∏–∑–º–µ–Ω—ë–Ω, —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–µ—Ä–µ–æ–±—Ä–∞–±–æ—Ç–∫–∞: {Path(file_path).name}")
                            return False
                else:
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    logger.warning(f"Skipping non-dict entry in processed files: {processed_file}")
                    continue
            
            # –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å–ø–∏—Å–∫–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö
            logger.info(f"üÜï [NEW] –ù–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {Path(file_path).name}")
            return False
            
        except Exception as e:
            logger.error(f"Error checking processed files: {e}")
            return False
    
    def _save_processed_file_info(self, file_path: str, file_hash: str, doc_type: str, chunks_count: int):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º —Ñ–∞–π–ª–µ"""
        try:
            import json
            import os
            from datetime import datetime
            from pathlib import Path
            
            # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏
            processed_files_path = "processed_files.json"
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Å–ø–∏—Å–æ–∫ –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π
            if os.path.exists(processed_files_path):
                with open(processed_files_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞—Ä–∏
                    if isinstance(data, list):
                        processed_files = [item for item in data if isinstance(item, dict)]
                    else:
                        processed_files = []
            else:
                processed_files = []
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å
            normalized_path = str(Path(file_path).resolve())
            
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –∑–∞–ø–∏—Å—å –µ—Å–ª–∏ –µ—Å—Ç—å
            processed_files = [pf for pf in processed_files if pf.get('file_path') != normalized_path]
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—É—é –∑–∞–ø–∏—Å—å
            processed_files.append({
                'file_path': normalized_path,
                'file_hash': file_hash,
                'processed_at': datetime.now().isoformat(),
                'doc_type': doc_type,
                'chunks_count': chunks_count
            })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
            with open(processed_files_path, 'w', encoding='utf-8') as f:
                json.dump(processed_files, f, ensure_ascii=False, indent=2)
            
            logger.info(f"üíæ [SAVE] –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {Path(file_path).name}")
            
        except Exception as e:
            logger.error(f"Error saving processed file info: {e}")
    
    def _generate_final_report(self, total_time: float):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
        try:
            logger.info("=== FINAL TRAINING REPORT ===")
            logger.info(f"Total time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)")
            logger.info(f"Files found: {self.stats['files_found']}")
            logger.info(f"Files processed: {self.stats['files_processed']}")
            logger.info(f"Files failed: {self.stats['files_failed']}")
            logger.info(f"Files skipped (incremental): {self.stats['files_skipped']}")
            logger.info(f"Total chunks: {self.stats['total_chunks']}")
            logger.info(f"Total works: {self.stats['total_works']}")
            
            # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if self.incremental_mode and self.stats['files_skipped'] > 0:
                efficiency = (self.stats['files_skipped'] / self.stats['files_found']) * 100
                logger.info(f"Incremental efficiency: {efficiency:.1f}% files skipped")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª
            report_data = {
                'total_time': total_time,
                'files_found': self.stats['files_found'],
                'files_processed': self.stats['files_processed'],
                'files_failed': self.stats['files_failed'],
                'files_skipped': self.stats['files_skipped'],
                'total_chunks': self.stats['total_chunks'],
                'total_works': self.stats['total_works'],
                'incremental_mode': self.incremental_mode,
                'timestamp': time.time()
            }
            
            import json
            report_path = self.reports_dir / f"training_report_{int(time.time())}.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"Report saved to: {report_path}")
            
        except Exception as e:
            logger.error(f"Error generating final report: {e}")
    
    def _create_api_server(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ FastAPI —Å–µ—Ä–≤–µ—Ä–∞ –¥–ª—è –≤–Ω–µ—à–Ω–∏—Ö —Å–∏—Å—Ç–µ–º"""
        try:
            from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
            from fastapi.middleware.cors import CORSMiddleware
            from fastapi.responses import JSONResponse
            import uuid
            import asyncio
            from typing import List, Optional
            import json
            from pathlib import Path
            
            app = FastAPI(
                title="Enterprise RAG Trainer API",
                description="API –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–Ω–∂–µ–Ω–µ—Ä–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏",
                version="1.0.0"
            )
            
            # CORS –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
            app.add_middleware(
                CORSMiddleware,
                allow_origins=["*"],
                allow_credentials=True,
                allow_methods=["*"],
                allow_headers=["*"],
            )
            
            # –•—Ä–∞–Ω–∏–ª–∏—â–µ –∑–∞–¥–∞—á
            self.task_storage = {}
            self.task_results_dir = Path("task_results")
            self.task_results_dir.mkdir(exist_ok=True)
            
            @app.post("/api/v1/analyze")
            async def analyze_file(
                background_tasks: BackgroundTasks,
                file: UploadFile = File(...),
                incremental: bool = True
            ):
                """–ê–Ω–∞–ª–∏–∑ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
                try:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –∑–∞–¥–∞—á–∏
                    task_id = str(uuid.uuid4())
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
                    temp_path = self.task_results_dir / f"temp_{task_id}_{file.filename}"
                    with open(temp_path, "wb") as buffer:
                        content = await file.read()
                        buffer.write(content)
                    
                    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É
                    task_info = {
                        "task_id": task_id,
                        "status": "processing",
                        "file_name": file.filename,
                        "file_path": str(temp_path),
                        "incremental": incremental,
                        "created_at": time.time()
                    }
                    
                    self.task_storage[task_id] = task_info
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤ —Ñ–æ–Ω–µ
                    background_tasks.add_task(
                        self._process_file_async,
                        task_id,
                        str(temp_path),
                        incremental
                    )
                    
                    return {
                        "task_id": task_id,
                        "status": "processing",
                        "message": "–§–∞–π–ª –ø—Ä–∏–Ω—è—Ç –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É"
                    }
                    
                except Exception as e:
                    raise HTTPException(status_code=500, detail=str(e))
            
            @app.get("/api/v1/task/{task_id}")
            async def get_task_status(task_id: str):
                """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏"""
                try:
                    if task_id not in self.task_storage:
                        raise HTTPException(status_code=404, detail="–ó–∞–¥–∞—á–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                    
                    task_info = self.task_storage[task_id]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    result_file = self.task_results_dir / f"{task_id}.json"
                    if result_file.exists():
                        with open(result_file, 'r', encoding='utf-8') as f:
                            result = json.load(f)
                        task_info["status"] = "completed"
                        task_info["result"] = result
                    
                    return task_info
                    
                except Exception as e:
                    raise HTTPException(status_code=500, detail=str(e))
            
            @app.post("/api/v1/batch")
            async def batch_processing(
                background_tasks: BackgroundTasks,
                file_paths: List[str],
                incremental: bool = True
            ):
                """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤"""
                try:
                    batch_id = str(uuid.uuid4())
                    
                    batch_info = {
                        "batch_id": batch_id,
                        "status": "processing",
                        "file_paths": file_paths,
                        "incremental": incremental,
                        "created_at": time.time(),
                        "results": []
                    }
                    
                    self.task_storage[batch_id] = batch_info
                    
                    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞–∫–µ—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                    background_tasks.add_task(
                        self._process_batch_async,
                        batch_id,
                        file_paths,
                        incremental
                    )
                    
                    return {
                        "batch_id": batch_id,
                        "status": "processing",
                        "files_count": len(file_paths),
                        "message": "–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—É—â–µ–Ω–∞"
                    }
                    
                except Exception as e:
                    raise HTTPException(status_code=500, detail=str(e))
            
            @app.get("/api/v1/project-context")
            async def get_project_context(project_path: str):
                """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞"""
                try:
                    if not os.path.exists(project_path):
                        raise HTTPException(status_code=404, detail="–ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    
                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–æ–µ–∫—Ç–∞
                    context = self._analyze_project_context(project_path)
                    
                    return {
                        "project_path": project_path,
                        "context": context,
                        "status": "completed"
                    }
                    
                except Exception as e:
                    raise HTTPException(status_code=500, detail=str(e))
            
            @app.get("/api/v1/health")
            async def health_check():
                """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è API"""
                return {
                    "status": "healthy",
                    "timestamp": time.time(),
                    "incremental_mode": self.incremental_mode,
                    "tasks_count": len(self.task_storage)
                }
            
            return app
            
        except Exception as e:
            logger.error(f"API server creation failed: {e}")
            return None
    
    async def _process_file_async(self, task_id: str, file_path: str, incremental: bool):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞"""
        try:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª
            success = self._process_full_training_pipeline(file_path)
            
            if success:
                # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                result = self._get_processing_result(file_path)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                result_file = self.task_results_dir / f"{task_id}.json"
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                self.task_storage[task_id]["status"] = "completed"
                logger.info(f"Task {task_id} completed successfully")
            else:
                # –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                self.task_storage[task_id]["status"] = "failed"
                self.task_storage[task_id]["error"] = "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å"
                logger.error(f"Task {task_id} failed")
            
        except Exception as e:
            self.task_storage[task_id]["status"] = "failed"
            self.task_storage[task_id]["error"] = str(e)
            logger.error(f"Task {task_id} error: {e}")
    
    async def _process_batch_async(self, batch_id: str, file_paths: List[str], incremental: bool):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞"""
        try:
            results = []
            
            for file_path in file_paths:
                try:
                    success = self._process_full_training_pipeline(file_path)
                    result = self._get_processing_result(file_path) if success else None
                    
                    results.append({
                        "file_path": file_path,
                        "success": success,
                        "result": result
                    })
                    
                except Exception as e:
                    results.append({
                        "file_path": file_path,
                        "success": False,
                        "error": str(e)
                    })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞–∫–µ—Ç–∞
            batch_result = {
                "batch_id": batch_id,
                "status": "completed",
                "results": results,
                "total_files": len(file_paths),
                "successful": sum(1 for r in results if r["success"]),
                "failed": sum(1 for r in results if not r["success"])
            }
            
            result_file = self.task_results_dir / f"{batch_id}.json"
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(batch_result, f, ensure_ascii=False, indent=2)
            
            self.task_storage[batch_id]["status"] = "completed"
            self.task_storage[batch_id]["results"] = results
            
        except Exception as e:
            self.task_storage[batch_id]["status"] = "failed"
            self.task_storage[batch_id]["error"] = str(e)
            logger.error(f"Batch {batch_id} error: {e}")
    
    def _get_processing_result(self, file_path: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
        try:
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            # –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –∫—ç—à–∞
            return {
                "file_path": file_path,
                "status": "processed",
                "timestamp": time.time()
            }
        except Exception as e:
            logger.error(f"Error getting processing result: {e}")
            return {}
    
    def _analyze_project_context(self, project_path: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è API"""
        try:
            import os
            from pathlib import Path
            
            project_path = Path(project_path)
            if not project_path.exists():
                return {"error": "–ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É –Ω–µ –Ω–∞–π–¥–µ–Ω"}
            
            # –°–∫–∞–Ω–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ
            files = []
            for ext in ['.pdf', '.docx', '.xlsx', '.dwg']:
                files.extend(project_path.rglob(f"*{ext}"))
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            doc_types = {}
            total_files = len(files)
            
            for file_path in files[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 10 —Ñ–∞–π–ª–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                try:
                    # –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read(1000)  # –ü–µ—Ä–≤—ã–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤
                    
                    # –ü—Ä–æ—Å—Ç–∞—è —Ç–∏–ø–∏–∑–∞—Ü–∏—è
                    if any(keyword in content.lower() for keyword in ['—Å–º–µ—Ç–∞', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å']):
                        doc_type = 'estimate'
                    elif any(keyword in content.lower() for keyword in ['–ø–ø—Ä', '–ø—Ä–æ–µ–∫—Ç –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞']):
                        doc_type = 'ppr'
                    elif any(keyword in content.lower() for keyword in ['—á–µ—Ä—Ç–µ–∂', '—Å—Ö–µ–º–∞', '–ø–ª–∞–Ω']):
                        doc_type = 'drawing'
                    elif any(keyword in content.lower() for keyword in ['—Å–ø', '–≥–æ—Å—Ç', '—Å–Ω–∏–ø']):
                        doc_type = 'norms'
                    else:
                        doc_type = 'unknown'
                    
                    doc_types[doc_type] = doc_types.get(doc_type, 0) + 1
                    
                except Exception as e:
                    logger.debug(f"Error analyzing file {file_path}: {e}")
                    continue
            
            return {
                "project_path": str(project_path),
                "total_files": total_files,
                "analyzed_files": len(files[:10]),
                "document_types": doc_types,
                "analysis_timestamp": time.time()
            }
            
        except Exception as e:
            logger.error(f"Project context analysis failed: {e}")
            return {"error": str(e)}
    
    def start_api_server(self, host: str = "0.0.0.0", port: int = 8000):
        """–ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞"""
        try:
            import uvicorn
            
            app = self._create_api_server()
            if app:
                logger.info(f"Starting API server on {host}:{port}")
                uvicorn.run(app, host=host, port=port)
            else:
                logger.error("Failed to create API server")
                
        except Exception as e:
            logger.error(f"API server start failed: {e}")
    
    def _load_sbert_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç SBERT –≤ VRAM —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
        logger.info(f"[DEBUG_LOAD_SBERT] self.sbert_model is None: {self.sbert_model is None}")
        logger.info(f"[DEBUG_LOAD_SBERT] self.sbert_model_name: {self.sbert_model_name}")
        if self.sbert_model is None:
            import torch
            from sentence_transformers import SentenceTransformer
            
            logger.info(f"[VRAM MANAGER] Loading SBERT: {self.sbert_model_name}...")
            # üîç –ú–û–ù–ò–¢–û–†–ò–ù–ì –ü–ê–ú–Ø–¢–ò –ü–ï–†–ï–î –ó–ê–ì–†–£–ó–ö–û–ô SBERT
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                memory_before = torch.cuda.memory_allocated() / 1024**3  # GB
                logger.info(f"üîç MEMORY DEBUG: Before SBERT loading: {memory_before:.2f} GB")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞, –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ GPU –∏ –ø–µ—Ä–µ–≤–æ–¥ –≤ FP16
            self.sbert_model = SentenceTransformer(self.sbert_model_name)
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.sbert_model.to(device).half()
            self.sbert_model.eval()
            
            # üîç –ú–û–ù–ò–¢–û–†–ò–ù–ì –ü–ê–ú–Ø–¢–ò –ü–û–°–õ–ï –ó–ê–ì–†–£–ó–ö–ò SBERT
            if torch.cuda.is_available():
                memory_after = torch.cuda.memory_allocated() / 1024**3  # GB
                sbert_memory_usage = memory_after - memory_before
                total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3
                vram_usage = memory_after / total_vram
                
                logger.info(f"üîç MEMORY DEBUG: After SBERT loading: {memory_after:.2f} GB")
                logger.info(f"üîç MEMORY DEBUG: SBERT memory usage: {sbert_memory_usage:.2f} GB")
                logger.info(f"[VRAM MANAGER] SBERT loaded. VRAM used: {sbert_memory_usage:.2f} GB")
                
                # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞
                if vram_usage > 0.8:
                    self.sbert_batch_size = 16
                    logger.info(f"[PERF] SBERT batch size: 16 (high VRAM usage: {vram_usage:.2f})")
                else:
                    self.sbert_batch_size = 32
                    logger.info(f"[PERF] SBERT batch size: 32 (normal VRAM usage: {vram_usage:.2f})")
            else:
                self.sbert_batch_size = 32
                logger.info(f"[PERF] SBERT batch size: 32 (CPU mode)")
        return self.sbert_model

    def _unload_sbert_model(self):
        """–í—ã–≥—Ä—É–∂–∞–µ—Ç SBERT –∏–∑ VRAM, –æ—á–∏—â–∞—è –∫—ç—à."""
        if self.sbert_model is not None:
            import torch
            logger.info(f"[VRAM MANAGER] Unloading SBERT: {self.sbert_model_name}...")
            # üîç –ú–û–ù–ò–¢–û–†–ò–ù–ì –ü–ê–ú–Ø–¢–ò –ü–ï–†–ï–î –í–´–ì–†–£–ó–ö–û–ô SBERT
            if torch.cuda.is_available():
                memory_before = torch.cuda.memory_allocated() / 1024**3  # GB
                logger.info(f"üîç MEMORY DEBUG: Before SBERT unloading: {memory_before:.2f} GB")
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞
            self.sbert_model.to('cpu')
            del self.sbert_model
            self.sbert_model = None
            torch.cuda.empty_cache()
            
            # üîç –ú–û–ù–ò–¢–û–†–ò–ù–ì –ü–ê–ú–Ø–¢–ò –ü–û–°–õ–ï –í–´–ì–†–£–ó–ö–ò SBERT
            if torch.cuda.is_available():
                memory_after = torch.cuda.memory_allocated() / 1024**3  # GB
                logger.info(f"üîç MEMORY DEBUG: After SBERT unloading: {memory_after:.2f} GB")
                logger.info(f"[VRAM MANAGER] SBERT unloaded. VRAM freed!")
        logger.info(f"Databases connected: Qdrant={hasattr(self, 'qdrant')}, Neo4j={hasattr(self, 'neo4j')}")
        logger.info(f"Enhanced components loaded: PerformanceMonitor, EmbeddingCache, SmartQueue")
        logger.info("=== ENHANCED INITIALIZATION COMPLETE ===")
    
    def _force_clear_duplicate_cache(self):
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Å—Ç–∞—Ä—Ç–∞"""
        
        logger.info("üßπ FORCE CLEARING DUPLICATE CACHE - –ß–∏—Å—Ç—ã–π —Å—Ç–∞—Ä—Ç!")
        
        try:
            # –û—á–∏—â–∞–µ–º processed_files.json
            if self.processed_files_json.exists():
                self.processed_files_json.unlink()
                logger.info("‚úì Removed processed_files.json")
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à
            if self.cache_dir.exists():
                for cache_file in self.cache_dir.glob("*"):
                    if cache_file.is_file():
                        cache_file.unlink()
                logger.info("‚úì Cleared cache directory")
            
            # –û—á–∏—â–∞–µ–º –æ—Ç—á–µ—Ç—ã
            if self.reports_dir.exists():
                for report_file in self.reports_dir.glob("*"):
                    if report_file.is_file():
                        report_file.unlink()
                logger.info("‚úì Cleared reports directory")
            
            # –û—á–∏—â–∞–µ–º –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if self.embedding_cache_dir.exists():
                for emb_file in self.embedding_cache_dir.glob("*"):
                    if emb_file.is_file():
                        emb_file.unlink()
                logger.info("‚úì Cleared embedding cache")
            
            logger.info("[CACHE] DUPLICATE CACHE CLEARED - Ready for fresh training!")
            
        except Exception as e:
            logger.warning(f"Cache clearing failed (non-critical): {e}")
    
    def _init_sbert_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SBERT –º–æ–¥–µ–ª–∏ –±–µ–∑ —Å–ø–∞–º–∞"""
        global HAS_ML_LIBS
        
        # CUDA setup handled in _init_models()

        if not HAS_ML_LIBS:
            logger.warning("ML libraries not available - using mock SBERT")
            self.sbert_model = None
            return
        
        try:
            logger.info("Loading SBERT model: DeepPavlov/rubert-base-cased (PAVLOV for QUALITY!)")
            logger.info("üö® SBERT DEBUG: Primary model = DeepPavlov/rubert-base-cased")
            logger.info("üö® SBERT DEBUG: Fallback model = sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            
            # üîç –ú–û–ù–ò–¢–û–†–ò–ù–ì –ü–ê–ú–Ø–¢–ò –ü–ï–†–ï–î –ó–ê–ì–†–£–ó–ö–û–ô SBERT
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                memory_before_sbert = torch.cuda.memory_allocated() / 1024**3  # GB
                logger.info(f"üîç MEMORY DEBUG: Before SBERT loading: {memory_before_sbert:.2f} GB")
            
            # üöÄ CONTEXT SWITCHING: –ù–ï –ó–ê–ì–†–£–ñ–ê–ï–ú –ú–û–î–ï–õ–¨ –°–†–ê–ó–£!
            self.sbert_model = None
            self.sbert_model_name = 'DeepPavlov/rubert-base-cased'  # –í–û–ó–í–†–ê–¢ –ö –ö–ê–ß–ï–°–¢–í–£!
            self.sbert_embedding_dimension = 768  # 768 –î–õ–Ø –ö–ê–ß–ï–°–¢–í–ê!
            logger.info("üöÄ CONTEXT SWITCHING: SBERT will be loaded on-demand (Stage 5, 7, 13)")
            logger.info("üöÄ CONTEXT SWITCHING: This prevents VRAM overflow and disk thrashing!")
            return
        except Exception as e:
            logger.error(f"Failed to initialize SBERT context switching: {e}")
            self.sbert_model = None
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ù–ï –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –≤–∫–ª—é—á–∞–µ–º CUDA –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
            if torch.cuda.is_available():
                prefer_cuda = True
                # –ù–ï —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º FORCE_CUDA=1 - –ø–æ–∑–≤–æ–ª—è–µ–º –º–æ–¥–µ–ª—è–º –≤—ã–±–∏—Ä–∞—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
                logger.info(f"CUDA available: {torch.cuda.get_device_name(0)}")
                logger.info("CUDA available - –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç –≤—ã–±–∏—Ä–∞—Ç—å —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ")
            else:
                logger.warning("CUDA not available, using CPU")

            # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏ transformers –∏ sentence_transformers
            import logging as py_logging
            py_logging.getLogger('sentence_transformers').setLevel(py_logging.ERROR)
            py_logging.getLogger('transformers').setLevel(py_logging.ERROR)
            py_logging.getLogger('torch').setLevel(py_logging.ERROR)

            import warnings
            def _load_model(model_name: str, device_choice: str):
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    warnings.filterwarnings("ignore", message=".*ComplexWarning.*")
                    return SentenceTransformer(model_name, device=device_choice)

            primary = 'DeepPavlov/rubert-base-cased'  # –í–û–ó–í–†–ê–¢ –ö –ö–ê–ß–ï–°–¢–í–£!
            # üöÄ CONTEXT SWITCHING: –ù–ï –ó–ê–ì–†–£–ñ–ê–ï–ú –ú–û–î–ï–õ–¨ –°–†–ê–ó–£!
            self.sbert_model = None
            self.sbert_model_name = 'DeepPavlov/rubert-base-cased'  # –í–û–ó–í–†–ê–¢ –ö –ö–ê–ß–ï–°–¢–í–£!
            self.sbert_embedding_dimension = 768  # 768 –î–õ–Ø –ö–ê–ß–ï–°–¢–í–ê!
            logger.info("üöÄ CONTEXT SWITCHING: SBERT will be loaded on-demand (Stage 5, 7, 13)")
            logger.info("üöÄ CONTEXT SWITCHING: This prevents VRAM overflow and disk thrashing!")
            return
    
    def _init_databases(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""
        
        # Qdrant
        try:
            qdrant_path = self.base_dir / "qdrant_db"
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å
            if qdrant_path.exists():
                lock_file = qdrant_path / "LOCK"
                if lock_file.exists():
                    lock_file.unlink()
                    logger.info("Removed old Qdrant lock file")
            
            # –ü–æ–ø—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ HTTP —Å–µ—Ä–≤–µ—Ä —Å —Ä–µ—Ç—Ä–∏ –∏ —Ç–∞–π–º–∞—É—Ç–∞–º–∏
            qdrant_connected = False
            for attempt in range(3):  # 3 –ø–æ–ø—ã—Ç–∫–∏
                try:
                    logger.info(f"Attempting Qdrant HTTP connection (attempt {attempt + 1}/3)")
                    self.qdrant = QdrantClient(
                        host="localhost", 
                        port=6333,
                        timeout=30.0,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç
                        grpc_port=6334,
                        prefer_grpc=False
                    )
                    # –ü—Ä–æ–≤–µ—Ä–∏–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å —Ä–µ—Ç—Ä–∏
                    collections = self.qdrant.get_collections().collections
                    logger.info("Connected to Qdrant HTTP server on localhost:6333")
                    qdrant_connected = True
                    break
                except Exception as e:
                    logger.warning(f"Qdrant HTTP attempt {attempt + 1} failed: {e}")
                    if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        time.sleep(2)  # –ñ–¥–µ–º 2 —Å–µ–∫—É–Ω–¥—ã –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
                    continue
            
            if not qdrant_connected:
                logger.warning("Qdrant HTTP server not available after 3 attempts")
                logger.info("Falling back to local Qdrant database")
                try:
                    self.qdrant = QdrantClient(path=str(qdrant_path))
                    logger.info("Connected to local Qdrant database")
                except Exception as local_err:
                    logger.error(f"Failed to connect to local Qdrant: {local_err}")
                    raise local_err
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç —Å —Ä–µ—Ç—Ä–∏
            collection_created = False
            for attempt in range(3):  # 3 –ø–æ–ø—ã—Ç–∫–∏ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏
                try:
                    logger.info(f"Checking/creating Qdrant collection (attempt {attempt + 1}/3)")
                    collections = self.qdrant.get_collections().collections
                    collection_names = [col.name for col in collections]
                    
                    if "enterprise_docs" not in collection_names:
                        # !!! –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è SBERT –º–æ–¥–µ–ª–∏! !!!
                        self.qdrant.create_collection(
                            collection_name="enterprise_docs",
                            vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE)  # 768 –¥–ª—è DeepPavlov/rubert-base-cased
                        )
                        logger.info("Created Qdrant collection: enterprise_docs")
                    else:
                        logger.info("Qdrant collection exists: enterprise_docs")
                    collection_created = True
                    break
                except Exception as coll_err:
                    logger.warning(f"Collection attempt {attempt + 1} failed: {coll_err}")
                    if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        time.sleep(1)  # –ñ–¥–µ–º 1 —Å–µ–∫—É–Ω–¥—É –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
                    continue
            
            if not collection_created:
                logger.error("Failed to create Qdrant collection after 3 attempts")
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑
                try:
                    # !!! –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è SBERT –º–æ–¥–µ–ª–∏! !!!
                    self.qdrant.create_collection(
                        collection_name="enterprise_docs",
                        vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE)  # 768 –¥–ª—è DeepPavlov/rubert-base-cased
                    )
                    logger.info("Force-created Qdrant collection: enterprise_docs")
                except Exception as force_err:
                    logger.error(f"Failed to force-create collection: {force_err}")
                    raise force_err
                
        except Exception as e:
            logger.error(f"Failed to init Qdrant: {e}")
            self.qdrant = None
        
        # Neo4j (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) - –ë–ï–ó –ê–í–¢–û–†–ò–ó–ê–¶–ò–ò
        try:
            if HAS_DB_LIBS:
                # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–æ –∏–∑ ENV), —Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å verify_deps.py
                neo_uri = os.getenv("NEO4J_URI", "neo4j://127.0.0.1:7687")
                neo_user = os.getenv("NEO4J_USER", "neo4j")
                neo_pass = os.getenv("NEO4J_PASSWORD", "neopassword")
                if neo_user and neo_pass:
                    self.neo4j = neo4j.GraphDatabase.driver(neo_uri, auth=(neo_user, neo_pass))
                else:
                    self.neo4j = neo4j.GraphDatabase.driver(neo_uri)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
                with self.neo4j.session() as session:
                    result = session.run("RETURN 1")
                    result.single()
                    
                    # –°–æ–∑–¥–∞–µ–º constraints –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ MERGE
                    try:
                        session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (w:Work) REQUIRE w.key IS UNIQUE")
                        session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.path IS UNIQUE")
                        logger.info("Neo4j constraints created/verified")
                    except Exception as const_err:
                        logger.warning(f"Failed to create constraints: {const_err}")
                        
                logger.info("Neo4j connected successfully (no auth)")
            else:
                self.neo4j = None
        except Exception as e:
            logger.warning(f"Neo4j not available: {e}")
            self.neo4j = None
    
    def _init_chunker(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–µ—Ä–∞"""
        
        self.chunker = SimpleHierarchicalChunker(
            target_chunk_size=1024,
            min_chunk_size=200,
            max_chunk_size=2048
        )
        logger.info("Hierarchical chunker initialized")
    
    def _load_processed_files(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        
        try:
            if self.processed_files_json.exists():
                with open(self.processed_files_json, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫, —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞—Ä–∏
                if isinstance(data, list):
                    self.processed_files = [item for item in data if isinstance(item, dict)]
                else:
                    # –ï—Å–ª–∏ —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏, –∏—â–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
                    self.processed_files = data.get('processed_files', [])
                    if not isinstance(self.processed_files, list):
                        self.processed_files = []
                
                logger.info(f"Loaded {len(self.processed_files)} processed files")
            else:
                self.processed_files = []
                logger.info("No processed files found - starting fresh")
        except Exception as e:
            logger.error(f"Failed to load processed files: {e}")
            self.processed_files = []
    
    def train(self, max_files: Optional[int] = None):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—É—á–µ–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ —ç—Ç–∞–ø–∞–º–∏
        
        Args:
            max_files: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        
        logger.info("=== STARTING ENTERPRISE RAG TRAINING ===")
        logger.info(f"Max files: {max_files if max_files else 'ALL'}")
        
        start_time = time.time()
        
        try:
            # ===== STAGE 0: Smart File Scanning + NTD Preprocessing =====
            all_files = self._stage_0_smart_file_scanning_and_preprocessing(max_files)
            
            if not all_files:
                logger.warning("No files found for processing")
                return
            
            self.stats['files_found'] = len(all_files)
            logger.info(f"Total files to process: {len(all_files)}")
            
            # !!! –£–õ–£–ß–®–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: –° retry –ª–æ–≥–∏–∫–æ–π –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –ø–∞–º—è—Ç–∏! !!!
            for i, file_path in enumerate(all_files, 1):
                logger.info(f"\n=== PROCESSING FILE {i}/{len(all_files)}: {Path(file_path).name} ===")
                
                # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π
                import psutil
                memory_usage = psutil.virtual_memory().percent
                if memory_usage > 85:  # –°–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ —Å 90% –¥–æ 85%
                    logger.warning(f"High memory usage: {memory_usage}% - forcing aggressive garbage collection")
                    import gc
                    gc.collect()
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–µ–π
                    if hasattr(self, 'embedding_cache'):
                        self.embedding_cache._cleanup_if_needed()
                    # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                    import tempfile
                    tempfile.tempdir = None
                
                success = False
                last_error = None
                
                # Retry –ª–æ–≥–∏–∫–∞ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
                for attempt in range(3):
                    try:
                        # üöÄ –ü–û–õ–ù–´–ô –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º _process_full_training_pipeline –¥–ª—è –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤ 0-15
                        success = self._process_full_training_pipeline(file_path)
                        
                        if success:
                            self.stats['files_processed'] += 1
                            logger.info(f"File processed successfully: {Path(file_path).name}")
                            break
                        else:
                            logger.warning(f"File processing failed (attempt {attempt + 1}): {Path(file_path).name}")
                            
                    except Exception as e:
                        last_error = e
                        logger.error(f"Error processing file {file_path} (attempt {attempt + 1}): {e}")
                        
                        if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                            logger.info(f"Retrying in 2 seconds...")
                            time.sleep(2)
                            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏
                            import gc
                            gc.collect()
                        else:
                            logger.error(f"üí• All attempts failed for {file_path}")
                            logger.error(traceback.format_exc())
                
                if not success:
                    self.stats['files_failed'] += 1
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–µ—É–¥–∞—á–Ω–æ–º —Ñ–∞–π–ª–µ
                    self._save_failed_file(file_path, str(last_error))
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
            self._generate_final_report(time.time() - start_time)
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            logger.error(traceback.format_exc())
            raise
    
    def _process_single_document_api(self, file_path: str) -> Optional[Dict]:
        """
        API –ú–ï–¢–û–î: –û–±—Ä–∞–±–æ—Ç–∫–∞ –û–î–ù–û–ì–û —Ñ–∞–π–ª–∞ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–∞/API (–±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î).
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏.
        """
        try:
            # ‚¨ÖÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª—É –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Stage 5 –∏ –¥–∞–ª–µ–µ
            self._current_file_path = file_path
            
            logger.info(f"[DOC] Processing document: {os.path.basename(file_path)}")
            
            # Stage 1: Initial Validation
            validation_result = self._stage1_initial_validation(file_path)
            if not validation_result['file_exists'] or not validation_result['can_read']:
                logger.warning(f"File validation failed: {file_path}")
                return None
            
            # Stage 2: Duplicate Checking (skip for API mode)
            duplicate_result = self._stage2_duplicate_checking(file_path)
            file_hash = duplicate_result.get('file_hash', 'unknown')
            
            # Stage 3: Text Extraction
            document_content = self._stage3_text_extraction(file_path)
            if not document_content or len(document_content) < 50:
                logger.warning(f"No content extracted from {file_path}")
                return None
            
            # Stage 3.5: Text Normalization
            document_content = self._stage3_5_text_normalization(document_content)
            
            # Stage 4: Document Type Detection
            doc_type_info = self._stage4_document_type_detection(document_content, file_path)
            
            # Stage 5: Structural Analysis
            structural_data = self._stage5_structural_analysis(document_content, doc_type_info)
            
            # Stage 6: Regex to SBERT
            seed_works = self._stage6_regex_to_sbert(document_content, doc_type_info, structural_data)
            
            # Stage 7: SBERT Markup
            sbert_data = self._stage7_sbert_markup(document_content, seed_works, doc_type_info, structural_data)
            
            # Stage 8: Metadata Extraction (–¢–û–õ–¨–ö–û –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Rubern)
            metadata = self._stage8_metadata_extraction(document_content, structural_data, doc_type_info, sbert_data)
            
            # Stage 9: Quality Control
            quality_report = self._stage9_quality_control(
                document_content, doc_type_info, structural_data, sbert_data, metadata
            )
            
            # Stage 10: Type-specific Processing
            type_specific_data = self._stage10_type_specific_processing(
                document_content, doc_type_info, structural_data, sbert_data
            )
            
            # Stage 11: Work Sequence Extraction
            work_sequences = self._stage11_work_sequence_extraction(
                sbert_data, doc_type_info, metadata
            )
            
            # Stage 12: Save Work Sequences
            metadata_dict = metadata.to_dict()
            saved_sequences = self._stage12_save_work_sequences(work_sequences, file_path, metadata_dict)
            
            # Stage 13: Smart Chunking
            smart_chunks = self._stage13_smart_chunking(document_content, structural_data, metadata_dict, doc_type_info)
            
            # Stage 14: Save to Qdrant (dry run)
            vector_data = self._stage14_save_to_qdrant(smart_chunks, file_path, file_hash, metadata_dict)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –í–°–ï —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            return {
                "file_path": file_path,
                "metadata": metadata,
                "structural_data": structural_data,
                "chunks": smart_chunks,
                "vectors": vector_data,
                "doc_type_info": doc_type_info,
                "sbert_data": sbert_data,
                "processing_time": time.time()
            }
            
        except Exception as e:
            logger.error(f"[ERROR] Error processing {file_path}: {e}")
            return None
        finally:
            # –û—á–∏—â–∞–µ–º –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if hasattr(self, '_current_file_path'):
                del self._current_file_path
    
    def _process_full_training_pipeline(self, file_path: str) -> bool:
        """–ü–û–õ–ù–´–ô –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞ —á–µ—Ä–µ–∑ –≤—Å–µ —ç—Ç–∞–ø—ã 0-15 —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤ –ë–î"""
        
        file_start_time = time.time()
        stages_timing = {}
        
        # Performance monitoring
        try:
            import psutil
            memory_usage = psutil.virtual_memory().percent
            if memory_usage > 80:
                logger.warning(f"High memory usage: {memory_usage}%. Consider garbage collection.")
                import gc
                gc.collect()
        except ImportError:
            pass  # psutil not available
        
        # –ü–ï–†–í–´–ô –®–ê–ì: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã
        # –≤ –±–ª–æ–∫–µ except –∏–ª–∏ –Ω–∞ –±–æ–ª–µ–µ –ø–æ–∑–¥–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö
        metadata = {}  # <--- –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±–ª–∞—Å—Ç–∏ –≤–∏–¥–∏–º–æ—Å—Ç–∏
        extracted_text = ""
        work_sequences = []
        doc_type_info = {}  # <--- –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è Stage 15
        file_hash = ""
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Stage 8
        self._current_file_path = file_path
        
        # üöÄ –ò–ù–ö–†–ï–ú–ï–ù–¢–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê
        if self.incremental_mode:
            # –í—ã—á–∏—Å–ª—è–µ–º —Ö—ç—à —Ñ–∞–π–ª–∞
            import hashlib
            with open(file_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª –ª–∏ —Ñ–∞–π–ª —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω
            if self._is_file_processed(file_path, file_hash):
                self.stats['files_skipped'] += 1
                logger.info(f"‚è© [SKIP] –§–∞–π–ª –ø—Ä–æ–ø—É—â–µ–Ω (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞): {Path(file_path).name}")
                return True  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º True, —Ç–∞–∫ –∫–∞–∫ —Ñ–∞–π–ª "—É—Å–ø–µ—à–Ω–æ" –ø—Ä–æ–ø—É—â–µ–Ω
        
        try:
            # ===== STAGE 1: Initial Validation =====
            stage1_start = time.time()
            validation_result = self._stage1_initial_validation(file_path)
            stages_timing['validation'] = time.time() - stage1_start
            
            if not validation_result['file_exists'] or not validation_result['can_read']:
                logger.warning(f"[Stage 1/14] File validation failed: {file_path}")
                self.performance_monitor.log_error("File validation failed", file_path)
                return False
            
            # ===== STAGE 2: Duplicate Checking (DISABLED FOR FORCE RETRAIN) =====
            duplicate_result = self._stage2_duplicate_checking(file_path)
            file_hash = duplicate_result['file_hash']  # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ö–µ—à –¥–ª—è Stage 15
            # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ü–ï–†–ï–û–ë–†–ê–ë–û–¢–ö–ê - –ò–ì–ù–û–†–ò–†–£–ï–ú –î–£–ë–õ–ò–ö–ê–¢–´
            if duplicate_result['is_duplicate']:
                logger.info(f"[Stage 2/14] DUPLICATE FOUND BUT FORCING RETRAIN: {file_path}")
                # return False  # –û–¢–ö–õ–Æ–ß–ï–ù–û –î–õ–Ø –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û–ô –ü–ï–†–ï–û–ë–†–ê–ë–û–¢–ö–ò
            
            # ===== STAGE 3: Text Extraction =====
            content = self._stage3_text_extraction(file_path)
            if not content or len(content) < 50:
                logger.warning(f"[Stage 3/14] Text extraction failed or content too short: {file_path}")
                return False
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –î–õ–Ø –ë–û–õ–¨–®–ò–• –§–ê–ô–õ–û–í
            file_size = Path(file_path).stat().st_size
            char_count = len(content.strip())
            if file_size > 50 * 1024 * 1024 and char_count < 10000:
                logger.error(f"[CRITICAL] Large file ({file_size/1024/1024:.1f}MB) but only {char_count} chars - ABORTING PROCESSING!")
                return False
            
            # ===== STAGE 3.5: Text Normalization =====
            content = self._stage3_5_text_normalization(content)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—Å—ã–ª–æ–∫ –Ω–∞ –ù–¢–î
            self._current_document_text = content
            
            # ===== STAGE 4: Document Type Detection =====
            doc_type_info = self._stage4_document_type_detection(content, file_path)
            
            # ===== STAGE 5: Structural Analysis =====
            structural_data = self._stage5_structural_analysis(content, doc_type_info)
            
            # üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ Stage 5
            if structural_data is None:
                logger.error(f"[ERROR] Stage 5 returned None for file: {file_path}")
                return False
            
            if not isinstance(structural_data, dict):
                logger.error(f"[ERROR] Stage 5 returned invalid data type: {type(structural_data)} for file: {file_path}")
                return False
            
            # !!! –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ù–ï –ü–ï–†–ï–ú–ï–©–ê–ï–ú –§–ê–ô–õ–´ –î–û –°–û–•–†–ê–ù–ï–ù–ò–Ø –í –ë–î! !!!
            # ===== STAGE 5.5: File Organization (–û–¢–õ–û–ñ–ï–ù–û –î–û STAGE 15) =====
            # –§–∞–π–ª—ã –±—É–¥—É—Ç –ø–µ—Ä–µ–º–µ—â–µ–Ω—ã —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ Stage 15
            
            # ===== STAGE 6: Regex to SBERT =====
            seed_works = self._stage6_regex_to_sbert(content, doc_type_info, structural_data)
            
            # ===== STAGE 7: SBERT Markup =====
            sbert_data = self._stage7_sbert_markup(content, seed_works, doc_type_info, structural_data)
            
            # ===== STAGE 8: Metadata Extraction (–¢–û–õ–¨–ö–û –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Rubern) =====
            metadata = self._stage8_metadata_extraction(content, structural_data, doc_type_info, sbert_data)
            
            # !!! –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ canonical_id –ø–æ—Å–ª–µ Stage 8 !!!
            if hasattr(metadata, 'is_duplicate') and metadata.is_duplicate:
                logger.warning(f"[Stage 8/14] File is duplicate by canonical_id: {metadata.canonical_id}, skipping: {file_path}")
                return False
            
            # ===== STAGE 9: Quality Control =====
            quality_report = self._stage9_quality_control(
                content, doc_type_info, structural_data, sbert_data, metadata
            )
            
            # ===== STAGE 10: Type-specific Processing =====
            type_specific_data = self._stage10_type_specific_processing(
                content, doc_type_info, structural_data, sbert_data
            )
            
            # ===== STAGE 11: Work Sequence Extraction =====
            work_sequences = self._stage11_work_sequence_extraction(
                sbert_data, doc_type_info, metadata
            )
            
            # ===== STAGE 12: Save Work Sequences =====
            metadata_dict = metadata.to_dict()
            saved_sequences = self._stage12_save_work_sequences(work_sequences, file_path, metadata_dict)
            
            # ===== STAGE 13: Smart Chunking =====
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DocumentMetadata –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è Stage 13
            metadata_dict = metadata.to_dict()
            chunks = self._stage13_smart_chunking(content, structural_data, metadata_dict, doc_type_info)
            
            # ===== STAGE 14: Save to Qdrant =====
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º DocumentMetadata –≤ —Å–ª–æ–≤–∞—Ä—å
            metadata_dict = metadata.to_dict()
            saved_chunks = self._stage14_save_to_qdrant(chunks, file_path, duplicate_result['file_hash'], metadata_dict)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats['total_chunks'] += len(chunks)
            self.stats['total_works'] += len(work_sequences)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–æ
            total_processing_time = time.time() - file_start_time
            quality_score = quality_report['quality_score'] / 100.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1
            
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ performance monitor
            self.performance_monitor.log_document(total_processing_time, quality_score, stages_timing)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ processed_files
            self._save_processed_file_info(
                file_path, 
                duplicate_result['file_hash'], 
                doc_type_info['doc_type'], 
                len(chunks)
            )
            
            # !!! –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ê–¢–û–ú–ê–†–ù–ê–Ø –§–ò–ö–°–ê–¶–ò–Ø –¢–û–õ–¨–ö–û –ü–û–°–õ–ï –°–û–•–†–ê–ù–ï–ù–ò–Ø –í –ë–î! !!!
            # ===== STAGE 15: Finalize Processing (–ê—Ç–æ–º–∞—Ä–Ω–∞—è —Ñ–∏–∫—Å–∞—Ü–∏—è) =====
            # !!! –ü–†–û–í–ï–†–Ø–ï–ú –£–°–ü–ï–• –ü–†–ï–î–´–î–£–©–ò–• –°–¢–ê–î–ò–ô! !!!
            neo4j_success = saved_sequences > 0
            qdrant_success = saved_chunks > 0
            
            # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
            import gc
            gc.collect()
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –µ—Å–ª–∏ –ø–∞–º—è—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞
            if hasattr(self, 'embedding_cache'):
                self.embedding_cache._cleanup_if_needed()
            
            # üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ß–∞–Ω–∫–∏ –¥–ª—è –ù–¢–î –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            doc_type = doc_type_info.get('doc_type', '')
            if doc_type in ['sp', 'gost', 'snip'] and len(chunks) == 0:
                logger.error(f"[CRITICAL] –ù–¢–î –¥–æ–∫—É–º–µ–Ω—Ç {doc_type} –±–µ–∑ —á–∞–Ω–∫–æ–≤ - –ù–ï –ü–ï–†–ï–ú–ï–©–ê–ï–ú –≤ processed!")
                finalization_result = False
            elif neo4j_success and qdrant_success:
                logger.info(f"[STAGE 15] Finalization: SUCCESS - File {file_path} processed successfully")
                finalization_result = True
            else:
                logger.warning(f"[STAGE 15] Finalization: PARTIAL - File {file_path} processed with issues")
                finalization_result = False
            
            logger.info(f"[COMPLETE] File processed: {len(chunks)} chunks, {len(work_sequences)} works, quality: {quality_score:.2f}, time: {total_processing_time:.2f}s")
            
            # üöÄ –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–ù–§–û–†–ú–ê–¶–ò–ò –û–ë –û–ë–†–ê–ë–û–¢–ê–ù–ù–û–ú –§–ê–ô–õ–ï
            if self.incremental_mode:
                self._save_processed_file_info(
                    file_path=file_path,
                    file_hash=duplicate_result['file_hash'],
                    doc_type=doc_type_info.get('doc_type', 'unknown'),
                    chunks_count=len(chunks)
                )
            
            # üöÄ –ü–ï–†–ï–ú–ï–©–ï–ù–ò–ï –û–ë–†–ê–ë–û–¢–ê–ù–ù–û–ì–û –§–ê–ô–õ–ê –í –ü–ê–ü–ö–£ PROCESSED
            try:
                # –ü–µ—Ä–µ–¥–∞–µ–º –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
                canonical_id = metadata_dict.get('canonical_id', '')
                title = metadata_dict.get('title', '')
                doc_type = doc_type_info.get('doc_type', '')
                
                self._move_processed_file(
                    file_path=file_path,
                    canonical_id=canonical_id,
                    title=title,
                    doc_type=doc_type
                )
                logger.info(f"‚úÖ [SUCCESS] –§–∞–π–ª –ø–µ—Ä–µ–º–µ—â—ë–Ω –≤ processed: {Path(file_path).name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [WARNING] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å —Ñ–∞–π–ª: {e}")
            
            return True
            
        except Exception as e:
            logger.error(f"Error in single file processing: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def _move_processed_file(self, file_path: str, canonical_id: str = None, title: str = None, doc_type: str = None) -> None:
        """–ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –≤ –ø–∞–ø–∫—É processed —Å –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–º –∏–º–µ–Ω–µ–º"""
        try:
            source_path = Path(file_path)
            if not source_path.exists():
                logger.warning(f"Source file not found: {file_path}")
                return
            
            # –°–æ–∑–¥–∞—ë–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–∞–ø–æ–∫ –≤ processed_dir
            processed_dir = config.processed_dir
            processed_dir.mkdir(parents=True, exist_ok=True)
            
            # üéØ –ö–ê–ù–û–ù–ò–ß–ï–°–ö–û–ï –ü–ï–†–ï–ò–ú–ï–ù–û–í–ê–ù–ò–ï
            if canonical_id and title and doc_type:
                # –°–æ–∑–¥–∞–µ–º –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
                canonical_name = self._create_canonical_filename(canonical_id, title, doc_type, source_path.suffix)
                dest_path = processed_dir / canonical_name
                logger.info(f"[CANONICAL] –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: {source_path.name} -> {canonical_name}")
            else:
                # Fallback –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –∏–º–µ–Ω–∏
                relative_path = source_path.relative_to(config.base_dir)
                dest_path = processed_dir / relative_path
                logger.info(f"[FALLBACK] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏–º–µ–Ω–∏: {source_path.name}")
            
            # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
            dest_path.parent.mkdir(parents=True, exist_ok=True)
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª
            import shutil
            shutil.move(str(source_path), str(dest_path))
            
            logger.info(f"–§–∞–π–ª –ø–µ—Ä–µ–º–µ—â—ë–Ω: {source_path} -> {dest_path}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            raise

    def _create_canonical_filename(self, canonical_id: str, title: str, doc_type: str, extension: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        try:
            # –û—á–∏—â–∞–µ–º canonical_id –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
            clean_id = canonical_id.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
            
            # –û—á–∏—â–∞–µ–º title –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
            clean_title = title.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')
            if len(clean_title) > 50:
                clean_title = clean_title[:50]
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–µ –∏–º—è
            canonical_name = f"{clean_id}_{clean_title}{extension}"
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â—É—é –¥–ª–∏–Ω—É –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (Windows limit ~255)
            if len(canonical_name) > 200:
                canonical_name = f"{clean_id}_{clean_title[:30]}{extension}"
            
            logger.info(f"[CANONICAL] –°–æ–∑–¥–∞–Ω–æ –∏–º—è: {canonical_name}")
            return canonical_name
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –∏–º–µ–Ω–∏: {e}")
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –∏–º–µ–Ω–∏
            return f"document_{doc_type}{extension}"

    def _save_failed_file(self, file_path: str, error_message: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –Ω–µ—É–¥–∞—á–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º —Ñ–∞–π–ª–µ"""
        try:
            failed_files_path = self.config.log_dir / 'failed_files.json'
            failed_files = []
            
            if failed_files_path.exists():
                with open(failed_files_path, 'r', encoding='utf-8') as f:
                    failed_files = json.load(f)
            
            failed_files.append({
                'file_path': file_path,
                'error': error_message,
                'timestamp': datetime.now().isoformat()
            })
            
            with open(failed_files_path, 'w', encoding='utf-8') as f:
                json.dump(failed_files, f, ensure_ascii=False, indent=2)
                
            logger.info(f"Failed file info saved: {Path(file_path).name}")
            
        except Exception as e:
            logger.error(f"Error saving failed file info: {e}")

    def _stage_0_smart_file_scanning_and_preprocessing(self, max_files: Optional[int] = None) -> List[str]:
        """STAGE 0: Smart File Scanning + NTD Preprocessing"""
        
        logger.info("[Stage 0/14] SMART FILE SCANNING + NTD PREPROCESSING")
        start_time = time.time()
        
        file_patterns = ['*.pdf', '*.docx', '*.doc', '*.txt', '*.rtf', '*.xlsx', '*.xls']
        all_files = []
        
        # –ò—â–µ–º —Ñ–∞–π–ª—ã
        for pattern in file_patterns:
            pattern_files = glob.glob(str(self.base_dir / '**' / pattern), recursive=True)
            all_files.extend(pattern_files)
            logger.info(f"Found {len(pattern_files)} {pattern} files")
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
        valid_files = []
        for file_path in all_files:
            if self._is_valid_file(file_path):
                valid_files.append(file_path)
        
        logger.info(f"Valid files after filtering: {len(valid_files)}")
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ
        if max_files and len(valid_files) > max_files:
            valid_files = valid_files[:max_files]
            logger.info(f"Limited to {max_files} files")
        
        # Enhanced NTD Preprocessing with Smart Queue prioritization
        prioritized_files = self._enhanced_ntd_preprocessing_with_smart_queue(valid_files)
        
        elapsed = time.time() - start_time
        logger.info(f"[Stage 0/14] COMPLETE - Found {len(prioritized_files)} files in {elapsed:.2f}s")
        
        return prioritized_files
    
    def _is_valid_file(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞"""
        
        try:
            path = Path(file_path)
            
            if not path.exists():
                return False
            
            # –†–∞–∑–º–µ—Ä –æ—Ç 1KB –¥–æ 150MB
            size = path.stat().st_size
            if size < 1024 or size > 150 * 1024 * 1024:
                return False
            
            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ñ–∞–π–ª—ã
            exclude_patterns = ['temp', 'tmp', 'cache', '__pycache__', '.git', 'backup']
            file_str = str(path).lower()
            if any(pattern in file_str for pattern in exclude_patterns):
                return False
            
            return True
            
        except Exception:
            return False
    
    def _ntd_preprocessing(self, files: List[str]) -> List[str]:
        """NTD Preprocessing - —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É"""
        
        type_priorities = {
            'gost': 10,
            'sp': 9,
            'snip': 8,
            'ppr': 6,
            'smeta': 4,
            'other': 2
        }
        
        prioritized = []
        for file_path in files:
            priority = self._get_file_priority(file_path, type_priorities)
            prioritized.append((file_path, priority))
        
        prioritized.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"NTD Preprocessing: Prioritized {len(prioritized)} files")
        
        return [file_path for file_path, _ in prioritized]
    
    def _enhanced_ntd_preprocessing_with_smart_queue(self, files: List[str]) -> List[str]:
        """–£–õ–£–ß–®–ï–ù–ò–ï 7: Enhanced NTD Preprocessing with SmartQueue prioritization"""
        
        logger.info("Starting Enhanced NTD Preprocessing with Smart Queue...")
        start_time = time.time()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SmartQueue –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
        prioritized_files = self.smart_queue.sort_files(files)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ø-10 —Ñ–∞–π–ª–æ–≤ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
        logger.info("Top 10 priority files:")
        for i, (file_path, priority) in enumerate(prioritized_files[:10]):
            filename = Path(file_path).name
            logger.info(f"  {i+1}. [{priority:2d}] {filename}")
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è NTD —Ñ–∞–π–ª–æ–≤
        ntd_enhanced_files = []
        for file_path, priority in prioritized_files:
            filename = Path(file_path).name.lower()
            
            # –ë–æ–Ω—É—Å –¥–ª—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if any(pattern in filename for pattern in ['–≥–æ—Å—Ç', '—Å–Ω–∏–ø', '—Å–ø', '–Ω—Ç–¥']):
                priority += 5
                logger.debug(f"NTD bonus applied: {filename} -> priority {priority}")
            
            # –®—Ç—Ä–∞—Ñ –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            if any(pattern in filename for pattern in ['—Ç–µ—Å—Ç', 'test', 'temp', 'copy']):
                priority -= 3
                logger.debug(f"Test penalty applied: {filename} -> priority {priority}")
            
            ntd_enhanced_files.append((file_path, priority))
        
        # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä—É–µ–º —Å —É—á–µ—Ç–æ–º NTD –±–æ–Ω—É—Å–æ–≤
        ntd_enhanced_files.sort(key=lambda x: x[1], reverse=True)
        
        elapsed = time.time() - start_time
        logger.info(f"Enhanced NTD Preprocessing complete: {len(ntd_enhanced_files)} files prioritized in {elapsed:.2f}s")
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—É—Ç–∏ —Ñ–∞–π–ª–æ–≤ (–±–µ–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤)
        return [file_path for file_path, _ in ntd_enhanced_files]
    
    def _get_file_priority(self, file_path: str, priorities: Dict[str, int]) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ —Ñ–∞–π–ª–∞ –ø–æ –∏–º–µ–Ω–∏"""
        
        filename = Path(file_path).name.lower()
        
        if any(word in filename for word in ['–≥–æ—Å—Ç', 'gost']):
            return priorities['gost']
        elif any(word in filename for word in ['—Å–ø', 'sp']) and '—Å–≤–æ–¥' in filename:
            return priorities['sp']
        elif any(word in filename for word in ['—Å–Ω–∏–ø', 'snip']):
            return priorities['snip']
        elif any(word in filename for word in ['–ø–ø—Ä', 'ppr', '—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞']):
            return priorities['ppr']
        elif any(word in filename for word in ['—Å–º–µ—Ç–∞', '—Ä–∞—Å—Ü–µ–Ω–∫', '—Å—Ç–æ–∏–º–æ—Å—Ç—å']):
            return priorities['smeta']
        else:
            return priorities['other']
    
    def _stage1_initial_validation(self, file_path: str) -> Dict[str, Any]:
        """STAGE 1: Initial Validation"""
        
        logger.info(f"[Stage 1/14] INITIAL VALIDATION: {Path(file_path).name}")
        start_time = time.time()
        
        result = {
            'file_exists': False,
            'file_size': 0,
            'can_read': False,
            'file_type': 'unknown',
            'supported_format': False
        }
        
        # üöÄ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –ü–û–î–î–ï–†–ñ–ö–ê –§–û–†–ú–ê–¢–û–í
        SUPPORTED_EXTENSIONS = {
            '.pdf', '.docx', '.doc', '.xlsx', '.xls', '.txt', '.rtf',
            '.xml', '.json', '.dwg', '.dxf', '.tiff', '.tif', '.png', 
            '.jpg', '.jpeg', '.zip', '.rar'
        }
        
        try:
            path = Path(file_path)
            
            result['file_exists'] = path.exists()
            
            if result['file_exists']:
                result['file_size'] = path.stat().st_size
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ñ–∞–π–ª–∞
                file_ext = path.suffix.lower()
                result['file_type'] = file_ext
                result['supported_format'] = file_ext in SUPPORTED_EXTENSIONS
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–¥–¥–µ—Ä–∂–∫—É —Ñ–æ—Ä–º–∞—Ç–∞
                if result['supported_format']:
                    logger.info(f"‚úÖ [Stage 1/14] –§–æ—Ä–º–∞—Ç {file_ext} –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
                else:
                    logger.warning(f"‚ö†Ô∏è [Stage 1/14] –§–æ—Ä–º–∞—Ç {file_ext} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã
                if file_ext in ['.dwg', '.dxf']:
                    result['file_type'] = 'drawing_cad'
                elif file_ext in ['.tiff', '.tif', '.png', '.jpg', '.jpeg']:
                    result['file_type'] = 'scan_image'
                elif file_ext in ['.zip', '.rar']:
                    result['file_type'] = 'archive'
                elif file_ext in ['.xml', '.json']:
                    result['file_type'] = 'structured'
                
                try:
                    with open(path, 'rb') as f:
                        f.read(100)
                    result['can_read'] = True
                except:
                    result['can_read'] = False
        
        except Exception as e:
            logger.warning(f"Validation error: {e}")
        
        elapsed = time.time() - start_time
        
        # !!! –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞! !!!
        if result['file_exists'] and result['can_read']:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ (–ª–∏–º–∏—Ç 150MB)
            max_size = 150 * 1024 * 1024  # 150MB
            if result['file_size'] > max_size:
                logger.warning(f"[Stage 1/14] FAILED - File too large: {result['file_size']} bytes (limit: {max_size})")
                result['can_read'] = False
            else:
                logger.info(f"[Stage 1/14] COMPLETE - File valid, size: {result['file_size']} bytes ({elapsed:.2f}s)")
        else:
            logger.warning(f"[Stage 1/14] FAILED - File invalid ({elapsed:.2f}s)")
        
        return result
    
    def _stage2_duplicate_checking(self, file_path: str) -> Dict[str, Any]:
        """STAGE 2: Enhanced Duplicate Checking (Hash + Document Numbers)"""
        
        logger.info(f"[Stage 2/14] ENHANCED DUPLICATE CHECKING: {Path(file_path).name}")
        start_time = time.time()
        
        file_hash = self._calculate_file_hash(file_path)
        is_duplicate = file_hash in self.processed_files
        
        # !!! –û–¢–ö–õ–Æ–ß–ï–ù–û: –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –Ω–æ–º–µ—Ä–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤—ã–∑—ã–≤–∞–µ—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è !!!
        # –ü—Ä–æ–±–ª–µ–º–∞: —Å–∏—Å—Ç–µ–º–∞ —Å—á–∏—Ç–∞–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–∞–º–∏ —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—Å—Ç–æ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ –æ–¥–Ω–∏ –ù–¢–î
        # –†–µ—à–µ–Ω–∏–µ: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–æ —Ö–µ—à—É —Ñ–∞–π–ª–∞, –∞ –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ –Ω–æ–º–µ—Ä–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        # –ø–µ—Ä–µ–Ω–µ—Å–µ–º –≤ Stage 8, –≥–¥–µ —É–∂–µ –±—É–¥–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_numbers = []
        if not is_duplicate:
            # –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è)
            try:
                content = self._quick_text_extract(file_path)
                if content:
                    doc_numbers = self._extract_document_numbers(content)
                    if doc_numbers:
                        logger.info(f"[Stage 2/14] Found document numbers: {doc_numbers[:3]} (for reference only)")
                        # –û–¢–ö–õ–Æ–ß–ï–ù–û: is_duplicate = self._check_duplicate_by_doc_numbers(doc_numbers)
                        # if is_duplicate:
                        #     logger.info(f"[Stage 2/14] DUPLICATE BY DOC NUMBERS: {doc_numbers[:3]}")
            except Exception as e:
                logger.debug(f"Document numbers extraction failed: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤ Qdrant –ø–æ —Ö–µ—à—É
        duplicate_source = ""
        if not is_duplicate and self.qdrant:
            try:
                search_result = self.qdrant.scroll(
                    collection_name="enterprise_docs",
                    scroll_filter=models.Filter(
                        must=[
                            models.FieldCondition(
                                key="file_hash",
                                match=models.MatchValue(value=file_hash)
                            )
                        ]
                    ),
                    limit=1
                )
                if len(search_result[0]) > 0:
                    is_duplicate = True
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏—Ä—É—é—â–µ–º—Å—è —Ñ–∞–π–ª–µ –∏–∑ Qdrant
                    duplicate_record = search_result[0][0]
                    if 'file_path' in duplicate_record.payload:
                        original_name = Path(duplicate_record.payload['file_path']).name
                        duplicate_source = f" (duplicates: {original_name})"
                    else:
                        duplicate_source = " (duplicates: existing file in Qdrant)"
                
            except Exception as e:
                logger.warning(f"Qdrant duplicate check failed: {e}")
        
        result = {
            'is_duplicate': is_duplicate,
            'file_hash': file_hash,
            'doc_numbers': doc_numbers
        }
        
        elapsed = time.time() - start_time
        
        if is_duplicate:
            # –ù–∞—Ö–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏—Ä—É—é—â–µ–º—Å—è —Ñ–∞–π–ª–µ
            duplicate_info = ""
            if file_hash in self.processed_files:
                original_file = self.processed_files[file_hash].get('file_path', 'unknown')
                original_name = Path(original_file).name
                duplicate_info = f" (duplicates: {original_name})"
            elif duplicate_source:
                duplicate_info = duplicate_source
            else:
                duplicate_info = " (duplicates: existing file in Qdrant)"
            
            logger.info(f"[Stage 2/14] DUPLICATE FOUND (hash) - Hash: {file_hash[:16]}...{duplicate_info} ({elapsed:.2f}s)")
        else:
            logger.info(f"[Stage 2/14] UNIQUE FILE - Hash: {file_hash[:16]}... ({elapsed:.2f}s)")
        
        return result
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ MD5 —Ö–µ—à–∞ —Ñ–∞–π–ª–∞"""
        
        hasher = hashlib.md5()
        
        try:
            with open(file_path, 'rb') as f:
                while chunk := f.read(8192):
                    hasher.update(chunk)
                    
            return hasher.hexdigest()
            
        except Exception as e:
            logger.error(f"Hash calculation failed: {e}")
            fallback_string = f"{file_path}_{os.path.getsize(file_path)}"
            return hashlib.md5(fallback_string.encode()).hexdigest()
    
    def _quick_text_extract(self, file_path: str) -> str:
        """–ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5000 —Å–∏–º–≤–æ–ª–æ–≤)"""
        try:
            ext = Path(file_path).suffix.lower()
            if ext == '.pdf':
                # –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü PDF
                if HAS_FILE_PROCESSING:
                    with open(file_path, 'rb') as file:
                        pdf_reader = PyPDF2.PdfReader(file)
                        content = ""
                        for i, page in enumerate(pdf_reader.pages):  # –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                            content += page.extract_text() + "\n"
                        return content[:500000]  # 500K —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ù–¢–î
            elif ext == '.docx':
                if HAS_FILE_PROCESSING:
                    doc = Document(file_path)
                    content = ""
                    for para in doc.paragraphs:  # –í–°–ï –∞–±–∑–∞—Ü—ã
                        content += para.text + "\n"
                    return content[:500000]  # 500K —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ù–¢–î
            elif ext == '.doc':
                if HAS_FILE_PROCESSING:
                    content = self._extract_from_doc_enterprise(file_path)
                    return content[:500000] if content else ""  # 500K —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ù–¢–î
            return ""
        except Exception as e:
            logger.debug(f"Quick text extraction failed: {e}")
            return ""
    
    def _check_duplicate_by_doc_numbers(self, doc_numbers: List[str]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –Ω–æ–º–µ—Ä–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant (–û–¢–ö–õ–Æ–ß–ï–ù–û - –≤—ã–∑—ã–≤–∞–µ—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)"""
        # –û–¢–ö–õ–Æ–ß–ï–ù–û: –≠—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤—ã–∑—ã–≤–∞–µ—Ç –ª–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è, —Ç–∞–∫ –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç
        # —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ –ù–¢–î, –∞ –Ω–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π ID –¥–æ–∫—É–º–µ–Ω—Ç–∞
        return False
    
    def _check_duplicate_by_canonical_id(self, canonical_id: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º—É canonical_id –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ Qdrant"""
        if not self.qdrant or not canonical_id:
            return False
        
        try:
            search_result = self.qdrant.scroll(
                collection_name="enterprise_docs",
                scroll_filter=models.Filter(
                    must=[
                        models.FieldCondition(
                            key="canonical_id",
                            match=models.MatchValue(value=canonical_id)
                        )
                    ]
                ),
                limit=1
            )
            return len(search_result[0]) > 0
        except Exception as e:
            logger.debug(f"Canonical ID duplicate check failed: {e}")
            return False
    
    def _stage3_text_extraction(self, file_path: str) -> str:
        """Unified extraction (best from duplicates + LangChain fallback)."""
        ext = Path(file_path).suffix.lower()
        if ext not in ['.pdf', '.docx', '.doc', '.txt', '.xlsx']:
            logger.warning(f"Unsupported ext: {ext}")
            return ''
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –†–ê–ó–ú–ï–†–ê –§–ê–ô–õ–ê
        file_size = Path(file_path).stat().st_size
        if file_size > 50 * 1024 * 1024:  # > 50MB
            logger.info(f"[LARGE FILE] Processing {file_size/1024/1024:.1f}MB file - expecting high-quality extraction")
        
        if HAS_FILE_PROCESSING:
            try:
                if ext == '.pdf':
                    try:
                        with open(file_path, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            content = '\n'.join(page.extract_text() for page in reader.pages[:5])  # Limit pages
                        if len(content.strip()) > 100:
                            return self._clean_text(content)
                        # Fallback OCR if poor
                        if HAS_OCR_LIBS:
                            return self._ocr_fallback(file_path)
                    except Exception as pdf_error:
                        if "PyCryptodome" in str(pdf_error) or "AES" in str(pdf_error) or "encryption" in str(pdf_error).lower():
                            logger.warning(f"Encrypted PDF detected: {file_path}. Skipping.")
                            return ""  # Skip encrypted PDFs gracefully
                        else:
                            logger.error(f"PDF extraction failed: {pdf_error}")
                            return ""
                elif ext == '.docx':
                    doc = Document(file_path)
                    content = '\n'.join(para.text for para in doc.paragraphs)
                    return self._clean_text(content)
                elif ext == '.doc':
                    # Handle legacy .doc files with multiple fallback methods
                    content = self._extract_from_doc_enterprise(file_path)
                    return self._clean_text(content) if content else ""
                elif ext == '.txt':
                    with open(file_path, 'r', encoding='utf-8') as f:
                        return self._clean_text(f.read())
                elif ext == '.xlsx':
                    df = pd.read_excel(file_path, sheet_name=0)  # First sheet
                    return self._clean_text(df.to_string())
            except Exception as e:
                logger.error(f"Extraction failed {ext}: {e}")
                return ''
        
        # OCR fallback for images/PDF
        if HAS_OCR_LIBS and ext in ['.pdf', '.png', '.jpg']:
            return self._ocr_fallback(file_path)
        return ''
    
    def _ocr_fallback(self, file_path: str) -> str:
        """Unified OCR (from all duplicates)."""
        if not HAS_OCR_LIBS:
            return ''
        try:
            if Path(file_path).suffix == '.pdf':
                pdf_path = Path(file_path)
                if HAS_FILE_PROCESSING:
                    try:
                        doc = fitz.open(pdf_path)
                        # –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–ú –í–°–ï –°–¢–†–ê–ù–ò–¶–´ –î–õ–Ø –ü–û–õ–ù–û–ì–û –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –¢–ï–ö–°–¢–ê!
                        total_pages = len(doc)
                        content = '\n'.join(page.get_text() for page in doc)
                        doc.close()
                        logger.info(f"[PDF] Processed ALL {total_pages} pages from {pdf_path.stat().st_size/1024/1024:.1f}MB file")
                        
                        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –ï—Å–ª–∏ Fitz –∏–∑–≤–ª–µ–∫ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ - —Ñ–æ—Ä—Å–∏—Ä—É–µ–º OCR!
                        if len(content.strip()) < 1000:  # –ú–µ–Ω—å—à–µ 1K —Å–∏–º–≤–æ–ª–æ–≤ - –í–°–ï–ì–î–ê OCR!
                            logger.warning(f"[CRITICAL] Fitz extracted too little text ({len(content)} chars), forcing OCR fallback")
                            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Ä–µ–∞–ª—å–Ω–æ–º—É OCR —Å pdf2image –≤–º–µ—Å—Ç–æ —Ä–µ–∫—É—Ä—Å–∏–∏
                            content = ""  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º content —á—Ç–æ–±—ã –ø–µ—Ä–µ–π—Ç–∏ –∫ pdf2image
                        
                        if len(content) > 50:
                            return self._clean_text(content)
                    except Exception as fitz_error:
                        if "PyCryptodome" in str(fitz_error) or "AES" in str(fitz_error) or "encryption" in str(fitz_error).lower():
                            logger.warning(f"Encrypted PDF in OCR fallback: {file_path}. Skipping.")
                            return ""
                        else:
                            logger.warning(f"Fitz PDF processing failed: {fitz_error}")
                
                # PDF2image OCR - –í–°–ï –°–¢–†–ê–ù–ò–¶–´! (–µ—Å–ª–∏ Fitz –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –∏–ª–∏ –∏–∑–≤–ª–µ–∫ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞)
                if not content or len(content.strip()) < 1000:
                    try:
                        # –û–ë–†–ê–ë–ê–¢–´–í–ê–ï–ú –í–°–ï –°–¢–†–ê–ù–ò–¶–´ –î–õ–Ø –ü–û–õ–ù–û–ì–û –ò–ó–í–õ–ï–ß–ï–ù–ò–Ø –¢–ï–ö–°–¢–ê!
                        images = convert_from_path(file_path, dpi=150)
                        logger.info(f"[OCR] Processing ALL {len(images)} pages for complete text extraction")
                        content = '\n'.join(pytesseract.image_to_string(img, lang='rus+eng') for img in images)
                    except Exception as pdf2img_error:
                        if "PyCryptodome" in str(pdf2img_error) or "AES" in str(pdf2img_error) or "encryption" in str(pdf2img_error).lower():
                            logger.warning(f"Encrypted PDF in PDF2image: {file_path}. Skipping.")
                            return ""
                        else:
                            logger.warning(f"PDF2image failed: {pdf2img_error}")
                            return ""
            else:
                content = pytesseract.image_to_string(Image.open(file_path), lang='rus+eng')
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê –ö–ê–ß–ï–°–¢–í–ê –¢–ï–ö–°–¢–ê
            cleaned_content = self._clean_text(content)
            char_count = len(cleaned_content.strip())
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            try:
                file_size = Path(file_path).stat().st_size
            except:
                file_size = 0
            
            # –î–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ —Ç—Ä–µ–±—É–µ–º –º–∏–Ω–∏–º—É–º 10K —Å–∏–º–≤–æ–ª–æ–≤
            if file_size > 50 * 1024 * 1024 and char_count < 10000:
                logger.error(f"[CRITICAL] Large file ({file_size/1024/1024:.1f}MB) but only {char_count} chars extracted - QUALITY TOO LOW!")
                return ''  # –ü—Ä–µ—Ä—ã–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
            
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –º–∏–Ω–∏–º—É–º 1K —Å–∏–º–≤–æ–ª–æ–≤
            elif char_count < 1000:
                logger.warning(f"[WARNING] Only {char_count} chars extracted - quality may be low")
            
            logger.info(f"[QUALITY] Extracted {char_count} chars from {file_size/1024/1024:.1f}MB file")
            return cleaned_content
            
        except Exception as e:
            logger.warning(f"OCR fallback failed: {e}")
            return ''
    
    def _extract_from_doc_enterprise(self, file_path: str) -> str:
        """Enterprise .doc file extraction with multiple fallback methods"""
        
        if not HAS_FILE_PROCESSING:
            logger.error("File processing not available for .doc files")
            return ""
        
        logger.info(f"[DOC] Starting enterprise extraction for: {Path(file_path).name}")
        
        # Method 1: Try Microsoft Word COM automation (most reliable for .doc files)
        if HAS_PYWIN32:
            try:
                logger.debug("[DOC] Attempting Microsoft Word COM extraction...")
                content = self._extract_with_word_com(file_path)
                if content and len(content.strip()) > 50:
                    logger.info(f"[DOC] Word COM extracted {len(content)} characters")
                    return content
                else:
                    logger.warning("[DOC] Word COM returned insufficient content")
            except Exception as e:
                logger.warning(f"[DOC] Word COM failed: {e}")
        else:
            logger.debug("[DOC] PyWin32 not available for Word COM automation")
        
        # Method 2: Try textract (if available and working)
        if HAS_TEXTRACT:
            try:
                logger.debug("[DOC] Attempting textract extraction...")
                content = textract.process(file_path).decode('utf-8')
                if content and len(content.strip()) > 50:
                    logger.info(f"[DOC] textract extracted {len(content)} characters")
                    return content
                else:
                    logger.warning("[DOC] textract returned insufficient content")
            except Exception as e:
                logger.warning(f"[DOC] textract failed: {e}")
        else:
            logger.debug("[DOC] textract not available (dependency issues)")
        
        # Method 3: Try python-docx (sometimes works with .doc files)
        try:
            logger.debug("[DOC] Attempting python-docx extraction...")
            doc = Document(file_path)
            content = '\n'.join(para.text for para in doc.paragraphs)
            if content and len(content.strip()) > 50:
                logger.info(f"[DOC] python-docx extracted {len(content)} characters")
                return content
            else:
                logger.warning("[DOC] python-docx returned insufficient content")
        except Exception as e:
            logger.warning(f"[DOC] python-docx failed: {e}")
        
        # Method 4: Try docx2txt (sometimes works with .doc files)
        try:
            logger.debug("[DOC] Attempting docx2txt extraction...")
            content = docx2txt.process(file_path)
            if content and len(content.strip()) > 50:
                logger.info(f"[DOC] docx2txt extracted {len(content)} characters")
                return content
            else:
                logger.warning("[DOC] docx2txt returned insufficient content")
        except Exception as e:
            logger.warning(f"[DOC] docx2txt failed: {e}")
        
        # Method 5: Try subprocess with antiword (if available)
        try:
            logger.debug("[DOC] Attempting antiword extraction...")
            import subprocess
            result = subprocess.run(['antiword', file_path], capture_output=True, text=True, timeout=30)
            if result.returncode == 0 and result.stdout.strip():
                content = result.stdout
                if len(content.strip()) > 50:
                    logger.info(f"[DOC] antiword extracted {len(content)} characters")
                    return content
            else:
                logger.warning("[DOC] antiword failed or returned insufficient content")
        except Exception as e:
            logger.warning(f"[DOC] antiword failed: {e}")
        
        # Method 6: Try subprocess with catdoc (if available)
        try:
            logger.debug("[DOC] Attempting catdoc extraction...")
            import subprocess
            result = subprocess.run(['catdoc', file_path], capture_output=True, text=True, timeout=30)
            if result.returncode == 0 and result.stdout.strip():
                content = result.stdout
                if len(content.strip()) > 50:
                    logger.info(f"[DOC] catdoc extracted {len(content)} characters")
                    return content
            else:
                logger.warning("[DOC] catdoc failed or returned insufficient content")
        except Exception as e:
            logger.warning(f"[DOC] catdoc failed: {e}")
        
        # Method 7: Try subprocess with LibreOffice (if available)
        try:
            logger.debug("[DOC] Attempting LibreOffice extraction...")
            import subprocess
            import tempfile
            import os
            
            # Create temporary directory for conversion
            with tempfile.TemporaryDirectory() as temp_dir:
                # Convert .doc to .txt using LibreOffice
                result = subprocess.run([
                    'libreoffice', '--headless', '--convert-to', 'txt', 
                    '--outdir', temp_dir, file_path
                ], capture_output=True, text=True, timeout=60)
                
                if result.returncode == 0:
                    # Find the converted file
                    base_name = Path(file_path).stem
                    txt_file = Path(temp_dir) / f"{base_name}.txt"
                    if txt_file.exists():
                        with open(txt_file, 'r', encoding='utf-8', errors='ignore') as f:
                            content = f.read()
                        if len(content.strip()) > 50:
                            logger.info(f"[DOC] LibreOffice extracted {len(content)} characters")
                            return content
            logger.warning("[DOC] LibreOffice failed or returned insufficient content")
        except Exception as e:
            logger.warning(f"[DOC] LibreOffice failed: {e}")
        
        # Method 8: Try subprocess with pandoc (if available)
        try:
            logger.debug("[DOC] Attempting pandoc extraction...")
            import subprocess
            result = subprocess.run(['pandoc', file_path, '-t', 'plain'], capture_output=True, text=True, timeout=30)
            if result.returncode == 0 and result.stdout.strip():
                content = result.stdout
                if len(content.strip()) > 50:
                    logger.info(f"[DOC] pandoc extracted {len(content)} characters")
                    return content
            else:
                logger.warning("[DOC] pandoc failed or returned insufficient content")
        except Exception as e:
            logger.warning(f"[DOC] pandoc failed: {e}")
        
        # All methods failed
        logger.error(f"[DOC] All extraction methods failed for {Path(file_path).name}")
        return ""
    
    def _extract_with_word_com(self, file_path: str) -> str:
        """Extract text from .doc file using Microsoft Word COM automation"""
        
        if not HAS_PYWIN32:
            logger.error("PyWin32 not available for Word COM automation")
            return ""
        
        word_app = None
        doc = None
        
        try:
            logger.debug("[DOC-COM] Starting Microsoft Word COM automation...")
            
            # Create Word application object
            word_app = win32com.client.Dispatch("Word.Application")
            word_app.Visible = False  # Run Word in background
            word_app.DisplayAlerts = False  # Suppress alerts
            
            # Open the document
            doc = word_app.Documents.Open(file_path, ReadOnly=True)
            
            # Extract text from the document
            content = doc.Content.Text
            
            # Also try to extract from tables if present
            table_text = ""
            if doc.Tables.Count > 0:
                logger.debug(f"[DOC-COM] Found {doc.Tables.Count} tables, extracting...")
                for i, table in enumerate(doc.Tables):
                    table_text += f"\n--- TABLE {i+1} ---\n"
                    for row in table.Rows:
                        row_text = []
                        for cell in row.Cells:
                            cell_text = cell.Range.Text.strip()
                            # Remove table cell markers
                            cell_text = cell_text.replace('\r\x07', '').strip()
                            if cell_text:
                                row_text.append(cell_text)
                        if row_text:
                            table_text += " | ".join(row_text) + "\n"
                    table_text += "--- END TABLE ---\n"
            
            # Combine main content with table content
            full_content = content + table_text
            
            logger.debug(f"[DOC-COM] Extracted {len(full_content)} characters from Word document")
            return full_content
            
        except Exception as e:
            logger.error(f"[DOC-COM] Word COM automation failed: {e}")
            return ""
            
        finally:
            # Clean up COM objects
            try:
                if doc:
                    doc.Close(SaveChanges=False)
                if word_app:
                    word_app.Quit()
            except Exception as cleanup_error:
                logger.warning(f"[DOC-COM] Cleanup error: {cleanup_error}")
    
    def _stage3_5_text_normalization(self, content: str) -> str:
        """
        Stage 3.5: –°–∏–º–±–∏–æ—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞.
        Step A: –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ (Regex)
        Step B: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ (DeepSeek-Coder)
        """
        import re
        import time
        
        logger.info("[Stage 3.5/14] SYMBIOTIC TEXT NORMALIZATION")
        start_time = time.time()
        
        # === STEP A: –ú–ì–ù–û–í–ï–ù–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ù–ê–Ø –û–ß–ò–°–¢–ö–ê (Regex) ===
        logger.info("[Stage 3.5-A/14] FAST REGEX CLEANING")
        regex_start = time.time()
        
        original_length = len(content)
        logger.info(f"[Stage 3.5-A/14] –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {original_length} —Å–∏–º–≤–æ–ª–æ–≤")

        # 1. –£–¥–∞–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫–∏
        before_newlines = content.count('\n\n\n')
        content = re.sub(r'\n{3,}', '\n\n', content)
        after_newlines = content.count('\n\n\n')
        logger.info(f"[Stage 3.5-A/14] –£–¥–∞–ª–µ–Ω–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø–µ—Ä–µ–Ω–æ—Å–æ–≤: {before_newlines - after_newlines}")

        # 2. –£–¥–∞–ª–µ–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        before_spaces = len(re.findall(r'[ \t\r\f]{2,}', content))
        content = re.sub(r'[ \t\r\f]+', ' ', content)
        after_spaces = len(re.findall(r'[ \t\r\f]{2,}', content))
        logger.info(f"[Stage 3.5-A/14] –£–¥–∞–ª–µ–Ω–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤: {before_spaces - after_spaces}")

        # 3. –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö OCR-–ø–µ—Ä–µ–Ω–æ—Å–æ–≤
        before_hyphens = len(re.findall(r'(\w+)[-‚Äî]\s*\n\s*(\w+)', content))
        content = re.sub(r'(\w+)[-‚Äî]\s*\n\s*(\w+)', r'\1\2', content, flags=re.MULTILINE)
        after_hyphens = len(re.findall(r'(\w+)[-‚Äî]\s*\n\s*(\w+)', content))
        logger.info(f"[Stage 3.5-A/14] –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ OCR-–ø–µ—Ä–µ–Ω–æ—Å–æ–≤: {before_hyphens - after_hyphens}")

        # 4. –û—á–∏—Å—Ç–∫–∞ –Ω–∞—á–∞–ª–∞/–∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫
        lines_before = content.split('\n')
        lines_after = [line.strip() for line in lines_before]
        content = '\n'.join(lines_after).strip()
        logger.info(f"[Stage 3.5-A/14] –û—á–∏—â–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(lines_before)} -> {len(lines_after)}")

        regex_elapsed = time.time() - regex_start
        final_length = len(content)
        logger.info(f"[Stage 3.5-A/14] COMPLETE - Regex cleaned: {original_length} -> {final_length} chars ({regex_elapsed:.2f}s)")
        
        # === STEP B: –ö–û–ù–¢–ï–ö–°–¢–ù–ê–Ø –û–ß–ò–°–¢–ö–ê (RULE-BASED) ===
        logger.info("[Stage 3.5-B/14] RULE-BASED CONTEXTUAL CLEANING")
        llm_start = time.time()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è rule-based –æ—á–∏—Å—Ç–∫–∞ (–±—ã—Å—Ç—Ä–∞—è –∏ –Ω–∞–¥–µ–∂–Ω–∞—è)
        # 1. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã—Ö —Å–ª–æ–≤ (OCR-–ø–µ—Ä–µ–Ω–æ—Å—ã)
        content = re.sub(r'(\w+)[-‚Äî]\s*\n\s*(\w+)', r'\1\2', content, flags=re.MULTILINE)
        
        # 2. –£—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤
        content = re.sub(r'(\w+)\s+(\w+)', r'\1\2', content)
        
        # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–≤—ã—á–µ–∫ –∏ —Ç–∏—Ä–µ
        content = content.replace('"', '"').replace('"', '"')
        content = content.replace('‚Äî', '-').replace('‚Äì', '-')
        
        # 4. –£–¥–∞–ª–µ–Ω–∏–µ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
        content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]+', ' ', content)
        
        # 5. –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤
        content = re.sub(r' {2,}', ' ', content)
        content = re.sub(r' *\n *', '\n', content)
        
        llm_elapsed = time.time() - llm_start
        logger.info(f"[Stage 3.5-B/14] COMPLETE - Rule-based cleaning –∑–∞ {llm_elapsed:.2f}s")
        logger.info(f"[Stage 3.5-B/14] –ò—Ç–æ–≥–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        total_elapsed = time.time() - start_time
        final_length = len(content)
        logger.info(f"[Stage 3.5/14] COMPLETE - Symbiotic normalization: {original_length} -> {final_length} chars ({total_elapsed:.2f}s)")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
        compression_ratio = (original_length - final_length) / original_length * 100 if original_length > 0 else 0
        logger.info(f"[Stage 3.5/14] –°–¢–ê–¢–ò–°–¢–ò–ö–ê: –°–∂–∞—Ç–∏–µ {compression_ratio:.1f}% ({original_length - final_length} —Å–∏–º–≤–æ–ª–æ–≤ —É–¥–∞–ª–µ–Ω–æ)")
        logger.info(f"[Stage 3.5/14] –°–¢–ê–¢–ò–°–¢–ò–ö–ê: –ò—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è: '{content[:100]}...'")
        
        return content
    
    def _assess_text_quality(self, text: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (0.0 - 1.0)"""
        
        if not text or len(text.strip()) < 10:
            return 0.0
        
        quality_score = 0.0
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–∏–µ –±—É–∫–≤—ã (–æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç)
        russian_chars = len(re.findall(r'[–∞-—è—ë]', text.lower()))
        total_chars = len(re.findall(r'[–∞-—è—ëa-z]', text.lower()))
        if total_chars > 0:
            russian_ratio = russian_chars / total_chars
            quality_score += russian_ratio * 0.4  # 40% –≤–µ—Å–∞
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ (–Ω–µ —Å–ª—É—á–∞–π–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã)
        words = text.split()
        meaningful_words = [w for w in words if len(w) > 2 and re.match(r'^[–∞-—è—ëa-z]+$', w.lower())]
        if len(words) > 0:
            meaningful_ratio = len(meaningful_words) / len(words)
            quality_score += meaningful_ratio * 0.3  # 30% –≤–µ—Å–∞
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø—É–Ω–∫—Ç—ã)
        structure_indicators = len(re.findall(r'[–ê-–Ø–Å][–∞-—è—ë\s]+:', text))  # –ó–∞–≥–æ–ª–æ–≤–∫–∏
        if len(text) > 0:
            structure_ratio = min(structure_indicators / (len(text) / 1000), 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            quality_score += structure_ratio * 0.2  # 20% –≤–µ—Å–∞
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        sentences = re.split(r'[.!?]+', text)
        avg_sentence_length = sum(len(s.split()) for s in sentences if s.strip()) / max(len(sentences), 1)
        if avg_sentence_length > 3:  # –ú–∏–Ω–∏–º—É–º 3 —Å–ª–æ–≤–∞ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏
            quality_score += 0.1  # 10% –≤–µ—Å–∞
        
        return min(quality_score, 1.0)
    
    
    
    
    
    
    
    def _extract_cad_specifications(self, content: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π –∏–∑ CAD —Ñ–∞–π–ª–æ–≤"""
        try:
            import re
            
            specifications = []
            blocks = []
            layers = []
            
            # –ò—â–µ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ
            spec_patterns = [
                r'–ø–æ–∑\.?\s*(\d+)',  # –ø–æ–∑–∏—Ü–∏—è
                r'–æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ[:\s]*([–ê-–Ø0-9\-\.]+)',  # –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–µ
                r'–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ[:\s]*([–ê-–Ø–∞-—è\s]+)',  # –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
                r'–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ[:\s]*(\d+)',  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            ]
            
            for pattern in spec_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    specifications.extend(matches)
            
            # –ò—â–µ–º –±–ª–æ–∫–∏
            block_patterns = [
                r'–±–ª–æ–∫[:\s]*([–ê-–Ø0-9\-\.]+)',
                r'block[:\s]*([A-Z0-9\-\.]+)',
            ]
            
            for pattern in block_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    blocks.extend(matches)
            
            # –ò—â–µ–º —Å–ª–æ–∏
            layer_patterns = [
                r'—Å–ª–æ–π[:\s]*([–ê-–Ø0-9\-\.]+)',
                r'layer[:\s]*([A-Z0-9\-\.]+)',
            ]
            
            for pattern in layer_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    layers.extend(matches)
            
            return {
                'specifications': specifications,
                'blocks': blocks,
                'layers': layers
            }
            
        except Exception as e:
            logger.error(f"CAD specifications extraction failed: {e}")
            return {}
    
    def _extract_bim_data(self, content: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ BIM –¥–∞–Ω–Ω—ã—Ö –∏–∑ XML/JSON"""
        try:
            import re
            
            objects = []
            properties = []
            relationships = []
            
            # –ò—â–µ–º BIM –æ–±—ä–µ–∫—Ç—ã
            object_patterns = [
                r'<Ifc[A-Z][a-zA-Z]*[^>]*>',  # IFC –æ–±—ä–µ–∫—Ç—ã
                r'"type":\s*"([^"]+)"',  # JSON —Ç–∏–ø—ã
            ]
            
            for pattern in object_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    objects.extend(matches)
            
            # –ò—â–µ–º —Å–≤–æ–π—Å—Ç–≤–∞
            prop_patterns = [
                r'<property[^>]*name="([^"]+)"[^>]*>',
                r'"property":\s*"([^"]+)"',
            ]
            
            for pattern in prop_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    properties.extend(matches)
            
            # –ò—â–µ–º —Å–≤—è–∑–∏
            rel_patterns = [
                r'<relationship[^>]*>',
                r'"relationship":\s*"([^"]+)"',
            ]
            
            for pattern in rel_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    relationships.extend(matches)
            
            return {
                'objects': objects,
                'properties': properties,
                'relationships': relationships
            }
            
        except Exception as e:
            logger.error(f"BIM data extraction failed: {e}")
            return {}
    
    def _extract_1c_data(self, content: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±–º–µ–Ω–∞ —Å 1–°"""
        try:
            import re
            
            objects = []
            transactions = []
            metadata = {}
            
            # –ò—â–µ–º –æ–±—ä–µ–∫—Ç—ã 1–°
            object_patterns = [
                r'<–û–±—ä–µ–∫—Ç[^>]*>',
                r'"object":\s*"([^"]+)"',
            ]
            
            for pattern in object_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    objects.extend(matches)
            
            # –ò—â–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
            trans_patterns = [
                r'<–¢—Ä–∞–Ω–∑–∞–∫—Ü–∏—è[^>]*>',
                r'"transaction":\s*"([^"]+)"',
            ]
            
            for pattern in trans_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    transactions.extend(matches)
            
            # –ò—â–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            meta_patterns = [
                r'<–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ[^>]*>',
                r'"metadata":\s*"([^"]+)"',
            ]
            
            for pattern in meta_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    metadata['found'] = len(matches)
            
            return {
                'objects': objects,
                'transactions': transactions,
                'metadata': metadata
            }
            
        except Exception as e:
            logger.error(f"1C data extraction failed: {e}")
            return {}
    
    def _analyze_scan_with_vlm(self, file_path: str) -> Dict:
        """VLM –∞–Ω–∞–ª–∏–∑ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
        try:
            if not self.vlm_available:
                return {}
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π VLM –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            vlm_result = self.vlm_processor.analyze_image(file_path)
            
            return {
                'text': vlm_result.get('text', ''),
                'tables': vlm_result.get('tables', []),
                'quality': vlm_result.get('quality', 0.0)
            }
            
        except Exception as e:
            logger.error(f"Scan VLM analysis failed: {e}")
            return {}
    
    def _analyze_archive_content(self, content: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∞—Ä—Ö–∏–≤–∞"""
        try:
            import re
            
            files = []
            structure = {}
            total_size = 0
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∞—Ä—Ö–∏–≤–∞ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            file_patterns = [
                r'=== ([^=]+) ===',
                r'File: ([^\n]+)',
            ]
            
            for pattern in file_patterns:
                matches = re.findall(pattern, content)
                if matches:
                    files.extend(matches)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
            total_size = len(content.encode('utf-8'))
            
            return {
                'files': files,
                'structure': structure,
                'total_size': total_size
            }
            
        except Exception as e:
            logger.error(f"Archive analysis failed: {e}")
            return {}
    
    
    
    
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        
        if not text:
            return ""
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–¥–∞–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–∫—Ä–æ–º–µ —Ä—É—Å—Å–∫–∏—Ö –±—É–∫–≤, —Ü–∏—Ñ—Ä –∏ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
        text = re.sub(r'[^\w\s\.\\,\\;\\:\\!\\?\\-\\(\\)\\[\\]\\"\\\'‚Ññ]', ' ', text, flags=re.UNICODE)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r' +', ' ', text)
        
        return text.strip()
    
    
    
    
    def _stage4_document_type_detection(self, content: str, file_path: str) -> Dict[str, Any]:
        """STAGE 4: Document Type Detection (—Å–∏–º–±–∏–æ—Ç–∏—á–µ—Å–∫–∏–π: regex + SBERT + RuLongformer)"""
        
        logger.info(f"[Stage 4/14] DOCUMENT TYPE DETECTION: {Path(file_path).name}")
        start_time = time.time()
        
        # [CRITICAL OVERRIDE] –ü–†–û–í–ï–†–ö–ê –ù–ê –°–ü/–ì–û–°–¢ –í –ù–ê–ß–ê–õ–ï –î–û–ö–£–ú–ï–ù–¢–ê (<0.1s)
        content_preview = content[:2000].lower()
        
        # üöÄ –£–°–ò–õ–ï–ù–ù–ê–Ø –¢–ò–ü–ò–ó–ê–¶–ò–Ø –ù–¢–î - –ü–û–î–¢–ò–ü–´ –ò –í–ê–õ–ò–î–ê–¶–ò–Ø
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –°–ü (–≤–∫–ª—é—á–∞—è –°–ü158.13330.2014)
        if re.search(r'^\s*—Å–ø[\s\d\.]', content_preview):
            final_result = {
                'doc_type': 'sp',
                'doc_subtype': 'sp',
                'confidence': 0.99,
                'source': 'CRITICAL_REGEX_OVERRIDE',
                'norm_type': 'building_codes'  # –°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã
            }
            elapsed = time.time() - start_time
            logger.info(f"[Stage 4/14] CRITICAL OVERRIDE - Type: {final_result['doc_type']}, Confidence: {final_result['confidence']:.2f} ({elapsed:.2f}s)")
            return final_result
            
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ì–û–°–¢
        if re.search(r'^\s*–≥–æ—Å—Ç[\s\d\.]', content_preview):
            final_result = {
                'doc_type': 'gost',
                'doc_subtype': 'gost',
                'confidence': 0.99,
                'source': 'CRITICAL_REGEX_OVERRIDE',
                'norm_type': 'state_standards'  # –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã
            }
            elapsed = time.time() - start_time
            logger.info(f"[Stage 4/14] CRITICAL OVERRIDE - Type: {final_result['doc_type']}, Confidence: {final_result['confidence']:.2f} ({elapsed:.2f}s)")
            return final_result
        
        # üöÄ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ü–û–î–¢–ò–ü–´ –ù–¢–î
        # –°–ù–∏–ü (—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –ø—Ä–∞–≤–∏–ª–∞)
        if re.search(r'^\s*—Å–Ω–∏–ø[\s\d\.]', content_preview):
            final_result = {
                'doc_type': 'snip',
                'doc_subtype': 'snip',
                'confidence': 0.99,
                'source': 'CRITICAL_REGEX_OVERRIDE',
                'norm_type': 'building_rules'  # –°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
            }
            elapsed = time.time() - start_time
            logger.info(f"[Stage 4/14] CRITICAL OVERRIDE - Type: {final_result['doc_type']}, Confidence: {final_result['confidence']:.2f} ({elapsed:.2f}s)")
            return final_result
        
        # ISO (–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã)
        if re.search(r'^\s*iso[\s\d\.]', content_preview):
            final_result = {
                'doc_type': 'iso',
                'doc_subtype': 'iso',
                'confidence': 0.99,
                'source': 'CRITICAL_REGEX_OVERRIDE',
                'norm_type': 'international_standards'  # –ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã
            }
            elapsed = time.time() - start_time
            logger.info(f"[Stage 4/14] CRITICAL OVERRIDE - Type: {final_result['doc_type']}, Confidence: {final_result['confidence']:.2f} ({elapsed:.2f}s)")
            return final_result
        
        # Regex –∞–Ω–∞–ª–∏–∑
        regex_result = self._regex_type_detection(content, file_path)
        
        # SBERT –∞–Ω–∞–ª–∏–∑
        sbert_result = self._sbert_type_detection(content)
        
        # ENTERPRISE RAG 6.0: GPU LLM –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        llm_classification = {}
        logger.info(f"[STAGE4_DEBUG] hasattr gpu_llm_model: {hasattr(self, 'gpu_llm_model')}")
        logger.info(f"[STAGE4_DEBUG] gpu_llm_model value: {getattr(self, 'gpu_llm_model', 'NOT_FOUND')}")
        logger.info(f"[STAGE4_DEBUG] self attributes: {[attr for attr in dir(self) if 'llm' in attr.lower()]}")
        # LLM –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if self.use_llm and hasattr(self, 'gpu_llm_model') and self.gpu_llm_model:
            logger.info("[STAGE4_DEBUG] About to call GPU LLM...")
            logger.info(f"[STAGE4_DEBUG] GPU LLM available: {self.gpu_llm_model is not None}")
            try:
                # –ì–ª—É–±–æ–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å GPU LLM (32K+ —Ç–æ–∫–µ–Ω–æ–≤)
                logger.info("[STAGE4_DEBUG] Calling classify_document...")
                llm_classification = self._classify_document_with_gpu_llm(content)
                logger.info(f"GPU LLM Classification: {llm_classification.get('document_type', 'unknown')} (confidence: {llm_classification.get('confidence', 0.0)})")
            except Exception as e:
                logger.warning(f"GPU LLM classification failed: {e}")
        else:
            logger.info("[STAGE4_DEBUG] LLM disabled ‚Äî using regex+SBERT fallback")
            llm_classification = {}
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–≤–∫–ª—é—á–∞—è LLM)
        final_result = self._combine_type_detection_enhanced(regex_result, sbert_result, llm_classification)
        
        elapsed = time.time() - start_time
        logger.info(f"[Stage 4/14] COMPLETE - Type: {final_result['doc_type']}, "
                   f"Subtype: {final_result['doc_subtype']}, "
                   f"Confidence: {final_result['confidence']:.2f} ({elapsed:.2f}s)")
        
        return final_result
    
    def _get_extended_document_types_mapping(self) -> Dict[str, Dict]:
        """üöÄ –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –°–û–û–¢–í–ï–¢–°–¢–í–ò–Ø –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ perekos.net (20+ —Ç–∏–ø–æ–≤)"""
        return {
            # === –ù–û–†–ú–ê–¢–ò–í–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´ (—Å –Ω–æ–º–µ—Ä–∞–º–∏) ===
            'gost': {
                'id_type': 'NUMBER',
                'patterns': [r'\b–≥–æ—Å—Ç[\s\.]*\d+', r'^\s*–≥–æ—Å—Ç\d+', r'–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π.*—Å—Ç–∞–Ω–¥–∞—Ä—Ç', r'–≥–æ—Å—Å—Ç–∞–Ω–¥–∞—Ä—Ç'],
                'priority': 1,
                'folder': 'norms/gost',
                'description': '–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç'
            },
            'sp': {
                'id_type': 'NUMBER', 
                'patterns': [r'\b—Å–ø[\s\.]*\d+', r'^\s*—Å–ø\d+', r'—Å–≤–æ–¥.*–ø—Ä–∞–≤–∏–ª', r'—Å–≤–æ–¥—ã.*–ø—Ä–∞–≤–∏–ª'],
                'priority': 1,
                'folder': 'norms/sp',
                'description': '–°–≤–æ–¥ –ø—Ä–∞–≤–∏–ª –ø–æ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤—É'
            },
            'snip': {
                'id_type': 'NUMBER',
                'patterns': [r'\b—Å–Ω–∏–ø\s+\d+', r'—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ.*–Ω–æ—Ä–º—ã', r'—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ.*–ø—Ä–∞–≤–∏–ª–∞'],
                'priority': 1,
                'folder': 'norms/snip', 
                'description': '–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –ø—Ä–∞–≤–∏–ª–∞'
            },
            'sanpin': {
                'id_type': 'NUMBER',
                'patterns': [r'\b—Å–∞–Ω–ø–∏–Ω\s+\d+', r'—Å–∞–Ω–∏—Ç–∞—Ä–Ω—ã–µ.*–ø—Ä–∞–≤–∏–ª–∞', r'—Å–∞–Ω–∏—Ç–∞—Ä–Ω—ã–µ.*–Ω–æ—Ä–º—ã'],
                'priority': 1,
                'folder': 'norms/sanpin',
                'description': '–°–∞–Ω–∏—Ç–∞—Ä–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –∏ –Ω–æ—Ä–º—ã'
            },
            'vsn': {
                'id_type': 'NUMBER',
                'patterns': [r'\b–≤—Å–Ω\s+\d+', r'–≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω—ã–µ.*—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ.*–Ω–æ—Ä–º—ã'],
                'priority': 1,
                'folder': 'norms/vsn',
                'description': '–í–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã'
            },
            # üöÄ –ß–ï–†–¢–ï–ñ–ò –ò –ü–†–û–ï–ö–¢–ù–ê–Ø –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø
            'drawing': {
                'id_type': 'NUMBER',
                'patterns': [r'—á–µ—Ä—Ç–µ–∂', r'—Å—Ö–µ–º–∞', r'—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è', r'–ª–∏—Å—Ç', r'–ø–ª–∞–Ω.*—Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è', r'—Ä–∞–∑—Ä–µ–∑', r'—Ñ–∞—Å–∞–¥'],
                'priority': 2,
                'folder': 'drawings',
                'description': '–ß–µ—Ä—Ç–µ–∂–∏ –∏ –ø—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è'
            },
            'drawing_cad': {
                'id_type': 'NUMBER',
                'patterns': [r'\.dwg$', r'\.dxf$', r'autocad', r'cad.*—Ñ–∞–π–ª'],
                'priority': 2,
                'folder': 'drawings/cad',
                'description': 'AutoCAD —á–µ—Ä—Ç–µ–∂–∏ (DWG/DXF)'
            },
            'scan_image': {
                'id_type': 'TITLE',
                'patterns': [r'\.(tiff|tif|png|jpg|jpeg)$', r'—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', r'–æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π'],
                'priority': 3,
                'folder': 'scans',
                'description': '–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è'
            },
            'bim': {
                'id_type': 'TITLE',
                'patterns': [r'ifc', r'bim', r'xml.*bim', r'building.*information'],
                'priority': 2,
                'folder': 'bim',
                'description': 'BIM –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã–µ'
            },
            '1c_exchange': {
                'id_type': 'NUMBER',
                'patterns': [r'1c', r'–æ–±–º–µ–Ω', r'–≤—ã–≥—Ä—É–∑–∫–∞.*1—Å', r'–∏–º–ø–æ—Ä—Ç.*1—Å'],
                'priority': 2,
                'folder': '1c',
                'description': '–î–∞–Ω–Ω—ã–µ –æ–±–º–µ–Ω–∞ —Å 1–°'
            },
            'archive': {
                'id_type': 'TITLE',
                'patterns': [r'\.(zip|rar)$', r'–∞—Ä—Ö–∏–≤', r'–ø–∞–ø–∫–∞.*–ø—Ä–æ–µ–∫—Ç–∞'],
                'priority': 3,
                'folder': 'archives',
                'description': '–ê—Ä—Ö–∏–≤—ã –ø—Ä–æ–µ–∫—Ç–æ–≤'
            },
            # üöÄ –°–ú–ï–¢–´ –ò –†–ê–°–¶–ï–ù–ö–ò
            'estimate': {
                'id_type': 'NUMBER',
                'patterns': [r'—Å–º–µ—Ç–∞', r'–ª–æ–∫–∞–ª—å–Ω–∞—è.*—Å–º–µ—Ç–∞', r'–ì–≠–°–ù', r'–§–ï–†', r'—Ä–µ—Å—É—Ä—Å–Ω–∞—è.*—Å–º–µ—Ç–∞', r'—Å–º–µ—Ç–Ω—ã–π.*—Ä–∞—Å—á—ë—Ç'],
                'priority': 2,
                'folder': 'estimates',
                'description': '–°–º–µ—Ç—ã –∏ —Ä–∞—Å—Ü–µ–Ω–∫–∏ (–ì–≠–°–ù, –§–ï–†, –ª–æ–∫–∞–ª—å–Ω—ã–µ —Å–º–µ—Ç—ã)'
            },
            # üöÄ –ü–†–û–ï–ö–¢–´ –ü–†–û–ò–ó–í–û–î–°–¢–í–ê –†–ê–ë–û–¢
            'ppr': {
                'id_type': 'TITLE',
                'patterns': [r'\b–ø–ø—Ä\b', r'–ø—Ä–æ–µ–∫—Ç\s+–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞\s+—Ä–∞–±–æ—Ç', r'—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è\s+–∫–∞—Ä—Ç–∞', r'—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è\s+–≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è'],
                'priority': 2,
                'folder': 'ppr',
                'description': '–ü—Ä–æ–µ–∫—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–∞—Ä—Ç—ã'
            },
            'mds': {
                'id_type': 'NUMBER',
                'patterns': [r'\b–º–¥—Å\s+\d+', r'–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∞—è.*–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', r'–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ.*—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏'],
                'priority': 1,
                'folder': 'norms/mds',
                'description': '–ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–µ'
            },
            'pnst': {
                'id_type': 'NUMBER',
                'patterns': [r'\b–ø–Ω—Å—Ç\s+\d+', r'–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π.*–Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π.*—Å—Ç–∞–Ω–¥–∞—Ä—Ç'],
                'priority': 1,
                'folder': 'norms/pnst',
                'description': '–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç'
            },
            'sto': {
                'id_type': 'NUMBER',
                'patterns': [r'\b—Å—Ç–æ\s+\d+', r'—Å—Ç–∞–Ω–¥–∞—Ä—Ç.*–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏', r'–Ω–æ—Å—Ç—Ä–æ–π'],
                'priority': 1,
                'folder': 'norms/sto',
                'description': '–ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –°–¢–û –ù–û–°–¢–†–û–ô'
            },
            
            # === –û–†–ì–ê–ù–ò–ó–ê–¶–ò–û–ù–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´ (—Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏) ===
            'ppr': {
                'id_type': 'TITLE',
                'patterns': [r'\b–ø–ø—Ä\b', r'–ø—Ä–æ–µ–∫—Ç.*–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞.*—Ä–∞–±–æ—Ç', r'–ø—Ä–æ–µ–∫—Ç.*–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞'],
                'priority': 2,
                'folder': 'org_docs/ppr',
                'description': '–ü—Ä–æ–µ–∫—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç'
            },
            'ttk': {
                'id_type': 'TITLE',
                'patterns': [r'—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è.*–∫–∞—Ä—Ç–∞', r'\b—Ç—Ç–∫\b', r'—Ç–∏–ø–æ–≤–∞—è.*—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è.*–∫–∞—Ä—Ç–∞'],
                'priority': 2,
                'folder': 'org_docs/ttk',
                'description': '–¢–∏–ø–æ–≤—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–∞—Ä—Ç—ã'
            },
            'tu': {
                'id_type': 'NUMBER',
                'patterns': [r'\b—Ç—É\s+\d+', r'—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ.*—É—Å–ª–æ–≤–∏—è', r'—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ.*—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è'],
                'priority': 1,
                'folder': 'org_docs/tu',
                'description': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è'
            },
            'form': {
                'id_type': 'TITLE',
                'patterns': [r'—Ñ–æ—Ä–º–∞.*–¥–æ–∫—É–º–µ–Ω—Ç–∞', r'–∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è.*–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è', r'–æ—Ç—á–µ—Ç–Ω–∞—è.*–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è'],
                'priority': 2,
                'folder': 'org_docs/forms',
                'description': '–§–æ—Ä–º—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è'
            },
            'album': {
                'id_type': 'TITLE',
                'patterns': [r'–∞–ª—å–±–æ–º.*—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö.*—Ä–µ—à–µ–Ω–∏–π', r'–∞–ª—å–±–æ–º.*—Ä–µ—à–µ–Ω–∏–π', r'—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ.*—Ä–µ—à–µ–Ω–∏—è'],
                'priority': 2,
                'folder': 'org_docs/albums',
                'description': '–ê–ª—å–±–æ–º—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π'
            },
            
            # === –û–ë–†–ê–ó–û–í–ê–¢–ï–õ–¨–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´ (—Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∏ –∞–≤—Ç–æ—Ä–∞–º–∏) ===
            'book': {
                'id_type': 'TITLE',
                'patterns': [r'—É—á–µ–±–Ω–∏–∫', r'—É—á–µ–±–Ω–æ–µ.*–ø–æ—Å–æ–±–∏–µ', r'–∫–Ω–∏–≥–∞', r'—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ'],
                'priority': 2,
                'folder': 'learning/books',
                'description': '–ö–Ω–∏–≥–∏ –∏ —É—á–µ–±–Ω–∏–∫–∏ –ø–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤—É'
            },
            'manual': {
                'id_type': 'TITLE',
                'patterns': [r'–ø–æ—Å–æ–±–∏–µ', r'—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', r'–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', r'–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–æ–µ.*–ø–æ—Å–æ–±–∏–µ'],
                'priority': 2,
                'folder': 'learning/manuals',
                'description': '–ü–æ—Å–æ–±–∏—è –≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–µ'
            },
            'lecture': {
                'id_type': 'TITLE',
                'patterns': [r'–ª–µ–∫—Ü–∏—è', r'–ª–µ–∫—Ü–∏–æ–Ω–Ω—ã–π.*–º–∞—Ç–µ—Ä–∏–∞–ª', r'–∫—É—Ä—Å.*–ª–µ–∫—Ü–∏–π'],
                'priority': 2,
                'folder': 'learning/lectures',
                'description': '–õ–µ–∫—Ü–∏–∏ –ø–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—è–º'
            },
            'journal': {
                'id_type': 'TITLE',
                'patterns': [r'–∂—É—Ä–Ω–∞–ª', r'–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ.*–∏–∑–¥–∞–Ω–∏–µ', r'—Å—Ç–∞—Ç—å—è', r'–ø—É–±–ª–∏–∫–∞—Ü–∏—è'],
                'priority': 2,
                'folder': 'learning/journals',
                'description': '–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–µ –∏–∑–¥–∞–Ω–∏—è –ø–æ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤—É'
            },
            
            # === –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –î–û–ö–£–ú–ï–ù–¢–´ ===
            'smeta': {
                'id_type': 'NUMBER',
                'patterns': [r'\b—Å–º–µ—Ç–∞\b', r'—Ä–∞—Å—Ü–µ–Ω–∫', r'–∫–∞–ª—å–∫—É–ª—è—Ü', r'—Å—Ç–æ–∏–º–æ—Å—Ç—å.*—Ä–∞–±–æ—Ç', r'\b–≥—ç—Å–Ω\b', r'\b—Ñ–µ—Ä\b'],
                'priority': 1,
                'folder': 'finance/smeta',
                'description': '–°–º–µ—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è'
            },
            'safety': {
                'id_type': 'TITLE',
                'patterns': [r'–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å.*—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞', r'–ø—Ä–∞–≤–∏–ª–∞.*–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏', r'–æ—Ö—Ä–∞–Ω–∞.*—Ç—Ä—É–¥–∞'],
                'priority': 2,
                'folder': 'safety/rules',
                'description': '–ü—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –≤ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–µ'
            },
            'materials': {
                'id_type': 'TITLE',
                'patterns': [r'–º–∞—Ç–µ—Ä–∏–∞–ª—ã.*–¥–ª—è.*–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è', r'–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ.*–ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏.*—Ä–∞—Å—Ö–æ–¥–∞', r'—Ä–∞—Å—Ö–æ–¥.*–º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤'],
                'priority': 2,
                'folder': 'materials/docs',
                'description': '–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è'
            }
        }
    
    def _regex_type_detection(self, content: str, file_path: str) -> Dict[str, Any]:
        """üöÄ –†–ê–°–®–ò–†–ï–ù–ù–û–ï Regex-based –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è perekos.net"""
        
        filename = Path(file_path).name.lower()
        content_lower = content.lower()[:15000]  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Ç–∏–ø–æ–≤
        type_mapping = self._get_extended_document_types_mapping()
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        type_patterns = {}
        for doc_type, type_info in type_mapping.items():
            type_patterns[doc_type] = {
                'patterns': type_info['patterns'],
                'id_type': type_info['id_type'],
                'priority': type_info['priority'],
                'folder': type_info['folder']
        }
        
        best_type = 'other'
        best_subtype = 'general'
        best_score = 0.0
        best_id_type = 'TITLE'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ü–†–ò–û–†–ò–¢–ï–¢: –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –Ω–æ–º–µ—Ä–∞–º–∏ –∏–º–µ—é—Ç –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        normative_priority = 0.0
        normative_patterns = [r'\b—Å–ø\s+\d+', r'\b–≥–æ—Å—Ç\s+\d+', r'\b—Å–Ω–∏–ø\s+\d+', r'\b—Å–∞–Ω–ø–∏–Ω\s+\d+', r'\b–≤—Å–Ω\s+\d+', r'\b–º–¥—Å\s+\d+']
        for pattern in normative_patterns:
            if re.search(pattern, content_lower) or re.search(pattern, filename):
                normative_priority = 15.0  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                break
        
        # üöÄ –£–õ–£–ß–®–ï–ù–ò–ï –¢–û–ß–ù–û–°–¢–ò: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        equipment_patterns = [
            r'[–ê-–Ø]{2,}\d{1,3}-[–ê-–Ø0-9]{1,5}',  # –ì–©–£–í1, –¢—É–Ω–≥—É—Å-10–°—Ç
            r'[–ê-–Ø]\d+[–ê-–Ø]-\w+',                # –í7–ê-W5
            r'[–ê-–Ø]{2,}\d+[–ê-–Ø]-\w+',            # –†—É–ø–æ—Ä-5–ê-–ú
        ]
        
        estimate_patterns = [
            r'–ì–≠–°–ù[—Ä]?-\w+-\d{1,3}',             # –ì–≠–°–ù-–û–ü-51
            r'–§–ï–†[—Ä]?-\w+-\d{1,3}',             # –§–ï–†-–û–ü-51
            r'–ì–≠–°–ù—Ä-\w+-\d{1,3}',               # –ì–≠–°–ù—Ä-–û–ü-51
            r'–§–ï–†—Ä-\w+-\d{1,3}',                # –§–ï–†—Ä-–û–ü-51
        ]
        
        drawing_patterns = [
            r'[–ê-–Ø0-9]{2,}-[–ê-–Ø0-9]{2,}\.\w+\.\d{1,3}',  # –ù–û–§-–ü–†–û.1-–û–í.–õ8
            r'[–ê-–Ø]{2,}\.\d+\.\d+',                       # –ê–†.1.1
            r'[–ê-–Ø]{2,}-\d+\.\d+',                       # –ê–†-1.1
        ]
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        equipment_matches = sum(len(re.findall(pattern, content_lower)) for pattern in equipment_patterns)
        estimate_matches = sum(len(re.findall(pattern, content_lower)) for pattern in estimate_patterns)
        drawing_matches = sum(len(re.findall(pattern, content_lower)) for pattern in drawing_patterns)
        
        logger.info(f"[ACCURACY] Enhanced patterns: equipment={equipment_matches}, estimate={estimate_matches}, drawing={drawing_matches}")
        
        for doc_type, type_info in type_patterns.items():
            score = 0.0
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            for pattern in type_info['patterns']:
                matches_content = len(re.findall(pattern, content_lower))
                matches_filename = len(re.findall(pattern, filename))
                score += matches_content * 0.7 + matches_filename * 0.9
            
            # –ù–û–†–ú–ê–¢–ò–í–ù–´–ô –ü–†–ò–û–†–ò–¢–ï–¢: –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
            if normative_priority > 0 and type_info['id_type'] == 'NUMBER':
                score += normative_priority
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ –ü–û –¢–ò–ü–£: –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤–∞–∂–Ω–µ–µ
            if type_info['priority'] == 1:
                score += 5.0
            
            if score > best_score:
                best_score = score
                best_type = doc_type
                best_id_type = type_info['id_type']
                best_subtype = doc_type  # –ò—Å–ø–æ–ª—å–∑—É–µ–º doc_type –∫–∞–∫ subtype –¥–ª—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏
        
        confidence = min(best_score / 15.0, 1.0) if best_score > 0 else 0.1
        
        return {
            'doc_type': best_type,
            'doc_subtype': best_subtype,
            'id_type': best_id_type,
            'confidence': confidence,
            'method': 'regex',
            'folder': type_mapping.get(best_type, {}).get('folder', 'other')
        }
    
    def _sbert_type_detection(self, content: str) -> Dict[str, Any]:
        """SBERT-based —Ç–∏–ø –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"""
        
        if not self.sbert_model or not HAS_ML_LIBS:
            return {
                'doc_type': 'other',
                'doc_subtype': 'general',
                'confidence': 0.0,
                'method': 'sbert_unavailable'
            }
        
        try:
            # üöÄ –†–ê–°–®–ò–†–ï–ù–ù–´–ï –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ï –®–ê–ë–õ–û–ù–´ –¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            type_templates = {
                # –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                'gost': "–ì–û–°–¢ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è",
                'sp': "—Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –Ω–æ—Ä–º—ã",
                'snip': "–°–ù–∏–ü —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –ø—Ä–∞–≤–∏–ª–∞ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã",
                'sanpin': "–°–∞–Ω–ü–∏–ù —Å–∞–Ω–∏—Ç–∞—Ä–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –Ω–æ—Ä–º—ã –≥–∏–≥–∏–µ–Ω–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∑–¥–æ—Ä–æ–≤—å—è",
                'vsn': "–í–°–ù –≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –æ—Ç—Ä–∞—Å–ª–µ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
                'mds': "–ú–î–° –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –º–µ—Ç–æ–¥–∏–∫–∏",
                'pnst': "–ü–ù–°–¢ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–æ—Ä–º—ã",
                'sto': "–°–¢–û —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –ù–û–°–¢–†–û–ô –æ—Ç—Ä–∞—Å–ª–µ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã",
                
                # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                'ppr': "–ø—Ä–æ–µ–∫—Ç –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —ç—Ç–∞–ø—ã —Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–∞",
                'ttk': "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∞ —Ç–∏–ø–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Ç—Ä—É–¥–æ–≤—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è —Ä–∞–±–æ—Ç",
                'tu': "—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–æ–ø—É—Å–∫–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏",
                'form': "—Ñ–æ—Ä–º–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å –±–ª–∞–Ω–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–æ–±–æ—Ä–æ—Ç",
                'album': "–∞–ª—å–±–æ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏–π –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è —á–µ—Ä—Ç–µ–∂–∏ —Å—Ö–µ–º—ã",
                
                # –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                'book': "—É—á–µ–±–Ω–∏–∫ –∫–Ω–∏–≥–∞ —É—á–µ–±–Ω–æ–µ –ø–æ—Å–æ–±–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
                'manual': "–ø–æ—Å–æ–±–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –º–µ—Ç–æ–¥–∏—á–µ—Å–∫–æ–µ –ø–æ—Å–æ–±–∏–µ –æ–±—É—á–µ–Ω–∏–µ",
                'lecture': "–ª–µ–∫—Ü–∏—è –ª–µ–∫—Ü–∏–æ–Ω–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª –∫—É—Ä—Å –ª–µ–∫—Ü–∏–π –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–π –º–∞—Ç–µ—Ä–∏–∞–ª",
                'journal': "–∂—É—Ä–Ω–∞–ª –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –∏–∑–¥–∞–Ω–∏–µ —Å—Ç–∞—Ç—å—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è –Ω–∞—É—á–Ω–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞",
                
                # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
                'smeta': "—Å–º–µ—Ç–∞ —Ä–∞—Å—Ü–µ–Ω–∫–∏ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∫–∞–ª—å–∫—É–ª—è—Ü–∏—è —Ü–µ–Ω–∞ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –æ–±—ä–µ–º —Ä–∞–±–æ—Ç",
                'safety': "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞ –ø—Ä–∞–≤–∏–ª–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Ç–µ—Ö–Ω–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
                'materials': "–º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Ä–∞—Å—Ö–æ–¥ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤"
            }
            
            content_words = content.split()[:3000]  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            content_sample = ' '.join(content_words)
            content_embedding = self.sbert_model.encode([content_sample])
            
            template_embeddings = self.sbert_model.encode(list(type_templates.values()))
            
            similarities = []
            for i, template_emb in enumerate(template_embeddings):
                sim = np.dot(content_embedding[0], template_emb) / (
                    np.linalg.norm(content_embedding[0]) * np.linalg.norm(template_emb)
                )
                similarities.append(sim)
            
            best_idx = max(range(len(similarities)), key=lambda i: similarities[i])
            best_type = list(type_templates.keys())[best_idx]
            confidence = max(similarities)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–µ –∏–∑ mapping
            type_mapping = self._get_extended_document_types_mapping()
            type_info = type_mapping.get(best_type, {})
            
            return {
                'doc_type': best_type if confidence > 0.3 else 'other',
                'doc_subtype': best_type,
                'id_type': type_info.get('id_type', 'TITLE'),
                'confidence': float(confidence),
                'method': 'sbert',
                'folder': type_info.get('folder', 'other')
            }
            
        except Exception as e:
            logger.warning(f"SBERT type detection failed: {e}")
            return {
                'doc_type': 'other',
                'doc_subtype': 'general',
                'confidence': 0.0,
                'method': 'sbert_error'
            }
    
    def _combine_type_detection(self, regex_result: Dict, sbert_result: Dict) -> Dict[str, Any]:
        """üöÄ –£–õ–£–ß–®–ï–ù–ù–û–ï –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ regex –∏ SBERT –¥–ª—è perekos.net"""
        
        # –ü–†–ò–û–†–ò–¢–ï–¢: –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –Ω–æ–º–µ—Ä–∞–º–∏ –∏–º–µ—é—Ç –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        normative_types = ['gost', 'sp', 'snip', 'sanpin', 'vsn', 'mds', 'pnst', 'sto', 'tu']
        
        # –ï—Å–ª–∏ regex –Ω–∞—à–µ–ª –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –µ–º—É
        if regex_result['doc_type'] in normative_types and regex_result['confidence'] > 0.5:
            return {
                'doc_type': regex_result['doc_type'],
                'doc_subtype': regex_result['doc_subtype'],
                'id_type': regex_result.get('id_type', 'NUMBER'),
                'confidence': regex_result['confidence'],
                'folder': regex_result.get('folder', 'other'),
                'methods_used': f"regex_priority({regex_result['confidence']:.2f})"
            }
        
        # –ï—Å–ª–∏ —Ç–∏–ø—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç, –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if regex_result['doc_type'] == sbert_result['doc_type']:
            combined_confidence = min(
                (regex_result['confidence'] + sbert_result['confidence']) / 2 + 0.1,
                1.0
            )
            doc_type = regex_result['doc_type']
            doc_subtype = regex_result['doc_subtype']
            id_type = regex_result.get('id_type', sbert_result.get('id_type', 'TITLE'))
            folder = regex_result.get('folder', sbert_result.get('folder', 'other'))
        else:
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if regex_result['confidence'] > sbert_result['confidence']:
                doc_type = regex_result['doc_type']
                doc_subtype = regex_result['doc_subtype']
                id_type = regex_result.get('id_type', 'TITLE')
                folder = regex_result.get('folder', 'other')
                combined_confidence = regex_result['confidence']
            else:
                doc_type = sbert_result['doc_type']
                doc_subtype = sbert_result['doc_subtype']
                id_type = sbert_result.get('id_type', 'TITLE')
                folder = sbert_result.get('folder', 'other')
                combined_confidence = sbert_result['confidence']
        
        return {
            'doc_type': doc_type,
            'doc_subtype': doc_subtype,
            'id_type': id_type,
            'confidence': combined_confidence,
            'folder': folder,
            'methods_used': f"regex({regex_result['confidence']:.2f}) + sbert({sbert_result['confidence']:.2f})"
        }
    
    def _stage5_structural_analysis(self, content: str, doc_type_info: Dict) -> Dict[str, Any]:
        """STAGE 5: SBERT-based Structural Analysis (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞)"""
        
        logger.info(f"[Stage 5/14] SBERT STRUCTURAL ANALYSIS - Type: {doc_type_info['doc_type']}")
        start_time = time.time()
        
        # üöÄ CONTEXT SWITCHING: –ó–∞–≥—Ä—É–∂–∞–µ–º SBERT —Ç–æ–ª—å–∫–æ –¥–ª—è Stage 5
        self.sbert_model = self._load_sbert_model()
        logger.info(f"[VRAM MANAGER] SBERT loaded for Stage 5")
        
        try:
            # üî¨ –°–£–ü–ï–†-DEBUG –î–õ–Ø Stage 5
            logger.info(f"[DEBUG_STAGE5] Stage 5 –Ω–∞—á–∞–ª—Å—è –≤ {time.strftime('%H:%M:%S')}")
            logger.info(f"[DEBUG_STAGE5] SBERT –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞: {self.sbert_model is not None}")
            logger.info(f"[DEBUG_STAGE5] HAS_ML_LIBS: {HAS_ML_LIBS}")
            
            if self.sbert_model:
                if hasattr(self.sbert_model, 'device'):
                    logger.info(f"[DEBUG_STAGE5] SBERT device: {self.sbert_model.device}")
                else:
                    logger.warning(f"[DEBUG_STAGE5] SBERT device –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω!")
            
            # üö® –î–û–ë–ê–í–õ–Ø–ï–ú DEBUG –û –ú–û–î–ï–õ–ò –í STAGE 5
            logger.info("üö® STAGE5 SBERT DEBUG: Checking which model is loaded...")
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ _modules
                if hasattr(self.sbert_model, '_modules'):
                    for name, module in self.sbert_model._modules.items():
                        if hasattr(module, 'config') and hasattr(module.config, 'name_or_path'):
                            logger.info(f"üö® STAGE5 SBERT DEBUG: Found model = {module.config.name_or_path}")
                            break
                else:
                    logger.info("üö® STAGE5 SBERT DEBUG: Model modules not accessible")
            except Exception as e:
                logger.info(f"üö® STAGE5 SBERT DEBUG: Could not determine model name: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤
            try:
                embedding_dim = self.sbert_model.get_sentence_embedding_dimension()
                logger.info(f"üö® STAGE5 SBERT DEBUG: Embedding dimension = {embedding_dim}")
                if embedding_dim == 768:
                    logger.info("üö® STAGE5 SBERT DEBUG: Using PAVLOV BASE model (768 dims)")
                elif embedding_dim == 312:
                    logger.info("üö® STAGE5 SBERT DEBUG: Using TINY2 model (312 dims)")
                else:
                    logger.info(f"üö® STAGE5 SBERT DEBUG: Unknown model size ({embedding_dim} dims)")
            except Exception as e:
                logger.info(f"üö® STAGE5 SBERT DEBUG: Could not get embedding dimension: {e}")
            
            if not self.sbert_model or not HAS_ML_LIBS:
                logger.warning("SBERT not available, using fallback chunker")
                return self._structural_analysis_fallback(content)
            # üî¨ DEBUG: –ù–∞—á–∏–Ω–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π
            logger.info(f"[DEBUG_STAGE5] –ù–∞—á–∏–Ω–∞–µ–º _sbert_section_detection –≤ {time.strftime('%H:%M:%S')}")
            section_start = time.time()
            
            # ENTERPRISE RAG 3.0: –ü–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è
            semantic_sections = self._sbert_section_detection(content, doc_type_info, self.sbert_model)
            
            section_end = time.time()
            logger.info(f"[DEBUG_STAGE5] _sbert_section_detection –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {section_end - section_start:.2f}s")
            
            # üî¨ DEBUG: –ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
            logger.info(f"[DEBUG_STAGE5] –ù–∞—á–∏–Ω–∞–µ–º _sbert_table_detection –≤ {time.strftime('%H:%M:%S')}")
            table_start = time.time()
            
            # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü —Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π
            semantic_tables = self._sbert_table_detection(content, self.sbert_model)
            
            table_end = time.time()
            logger.info(f"[DEBUG_STAGE5] _sbert_table_detection –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {table_end - table_start:.2f}s")
            
            # ENTERPRISE: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            semantic_lists = self._enhanced_list_detection(content)
            
            # ENTERPRISE RAG 3.0: VLM –∞–Ω–∞–ª–∏–∑ –¥–ª—è PDF —Ñ–∞–π–ª–æ–≤ —Å Smart Chunking
            vlm_tables = []
            vlm_metadata = {}
            if self.vlm_available and self._current_file_path.endswith('.pdf'):
                try:
                    # üöÄ –£–ú–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –í–°–ï —Ñ–∞–π–ª—ã, –Ω–æ —Å —É–º–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
                    logger.info(f"[VLM_ANALYSIS] Starting comprehensive VLM analysis for complete content extraction")
                    
                    # Smart Chunking VLM –∞–Ω–∞–ª–∏–∑ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö
                    vlm_analysis = self._smart_vlm_analysis(self._current_file_path)
                    vlm_tables = vlm_analysis.get('tables', [])
                    vlm_metadata = vlm_analysis
                    logger.info(f"Smart VLM analysis: {len(vlm_tables)} tables found, {vlm_analysis.get('total_chunks', 0)} chunks processed")
                except Exception as e:
                    logger.warning(f"Smart VLM analysis failed: {e}")
            
            # ENTERPRISE RAG 4.0: –õ–æ–∫–∞–ª—å–Ω—ã–π —Ä–æ—Å—Å–∏–π—Å–∫–∏–π LLM –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            russian_llm_analysis = {}
            if hasattr(self, 'russian_llm_processor') and self.russian_llm_processor:
                try:
                    # –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å –ª–æ–∫–∞–ª—å–Ω—ã–º —Ä–æ—Å—Å–∏–π—Å–∫–∏–º LLM
                    russian_llm_analysis = self._russian_llm_deep_analysis(content, vlm_metadata)
                    logger.info(f"Local Russian LLM analysis: {russian_llm_analysis.get('analysis', {}).get('document_type', 'unknown')} document type detected")
                except Exception as e:
                    logger.warning(f"Local Russian LLM analysis failed: {e}")
            
            # STAGE 5.5: Deep Semantic Analysis —Å –ª–æ–∫–∞–ª—å–Ω—ã–º LLM
            deep_semantic_analysis = {}
            if russian_llm_analysis.get('local_llm_available', False):
                try:
                    # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã VLM + LLM –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
                    deep_semantic_analysis = self._stage_5_5_deep_semantic_analysis(
                        content, vlm_metadata, russian_llm_analysis
                    )
                    logger.info(f"Stage 5.5 Deep Analysis: {deep_semantic_analysis.get('enhanced_sections', 0)} enhanced sections, {deep_semantic_analysis.get('extracted_entities', 0)} entities")
                except Exception as e:
                    logger.warning(f"Stage 5.5 Deep Analysis failed: {e}")
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—ã –∏–∑ regex –∏ VLM
            all_tables = semantic_tables + vlm_tables
            
            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –¥–µ—Ä–µ–≤–∞
            hierarchical_tree = self._build_hierarchical_tree(semantic_sections, all_tables, semantic_lists)
            
            # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —á–µ—Ä–µ–∑ SBERT
            hierarchical_structure = self._sbert_hierarchy_analysis(semantic_sections, content)
            
            # !!! –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –î–æ–±–∞–≤–ª—è–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –ø—É—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞! !!!
            enhanced_sections = self._add_hierarchy_paths(semantic_sections, hierarchical_structure)
            
            structural_data = {
                'sections': enhanced_sections,
                'paragraphs_count': sum(len(s['content'].split('\n')) for s in enhanced_sections),
                'tables': all_tables,
                'lists': semantic_lists,
                'hierarchy': hierarchical_structure,
                'hierarchical_tree': hierarchical_tree,
                'structural_completeness': self._calculate_structural_completeness(enhanced_sections),
                'total_sections': len(enhanced_sections),
                'total_tables': len(all_tables),
                'total_lists': len(semantic_lists),
                'complexity_score': self._calculate_structure_complexity(enhanced_sections, all_tables, semantic_lists),
                'analysis_method': 'sbert_semantic_vlm' if vlm_tables else 'sbert_semantic',
                'vlm_available': self.vlm_available,
                'vlm_tables_count': len(vlm_tables)
            }
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if doc_type_info['doc_type'] == 'norms':
                structural_data = self._enhance_norms_structure(structural_data, content)
            elif doc_type_info['doc_type'] == 'ppr':
                structural_data = self._enhance_ppr_structure(structural_data, content)
            elif doc_type_info['doc_type'] == 'smeta':
                structural_data = self._enhance_smeta_structure(structural_data, content)
            
        except Exception as e:
            logger.error(f"SBERT structural analysis failed: {e}")
            structural_data = self._structural_analysis_fallback(content)
        
        finally:
            # üöÄ CONTEXT SWITCHING: –í—ã–≥—Ä—É–∂–∞–µ–º SBERT –ø–æ—Å–ª–µ Stage 5
            self._unload_sbert_model()
            logger.info(f"[VRAM MANAGER] SBERT unloaded after Stage 5")
        
        elapsed = time.time() - start_time
        
        # üî¨ –§–ò–ù–ê–õ–¨–ù–´–ô DEBUG –î–õ–Ø Stage 5
        logger.info(f"[DEBUG_STAGE5] Stage 5 –∑–∞–≤–µ—Ä—à–µ–Ω –≤ {time.strftime('%H:%M:%S')}")
        logger.info(f"[DEBUG_STAGE5] –û–±—â–µ–µ –≤—Ä–µ–º—è Stage 5: {elapsed:.2f}s")
        
        sections_count = len(structural_data.get('sections', []))
        paragraphs_count = structural_data.get('paragraphs_count', 0)
        tables_count = len(structural_data.get('tables', []))
        
        logger.info(f"[Stage 5/14] COMPLETE - Sections: {sections_count}, "
                   f"Paragraphs: {paragraphs_count}, Tables: {tables_count} ({elapsed:.2f}s)")
        
        return structural_data
    
    def _fallback_chunking(self, content: str) -> List:
        """Fallback chunking when _stage13_smart_chunking is not available"""
        logger.warning("[FALLBACK] Using simple chunking instead of smart chunking")
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 100]
        
        chunks = []
        for i, paragraph in enumerate(paragraphs[:50]):  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 50 –∞–±–∑–∞—Ü–∞–º–∏
            chunk = {
                'id': f'chunk_{i}',
                'content': paragraph,
                'type': 'paragraph',
                'position': i
            }
            chunks.append(chunk)
        
        logger.info(f"[FALLBACK] Created {len(chunks)} simple chunks")
        return chunks
    
    def _smart_vlm_analysis(self, pdf_path: str) -> Dict:
        """
        Smart VLM –∞–Ω–∞–ª–∏–∑ —Å —á–∞–Ω–∫–∏–Ω–≥–æ–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        –†–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è 512 —Ç–æ–∫–µ–Ω–æ–≤ –±–µ–∑ –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        """
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º PyMuPDF –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
            import fitz
            from PIL import Image
            import io
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å PyMuPDF
            doc = fitz.open(pdf_path)
            images = []
            
            for page_num in range(len(doc)):
                try:
                    page = doc[page_num]
                    mat = fitz.Matrix(300/72, 300/72)  # 300 DPI
                    pix = page.get_pixmap(matrix=mat, alpha=False)
                    
                    img_data = pix.tobytes("png")
                    
                    # [SUCCESS] –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–∞–º–∏
                    with Image.open(io.BytesIO(img_data)) as img:
                        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        if img.size[0] > 0 and img.size[1] > 0:
                            # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
                            images.append(img.copy())
                        
                except Exception as e:
                    logger.error(f"Failed to convert page {page_num + 1}: {e}")
                    continue
            
            doc.close()
            
            if not images:
                return {'vlm_available': False, 'tables': [], 'structure': 'no_images'}
            
            # üöÄ VLM –î–õ–Ø –¢–ê–ë–õ–ò–¶: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –í–°–ï —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü –∏ —Ä–∏—Å—É–Ω–∫–æ–≤
            logger.info(f"[VLM_ANALYSIS] Document has {len(images)} pages - analyzing ALL pages for table detection")
            
            # Smart Chunking –∞–Ω–∞–ª–∏–∑ –í–°–ï–• —Å—Ç—Ä–∞–Ω–∏—Ü —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–æ—Ç—á–µ—Ç–æ–º
            all_tables = []
            total_chunks = 0
            
            logger.info(f"[VLM_PROGRESS] Starting analysis of {len(images)} pages...")
            
            for page_num, image in enumerate(images):  # –í–°–ï –°–¢–†–ê–ù–ò–¶–´!
                # –ü—Ä–æ–≥—Ä–µ—Å—Å-–æ—Ç—á–µ—Ç –∫–∞–∂–¥—ã–µ 10 —Å—Ç—Ä–∞–Ω–∏—Ü
                if page_num % 10 == 0:
                    logger.info(f"[VLM_PROGRESS] Processing page {page_num + 1}/{len(images)} ({(page_num/len(images)*100):.1f}%)")
                try:
                    # üöÄ –£–ú–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–∂–µ —Ç–µ–º–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –Ω–æ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
                    import numpy as np
                    img_array = np.array(image.convert('L'))  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ grayscale
                    mean_brightness = np.mean(img_array)
                    
                    if mean_brightness < 30:  # –¢–µ–º–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
                        logger.info(f"[VLM_SMART] Dark page {page_num + 1} (brightness: {mean_brightness:.1f}) - using enhanced processing")
                        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è —Ç–µ–º–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
                        # –ù–∞–ø—Ä–∏–º–µ—Ä, —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
                    
                    # Smart Chunking –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                    page_tables, chunks_processed = self._analyze_page_smart_chunking(image, page_num)
                    all_tables.extend(page_tables)
                    total_chunks += chunks_processed
                    
                except Exception as e:
                    logger.error(f"Smart chunking failed for page {page_num + 1}: {e}")
                    continue
            
            # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –æ –ø–æ–ª–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ
            logger.info(f"[VLM_COMPLETE] Full analysis completed: {len(all_tables)} tables found, {total_chunks} chunks processed across {len(images)} pages")
            
            return {
                'vlm_available': True,
                'tables': all_tables,
                'total_tables': len(all_tables),
                'total_chunks': total_chunks,
                'pages_processed': len(images),
                'structure': 'smart_chunking_pymupdf_full_content'
            }
            
        except Exception as e:
            logger.error(f"Smart VLM analysis failed: {e}")
            return {'vlm_available': False, 'tables': [], 'structure': 'error'}
    
    def _analyze_page_smart_chunking(self, image: Image.Image, page_num: int) -> Tuple[List[Dict], int]:
        """
        –£–õ–£–ß–®–ï–ù–ù–´–ô Smart Chunking –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–ª–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö
        –†–∞–∑–¥–µ–ª—è–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ 512 —Ç–æ–∫–µ–Ω–æ–≤ –ë–ï–ó –ø–æ—Ç–µ—Ä–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if image.size[0] == 0 or image.size[1] == 0:
                return [], 0
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è OOM
            if image.size[0] > 1500 or image.size[1] > 1500:
                image.thumbnail((1500, 1500), Image.Resampling.LANCZOS)
            
            # –ü–ï–†–í–´–ô –ü–†–û–•–û–î: –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Ç–æ–∫–µ–Ω–æ–≤
            test_inputs = self.vlm_processor.layout_processor(
                image, 
                return_tensors="pt",
                max_length=512,
                truncation=True,
                padding=True
            )
            
            input_ids = test_inputs.get('input_ids', None)
            if input_ids is None:
                return [], 0
            
            token_count = input_ids.shape[1]
            
            if token_count <= 512:
                # –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –ª–∏–º–∏—Ç - –æ–±—ã—á–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                return self._analyze_single_chunk(image, page_num, 0)
            else:
                # –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–∞—è - –Ω—É–∂–µ–Ω Smart Chunking
                logger.info(f"[PAGE] Page {page_num + 1} has {token_count} tokens, applying Smart Chunking")
                return self._analyze_page_with_chunking(image, page_num, token_count)
            
        except RuntimeError as e:
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ CUDA –æ—à–∏–±–æ–∫
            if "device-side assert triggered" in str(e) or "srcSelectDimSize" in str(e):
                logger.error(f"üõë CUDA assertion failed on page {page_num + 1}. Skipping: {e}")
                return [], 0
            raise e
        except Exception as e:
            logger.error(f"Smart chunking analysis failed for page {page_num + 1}: {e}")
            return [], 0
    
    def _analyze_single_chunk(self, image: Image.Image, page_num: int, chunk_num: int) -> Tuple[List[Dict], int]:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ (—Å—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ 512 —Ç–æ–∫–µ–Ω–æ–≤)"""
        try:
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
            inputs = self.vlm_processor.layout_processor(
                image, 
                return_tensors="pt",
                max_length=512,  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ 512 —Ç–æ–∫–µ–Ω–æ–≤
                truncation=True,  # –û–±—Ä–µ–∑–∫–∞ –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
                padding=True
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            inputs = {k: v.to(self.vlm_processor.device) for k, v in inputs.items()}
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ FP16 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å VLM
            import torch
            inputs = {k: v.to(torch.float16) if v.dtype == torch.float32 else v for k, v in inputs.items()}
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ó–ê–©–ò–¢–ê: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ç–æ–∫–µ–Ω–æ–≤
            input_ids = inputs.get('input_ids', None)
            if input_ids is not None:
                token_count = input_ids.shape[1]
                if token_count > 1024:
                    logger.warning(f"[WARN] Page {page_num + 1}, Chunk {chunk_num} has {token_count} tokens (>{1024}), truncating to prevent CUDA error")
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–µ–∑–∫–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                    inputs['input_ids'] = input_ids[:, :1024]
                    if 'attention_mask' in inputs:
                        inputs['attention_mask'] = inputs['attention_mask'][:, :512]
            
            # –ó–∞—â–∏—Ç–∞ –æ—Ç –Ω—É–ª–µ–≤—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤
            for key, tensor in inputs.items():
                if tensor.numel() == 0:
                    logger.warning(f"[WARN] Page {page_num + 1}, Chunk {chunk_num} skipped: Empty tensor {key}")
                    return [], 0
            
            # VLM –∞–Ω–∞–ª–∏–∑ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç CUDA –æ—à–∏–±–æ–∫
            with torch.no_grad():
                outputs = self.vlm_processor.layout_model(**inputs)
                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                tables = self._extract_structure_smart_chunking(predictions, image, page_num, chunk_num)
                return tables, 1
            
        except RuntimeError as e:
            if "device-side assert triggered" in str(e) or "srcSelectDimSize" in str(e):
                logger.error(f"üõë CUDA assertion failed on page {page_num + 1}, chunk {chunk_num}. Skipping: {e}")
                return [], 0
            raise e
        except Exception as e:
            logger.error(f"Chunk analysis failed for page {page_num + 1}, chunk {chunk_num}: {e}")
            return [], 0
    
    def _analyze_page_with_chunking(self, image: Image.Image, page_num: int, token_count: int) -> Tuple[List[Dict], int]:
        """Smart Chunking –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # üöÄ –£–ú–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –í—ã—á–∏—Å–ª—è–µ–º —Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            chunks_needed = math.ceil(token_count / 1024)  # –¢–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            logger.info(f"[PAGE] Page {page_num + 1} needs {chunks_needed} chunks for {token_count} tokens - analyzing ALL chunks for complete content")
            
            all_tables = []
            chunks_processed = 0
            
            # üöÄ –£–ú–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –±–∞—Ç—á–µ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü, –Ω–æ –Ω–µ —Ç–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
            if chunks_needed <= 5:
                batch_size = 3  # –î–ª—è –Ω–µ–±–æ–ª—å—à–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü - –ø–æ 3 —á–∞–Ω–∫–∞
            elif chunks_needed <= 20:
                batch_size = 2  # –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü - –ø–æ 2 —á–∞–Ω–∫–∞
            else:
                batch_size = 1  # –î–ª—è –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü - –ø–æ 1 —á–∞–Ω–∫—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            
            logger.info(f"[SMART_BATCHING] Processing {chunks_needed} chunks with batch_size={batch_size} for optimal performance")
            
            for batch_start in range(0, chunks_needed, batch_size):
                batch_end = min(batch_start + batch_size, chunks_needed)
                
                try:
                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á —á–∞–Ω–∫–æ–≤ —Å –ø–æ–ª–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –¥–∞–Ω–Ω—ã—Ö
                    for chunk_num in range(batch_start, batch_end):
                        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ - –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª–æ –±—ã —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
                        chunk_tables, chunks = self._analyze_single_chunk(image, page_num, chunk_num)
                        all_tables.extend(chunk_tables)
                        chunks_processed += chunks
                        
                except Exception as e:
                    logger.error(f"Batch {batch_start}-{batch_end} analysis failed for page {page_num + 1}: {e}")
                    continue
            
            logger.info(f"[SUCCESS] Page {page_num + 1} processed with {chunks_processed} chunks")
            return all_tables, chunks_processed
            
        except Exception as e:
            logger.error(f"Page chunking analysis failed for page {page_num + 1}: {e}")
            return [], 0
    
    def _extract_structure_smart_chunking(self, predictions: torch.Tensor, image: Image.Image, page_num: int, chunk_num: int = 0) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å Smart Chunking"""
        tables = []
        
        try:
            # Smart Chunking –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞
            # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
            return tables
            
        except Exception as e:
            logger.error(f"Smart chunking structure extraction failed for page {page_num + 1}: {e}")
            return []
    
    def _russian_llm_deep_analysis(self, content: str, vlm_metadata: Dict) -> Dict:
        """
        –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–º–æ—â—å—é —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç YaLM/GigaChat –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM
            if not hasattr(self, 'russian_llm_processor') or not self.russian_llm_processor:
                return {'russian_llm_available': False, 'reason': 'processor_not_initialized'}
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ä–æ—Å—Å–∏–π—Å–∫–∏–º LLM
            analysis_result = self.russian_llm_processor.analyze_document_structure(content, vlm_metadata)
            
            if analysis_result.get('russian_llm_available', False):
                logger.info(f"[SUCCESS] Russian LLM analysis completed: {analysis_result.get('provider', 'unknown')} model")
                return analysis_result
            else:
                logger.warning(f"[WARN] Russian LLM analysis failed: {analysis_result.get('error', 'unknown error')}")
                return analysis_result
                
        except Exception as e:
            logger.error(f"Russian LLM deep analysis failed: {e}")
            return {
                'russian_llm_available': False,
                'error': str(e),
                'fallback_to_vlm': True
            }
    
    # –°–¢–ê–†–´–ô –ú–ï–¢–û–î –£–î–ê–õ–ï–ù - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–π GPU LLM
    
    def _switch_sbert_to_gpu(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ SBERT –Ω–∞ GPU –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        try:
            if hasattr(self, 'sbert_model') and self.sbert_model is not None:
                logger.info("[SWITCH] –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ SBERT –Ω–∞ GPU –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
                self.sbert_model.to("cuda")
                logger.info("[SUCCESS] SBERT –ø–µ—Ä–µ–≤–µ–¥–µ–Ω –Ω–∞ GPU - –≥–æ—Ç–æ–≤ –∫ –±—ã—Å—Ç—Ä–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
                return True
            else:
                logger.warning("[WARN] SBERT –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
        except Exception as e:
            logger.error(f"[ERROR] –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è SBERT –Ω–∞ GPU: {e}")
            return False
    
    def _switch_sbert_to_cpu(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ SBERT –Ω–∞ CPU –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è VRAM"""
        try:
            if hasattr(self, 'sbert_model') and self.sbert_model is not None:
                logger.info("[SWITCH] –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ SBERT –Ω–∞ CPU –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è VRAM...")
                self.sbert_model.to("cpu")
                logger.info("[SUCCESS] SBERT –ø–µ—Ä–µ–≤–µ–¥–µ–Ω –Ω–∞ CPU - VRAM –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω")
                return True
            else:
                logger.warning("[WARN] SBERT –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                return False
        except Exception as e:
            logger.error(f"[ERROR] –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è SBERT –Ω–∞ CPU: {e}")
            return False
    
    def _stage_5_5_deep_semantic_analysis(self, content: str, vlm_metadata: Dict, llm_analysis: Dict) -> Dict:
        """
        STAGE 5.5: Deep Semantic Analysis
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã VLM + –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ LLM –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç LLM
            llm_results = llm_analysis.get('analysis', {})
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º VLM –∏ LLM —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            enhanced_sections = self._enhance_sections_with_llm(
                vlm_metadata.get('sections', []), 
                llm_results.get('sections', [])
            )
            
            enhanced_tables = self._enhance_tables_with_llm(
                vlm_metadata.get('tables', []), 
                llm_results.get('tables', [])
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
            extracted_entities = self._extract_entities_with_llm(content, llm_results)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏
            semantic_relations = self._find_semantic_relations(enhanced_sections, llm_results)
            
            return {
                'stage_5_5_completed': True,
                'enhanced_sections': len(enhanced_sections),
                'enhanced_tables': len(enhanced_tables),
                'extracted_entities': len(extracted_entities),
                'semantic_relations': len(semantic_relations),
                'document_type': llm_results.get('document_type', 'unknown'),
                'scope': llm_results.get('scope', ''),
                'requirements': llm_results.get('requirements', []),
                'processing_time': llm_analysis.get('analysis', {}).get('processing_time', 0),
                'model_info': llm_analysis.get('model', 'unknown')
            }
            
        except Exception as e:
            logger.error(f"Stage 5.5 Deep Analysis failed: {e}")
            return {
                'stage_5_5_completed': False,
                'error': str(e),
                'fallback_to_vlm': True
            }
    
    def _enhance_sections_with_llm(self, vlm_sections: List[Dict], llm_sections: List[Dict]) -> List[Dict]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –æ—Ç VLM –∏ LLM –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è"""
        enhanced_sections = []
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å–µ–∫—Ü–∏–∏ –æ—Ç VLM –∏ LLM
        all_sections = vlm_sections + llm_sections
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —É–ª—É—á—à–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        seen_titles = set()
        for section in all_sections:
            title = section.get('title', '').strip()
            if title and title not in seen_titles:
                enhanced_section = {
                    'title': title,
                    'content': section.get('content', ''),
                    'level': section.get('level', 1),
                    'source': 'vlm_llm_combined',
                    'confidence': 0.9  # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏
                }
                enhanced_sections.append(enhanced_section)
                seen_titles.add(title)
        
        return enhanced_sections
    
    def _enhance_tables_with_llm(self, vlm_tables: List[Dict], llm_tables: List[Dict]) -> List[Dict]:
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –æ—Ç VLM –∏ LLM"""
        enhanced_tables = []
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—ã
        all_tables = vlm_tables + llm_tables
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        seen_titles = set()
        for table in all_tables:
            title = table.get('title', '').strip()
            if title and title not in seen_titles:
                enhanced_table = {
                    'title': title,
                    'data': table.get('data', ''),
                    'source': 'vlm_llm_combined',
                    'confidence': 0.9
                }
                enhanced_tables.append(enhanced_table)
                seen_titles.add(title)
        
        return enhanced_tables
    
    def _extract_entities_with_llm(self, content: str, llm_results: Dict) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é LLM"""
        entities = []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫–∞–∫ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        requirements = llm_results.get('requirements', [])
        for i, req in enumerate(requirements):
            entities.append({
                'text': req,
                'type': 'requirement',
                'confidence': 0.9,
                'source': 'llm_extraction'
            })
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
        scope = llm_results.get('scope', '')
        if scope:
            entities.append({
                'text': scope,
                'type': 'scope',
                'confidence': 0.8,
                'source': 'llm_extraction'
            })
        
        return entities
    
    def _find_semantic_relations(self, sections: List[Dict], llm_results: Dict) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        relations = []
        
        # –°–≤—è–∑–∏ –º–µ–∂–¥—É —Å–µ–∫—Ü–∏—è–º–∏
        for i, section in enumerate(sections):
            if i > 0:
                relations.append({
                    'source': sections[i-1]['title'],
                    'target': section['title'],
                    'relation_type': 'hierarchical',
                    'confidence': 0.8
                })
        
        return relations
    
    def _combine_type_detection_enhanced(self, regex_result: Dict, sbert_result: Dict, llm_result: Dict) -> Dict[str, Any]:
        """üöÄ –£–õ–£–ß–®–ï–ù–ù–û–ï –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ regex + SBERT + RuLongformer"""
        
        # –ü–†–ò–û–†–ò–¢–ï–¢: –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –Ω–æ–º–µ—Ä–∞–º–∏ –∏–º–µ—é—Ç –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        normative_types = ['gost', 'sp', 'snip', 'sanpin', 'vsn', 'mds', 'pnst', 'sto', 'tu']
        
        # –ï—Å–ª–∏ regex –Ω–∞—à–µ–ª –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç, –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –µ–º—É
        if regex_result['doc_type'] in normative_types and regex_result['confidence'] > 0.5:
            return {
                'doc_type': regex_result['doc_type'],
                'doc_subtype': regex_result['doc_subtype'],
                'id_type': regex_result.get('id_type', 'NUMBER'),
                'confidence': regex_result['confidence'],
                'folder': regex_result.get('folder', 'other'),
                'methods_used': f"regex_priority({regex_result['confidence']:.2f})"
            }
        
        # –ï—Å–ª–∏ LLM –¥–æ—Å—Ç—É–ø–µ–Ω –∏ –¥–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å (—Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –°–ü –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
        if llm_result and llm_result.get('document_type') and llm_result.get('confidence', 0) > 0.3:
            llm_confidence = llm_result.get('confidence', 0.0)
            llm_type = llm_result.get('document_type', 'unknown')
            
            # –ú–∞–ø–ø–∏–Ω–≥ —Ç–∏–ø–æ–≤ LLM –∫ –Ω–∞—à–∏–º —Ç–∏–ø–∞–º
            llm_type_mapping = {
                '–°–ù–∏–ü': 'snip',
                '–°–ü': 'sp', 
                '–ì–û–°–¢': 'gost',
                '–ü–ü–†': 'ppr',
                '–°–º–µ—Ç–∞': 'smeta',
                '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä–µ–≥–ª–∞–º–µ–Ω—Ç': 'tech_reg',
                '–ü—Ä–∏–∫–∞–∑': 'order',
                '–ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è': 'project'
            }
            
            mapped_type = llm_type_mapping.get(llm_type, 'other')
            
            # –ï—Å–ª–∏ LLM –¥–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if llm_confidence > 0.8:
                return {
                    'doc_type': mapped_type,
                    'doc_subtype': llm_result.get('subtype', 'general'),
                    'id_type': 'LLM_ANALYSIS',
                    'confidence': llm_confidence,
                    'folder': self._get_folder_by_type(mapped_type),
                    'methods_used': f"llm_priority({llm_confidence:.2f})",
                    'llm_keywords': llm_result.get('keywords', [])
                }
        
        # –ï—Å–ª–∏ —Ç–∏–ø—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç –º–µ–∂–¥—É regex –∏ SBERT, –∫–æ–º–±–∏–Ω–∏—Ä—É–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        if regex_result['doc_type'] == sbert_result['doc_type']:
            combined_confidence = min(
                (regex_result['confidence'] + sbert_result['confidence']) / 2 + 0.1,
                1.0
            )
            return {
                'doc_type': regex_result['doc_type'],
                'doc_subtype': regex_result['doc_subtype'],
                'id_type': regex_result.get('id_type', 'COMBINED'),
                'confidence': combined_confidence,
                'folder': regex_result.get('folder', 'other'),
                'methods_used': f"regex_sbert_combined({combined_confidence:.2f})"
            }
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
        if regex_result['confidence'] > sbert_result['confidence']:
            return {
                'doc_type': regex_result['doc_type'],
                'doc_subtype': regex_result['doc_subtype'],
                'id_type': regex_result.get('id_type', 'REGEX'),
                'confidence': regex_result['confidence'],
                'folder': regex_result.get('folder', 'other'),
                'methods_used': f"regex_higher({regex_result['confidence']:.2f})"
            }
        else:
            return {
                'doc_type': sbert_result['doc_type'],
                'doc_subtype': sbert_result['doc_subtype'],
                'id_type': sbert_result.get('id_type', 'SBERT'),
                'confidence': sbert_result['confidence'],
                'folder': sbert_result.get('folder', 'other'),
                'methods_used': f"sbert_higher({sbert_result['confidence']:.2f})"
            }
    
    def _get_folder_by_type(self, doc_type: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞–ø–∫–∏ –ø–æ —Ç–∏–ø—É –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        folder_mapping = {
            'snip': 'norms',
            'sp': 'norms', 
            'gost': 'norms',
            'ppr': 'ppr',
            'smeta': 'smeta',
            'project': 'project',
            'order': 'other',
            'tech_reg': 'norms'
        }
        return folder_mapping.get(doc_type, 'other')
    
    def _sbert_section_detection(self, content: str, doc_type_info: Dict, sbert_model) -> List[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π —á–µ—Ä–µ–∑ SBERT"""
        
        logger.debug(f"Starting enhanced section detection, content length: {len(content)}")
        
        # –®–ê–ì 1: –ú—É–ª—å—Ç–∏-—É—Ä–æ–≤–Ω–µ–≤–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
        sections = self._multi_level_section_detection(content)
        
        # –®–ê–ì 2: –ï—Å–ª–∏ —Å–µ–∫—Ü–∏–π –º–∞–ª–æ - —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
        if len(sections) < 5:
            logger.info(f"Only {len(sections)} sections found, applying semantic clustering")
            sections = self._semantic_section_clustering(content, sections)
        
        # –®–ê–ì 3: –û–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        final_sections = self._validate_sections(sections, content)
        
        logger.info(f"Enhanced section detection: {len(final_sections)} sections created")
        return final_sections
    
    def _add_hierarchy_paths(self, sections: List[Dict], hierarchy: Dict) -> List[Dict]:
        """–î–æ–±–∞–≤–ª—è–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –ø—É—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ - –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û!"""
        
        enhanced_sections = []
        current_path = []
        
        for i, section in enumerate(sections):
            # –°–æ–∑–¥–∞–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å
            section_level = section.get('level', 1)
            section_title = section.get('title', f'–†–∞–∑–¥–µ–ª {i+1}')
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Ä–æ–≤–Ω—è
            if section_level == 1:
                current_path = [section_title]
            elif section_level == 2:
                if len(current_path) > 1:
                    current_path = current_path[:1] + [section_title]
                else:
                    current_path.append(section_title)
            elif section_level == 3:
                if len(current_path) > 2:
                    current_path = current_path[:2] + [section_title]
                else:
                    current_path.append(section_title)
            else:
                # –î–ª—è –±–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏—Ö —É—Ä–æ–≤–Ω–µ–π –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º
                current_path.append(section_title)
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å
            hierarchy_path = ' -> '.join(current_path)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–µ–∫—Ü–∏—é
            enhanced_section = section.copy()
            enhanced_section['hierarchy_path'] = hierarchy_path
            enhanced_section['doc_title'] = self._extract_document_title(section.get('content', ''))
            
            enhanced_sections.append(enhanced_section)
        
        return enhanced_sections
    
    def _extract_document_title(self, content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
        lines = content.split('\n')[:20]  # –ü–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        title_patterns = [
            # –°–ü –∏ –°–ù–∏–ü –¥–æ–∫—É–º–µ–Ω—Ç—ã
            r'(–°–ü|–°–ù–∏–ü)\s+(\d+\.\d+\.\d+)\s*[-‚Äì‚Äî]?\s*(.+)',
            # –ì–û–°–¢ –¥–æ–∫—É–º–µ–Ω—Ç—ã  
            r'(–ì–û–°–¢|–ì–û–°–¢ –†)\s+(\d+\.\d+\.\d+)\s*[-‚Äì‚Äî]?\s*(.+)',
            # –¢–£ –∏ –°–¢–û –¥–æ–∫—É–º–µ–Ω—Ç—ã
            r'(–¢–£|–°–¢–û)\s+(\d+\.\d+\.\d+)\s*[-‚Äì‚Äî]?\s*(.+)',
            # –†–î –¥–æ–∫—É–º–µ–Ω—Ç—ã
            r'(–†–î|–†–î-11-\d+)\s+(\d+\.\d+\.\d+)\s*[-‚Äì‚Äî]?\s*(.+)',
            # –û–±—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–∑–≤–∞–Ω–∏–π
            r'([–ê-–Ø][–∞-—è—ë\s]+(?:–ø—Ä–∞–≤–∏–ª|–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π|—Å–æ–æ—Ä—É–∂–µ–Ω–∏–π|–∑–¥–∞–Ω–∏–π|—Å–æ–æ—Ä—É–∂–µ–Ω–∏–π))',
            r'([–ê-–Ø][–∞-—è—ë\s]+(?:–º–µ—Ç–æ–¥–∏–∫|–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π|—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤))'
        ]
        
        import re
        
        for line in lines:
            line = line.strip()
            if len(line) < 10 or len(line) > 300:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                continue
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for pattern in title_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    if len(match.groups()) >= 3:
                        # –§–æ—Ä–º–∞—Ç: –°–ü 16.13330.2017 - –ù–∞–∑–≤–∞–Ω–∏–µ
                        return f"{match.group(1)} {match.group(2)} - {match.group(3).strip()}"
                    elif len(match.groups()) >= 1:
                        # –ü—Ä–æ—Å—Ç–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
                        return match.group(1).strip()
            
            # Fallback: –∏—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏ –±—É–∫–≤–∞–º–∏
            if any(char.isupper() for char in line) and not line.isupper():
                if len(line) > 15 and len(line) < 150:
                    return line
        
        return "–î–æ–∫—É–º–µ–Ω—Ç"
    
    def _detect_amendment_to_sp(self, header_text: str) -> Optional[Dict[str, str]]:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫ –°–ü –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∫ –°–ü
        amendment_patterns = [
            r'–ò–∑–º–µ–Ω–µ–Ω–∏–µ\s*‚Ññ?\s*(\d+)\s*–∫\s*(–°–ü\s+\d+[.\d]*)',
            r'–ò–∑–º\s*\.?\s*(\d+)\s*–∫\s*(–°–ü\s+\d+[.\d]*)',
            r'–û–±\s+—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–∏\s+–ò–∑–º–µ–Ω–µ–Ω–∏—è\s*‚Ññ?\s*(\d+)\s*–∫\s*(–°–ü\s+\d+[.\d]*)',
            r'–ò–∑–º–µ–Ω–µ–Ω–∏—è\s*‚Ññ?\s*(\d+)\s*–∫\s*(–°–ü\s+\d+[.\d]*)',
            r'–ò–ó–ú–ï–ù–ï–ù–ò–ï\s*‚Ññ?\s*(\d+)\s*–ö\s*(–°–ü\s+\d+[.\d]*)'
        ]
        
        for pattern in amendment_patterns:
            match = re.search(pattern, header_text, re.IGNORECASE)
            if match:
                amendment_num = match.group(1)
                base_sp_id = match.group(2).strip()
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
                full_name = f"–ò–∑–º–µ–Ω–µ–Ω–∏–µ ‚Ññ {amendment_num} –∫ {base_sp_id}"
                
                return {
                    'amendment_num': amendment_num,
                    'base_sp_id': base_sp_id,
                    'full_name': full_name
                }
        
        return None
    
    def _extract_document_title_from_metadata(self, doc_type_info: Dict, metadata: Dict = None) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –ü–†–ê–í–ò–õ–¨–ù–´–ú –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –°–ü –Ω–∞–¥ –°–ù–∏–ü"""
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –ò–ó–ú–ï–ù–ï–ù–ò–ï–ú
        if metadata:
            doc_type = metadata.get('doc_type', 'standard')
            if doc_type == 'amendment':
                # –î–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ
                amendment_num = metadata.get('amendment_number', '')
                base_sp_id = metadata.get('base_sp_id', '')
                if amendment_num and base_sp_id:
                    return f"–ò–∑–º.{amendment_num}_–∫_{base_sp_id}"
            
            primary_doc_name = metadata.get('primary_doc_name', '')
            if primary_doc_name and primary_doc_name != '–î–æ–∫—É–º–µ–Ω—Ç':
                return primary_doc_name
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ª–æ–≥–∏–∫–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
            doc_numbers = metadata.get('doc_numbers', [])
            if doc_numbers:
                # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
                prioritized_docs = []
                
                for doc in doc_numbers:
                    priority = 0
                    
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1 (–í—ã—Å—à–∏–π): –°–ü —Å –ø–æ–ª–Ω—ã–º –≥–æ–¥–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, –°–ü 541.1325800.2024)
                    if doc.startswith('–°–ü ') and ('.20' in doc or '.19' in doc):
                        priority = 1000 + len(doc)  # –î–ª–∏–Ω–∞ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –ø–æ –ø–æ–ª–Ω–æ—Ç–µ
                    
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –°–ü —Å –ø–æ–ª–Ω—ã–º –Ω–æ–º–µ—Ä–æ–º –±–µ–∑ –≥–æ–¥–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –°–ü 88.13330)
                    elif doc.startswith('–°–ü ') and '.' in doc and '.20' not in doc and '.19' not in doc:
                        priority = 800 + len(doc)
                    
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –°–ü –∫–æ—Ä–æ—Ç–∫–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –°–ü 88)
                    elif doc.startswith('–°–ü ') and '.' not in doc:
                        priority = 600 + len(doc)
                    
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 4: –°–ù–∏–ü —Å –ø–æ–ª–Ω—ã–º –≥–æ–¥–æ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, –°–ù–∏–ü 23-03-2003)
                    elif doc.startswith('–°–ù–∏–ü ') and ('.20' in doc or '.19' in doc or '-' in doc):
                        priority = 400 + len(doc)
                    
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 5: –°–ù–∏–ü –∫–æ—Ä–æ—Ç–∫–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, –°–ù–∏–ü 2)
                    elif doc.startswith('–°–ù–∏–ü ') and '.' not in doc and '-' not in doc:
                        priority = 200 + len(doc)
                    
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 6: –õ—é–±–æ–π –¥—Ä—É–≥–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
                    else:
                        priority = 100 + len(doc)
                    
                    prioritized_docs.append((priority, doc))
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
                prioritized_docs.sort(key=lambda x: x[0], reverse=True)
                
                if prioritized_docs:
                    return prioritized_docs[0][1]
        
        # Fallback –Ω–∞ —Å—Ç–∞—Ä—É—é –ª–æ–≥–∏–∫—É –∏–∑ doc_type_info
        title = doc_type_info.get('doc_title', '')
        if title and title != '–î–æ–∫—É–º–µ–Ω—Ç':
            return title
        
        # –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ doc_numbers –≤ doc_type_info —Å —Ç–µ–º –∂–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
        doc_numbers = doc_type_info.get('doc_numbers', [])
        if doc_numbers:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: –°–ü —Å –ø–æ–ª–Ω—ã–º –≥–æ–¥–æ–º
            sp_full_year = [doc for doc in doc_numbers if doc.startswith('–°–ü ') and ('.20' in doc or '.19' in doc)]
            if sp_full_year:
                sp_full_year.sort(key=len, reverse=True)
                return sp_full_year[0]
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: –°–ü –±–µ–∑ –≥–æ–¥–∞
            sp_short = [doc for doc in doc_numbers if doc.startswith('–°–ü ') and '.' in doc and '.20' not in doc and '.19' not in doc]
            if sp_short:
                sp_short.sort(key=len, reverse=True)
                return sp_short[0]
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –°–ü —Å–æ–≤—Å–µ–º –∫–æ—Ä–æ—Ç–∫–∏–π
            sp_very_short = [doc for doc in doc_numbers if doc.startswith('–°–ü ') and '.' not in doc]
            if sp_very_short:
                return sp_very_short[0]
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 4: –°–ù–∏–ü —Å –ø–æ–ª–Ω—ã–º –≥–æ–¥–æ–º
            snip_full = [doc for doc in doc_numbers if doc.startswith('–°–ù–∏–ü ') and '.' in doc]
            if snip_full:
                return snip_full[0]
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 5: –°–ù–∏–ü –∫–æ—Ä–æ—Ç–∫–∏–π
            snip_short = [doc for doc in doc_numbers if doc.startswith('–°–ù–∏–ü ') and '.' not in doc]
            if snip_short:
                return snip_short[0]
            
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 6: –õ—é–±–æ–π –¥—Ä—É–≥–æ–π –¥–æ–∫—É–º–µ–Ω—Ç
            if doc_numbers:
                return doc_numbers[0]
        
        # Fallback –Ω–∞ –æ–±—â–µ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        doc_type = doc_type_info.get('doc_type', 'unknown')
        return f"–î–æ–∫—É–º–µ–Ω—Ç_{doc_type}"
    
    def _create_safe_filename(self, title: str) -> str:
        """–°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –£–ú–ù–û–ô –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        import re
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –£–º–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if not title or title == '–î–æ–∫—É–º–µ–Ω—Ç':
            return '–î–æ–∫—É–º–µ–Ω—Ç'
        
        # –û—á–∏—â–∞–µ–º –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —Ñ–∞–π–ª–æ–≤–æ–π —Å–∏—Å—Ç–µ–º—ã
        safe_title = re.sub(r'[<>:"/\\|?*]', '_', title)
        
        # –ó–∞–º–µ–Ω—è–µ–º –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
        safe_title = re.sub(r'\s+', '_', safe_title)
        
        # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
        safe_title = re.sub(r'_+', '_', safe_title)
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω—ã–µ —á–∞—Å—Ç–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        # –ï—Å–ª–∏ —ç—Ç–æ –°–ü –∏–ª–∏ –°–ù–∏–ü, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –±–æ–ª–µ–µ —É–º–Ω–æ
        if safe_title.startswith('–°–ü_') or safe_title.startswith('–°–ù–∏–ü_'):
            # –î–ª—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —á–∞—Å—Ç—å
            if len(safe_title) > 80:
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 80 —Å–∏–º–≤–æ–ª–æ–≤, –Ω–æ —Å—Ç–∞—Ä–∞–µ–º—Å—è –Ω–µ –æ–±—Ä–µ–∑–∞—Ç—å –ø–æ—Å–µ—Ä–µ–¥–∏–Ω–µ —Å–ª–æ–≤–∞
                safe_title = safe_title[:80]
                # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –Ω–µ–ø–æ–ª–Ω–æ–µ —Å–ª–æ–≤–æ
                last_underscore = safe_title.rfind('_')
                if last_underscore > 50:  # –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–∞–∑—É–º–Ω–æ–µ –º–µ—Å—Ç–æ –¥–ª—è –æ–±—Ä–µ–∑–∫–∏
                    safe_title = safe_title[:last_underscore]
        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤
            if len(safe_title) > 100:
                safe_title = safe_title[:100]
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        safe_title = safe_title.strip('_')
        
        # –ï—Å–ª–∏ –ø—É—Å—Ç–æ–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
        if not safe_title:
            safe_title = "–î–æ–∫—É–º–µ–Ω—Ç"
        
        return safe_title
    
    def _multi_level_section_detection(self, content: str) -> List[Dict]:
        """–ú—É–ª—å—Ç–∏-—É—Ä–æ–≤–Ω–µ–≤–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π - –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏"""
        
        lines = content.split('\n')
        sections = []
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ (–ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏)
        header_patterns = [
            # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
            (r'^\s*(\d+\.)\s+([^–∞-—è]*[A-–Ø–∞-—è—ë][^–Ω–∞-—è—ë]*)', 1),  # "1. –ó–∞–≥–æ–ª–æ–≤–æ–∫"
            (r'^\s*(\d+\.\d+\.)\s+([A-–Ø–∞-—è—ë].*)', 2),  # "1.1. –ü–æ–¥–∑–∞–≥–æ–ª–æ–≤–æ–∫"
            (r'^\s*(\d+\.\d+\.\d+\.)\s+([A-–Ø–∞-—è—ë].*)', 3),  # "1.1.1. –ü–æ–¥–ø—É–Ω–∫—Ç"
            
            # –†–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã
            (r'^\s*([IVX]+\.)\s+([A-–Ø–∞-—è—ë].*)', 1),  # "I. –†–∞–∑–¥–µ–ª"
            
            # –ë—É–∫–≤—ã
            (r'^\s*([a-z–∞-—è]\))\s+([A-–Ø–∞-—è—ë].*)', 3),  # "a) –ø—É–Ω–∫—Ç"
            (r'^\s*([A-–ê-–Ø]\))\s+([A-–Ø–∞-—è—ë].*)', 2),  # "A) –ü—É–Ω–∫—Ç"
            
            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            (r'^\s*(–ì–õ–ê–í–ê\s+\d+)[.:]?\s*([A-–Ø–∞-—è—ë].*)', 1),
            (r'^\s*(–†–ê–ó–î–ï–õ\s+\d+)[.:]?\s*([A-–Ø–∞-—è—ë].*)', 1),
            (r'^\s*(–ü–£–ù–ö–¢\s+\d+)[.:]?\s*([A-–Ø–∞-—è—ë].*)', 2),
            (r'^\s*(–ü–û–î–ü–£–ù–ö–¢\s+\d+)[.:]?\s*([A-–Ø–∞-—è—ë].*)', 3),
            
            # –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø –∏ —Ç.–¥.
            (r'^\s*([A-–Ø–Å]{4,}(?:\s+[A-–Ø–Å]{4,})*)[.:]?\s*$', 1),  # "–û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø"
            
            # –ü—Ä–æ—Å—Ç—ã–µ –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ
            (r'^\s*(\d+)\s+([A-–Ø–∞-—è—ë][A-–Ø–∞-—è—ë\s]{10,})', 1),  # "¬™ 1 –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞"
        ]
        
        current_section = None
        current_content = ""
        
        for line_num, line in enumerate(lines):
            line_stripped = line.strip()
            
            if not line_stripped:
                if current_content:
                    current_content += "\n"
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            is_header = False
            header_level = 1
            header_title = None
            
            for pattern, level in header_patterns:
                match = re.match(pattern, line_stripped, re.IGNORECASE)
                if match:
                    is_header = True
                    header_level = level
                    if len(match.groups()) >= 2:
                        header_title = f"{match.group(1)} {match.group(2)}".strip()
                    else:
                        header_title = line_stripped[:100]
                    break
            
            if is_header:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_section and current_content.strip():
                    current_section['content'] = current_content.strip()
                    current_section['end_line'] = line_num - 1
                    sections.append(current_section)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
                current_section = {
                    'title': header_title or line_stripped,
                    'content': '',
                    'level': header_level,
                    'semantic_type': 'section',
                    'confidence': 0.8,
                    'start_line': line_num,
                    'end_line': line_num
                }
                current_content = ""
            else:
                current_content += line + "\n"
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–µ–∫—Ü–∏—è
        if current_section and current_content.strip():
            current_section['content'] = current_content.strip()
            current_section['end_line'] = len(lines)
            sections.append(current_section)
        
        return sections
    
    def _semantic_section_clustering(self, content: str, existing_sections: List[Dict]) -> List[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–µ–∫—Ü–∏–∏"""
        
        if not self.sbert_model or not HAS_ML_LIBS:
            return self._fallback_paragraph_splitting(content)
        
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
            paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 100]
            
            if len(paragraphs) < 5:
                # –ú–∞–ª–æ –∞–±–∑–∞—Ü–µ–≤ - —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                return self._fallback_sentence_splitting(content)
            
            # [SUCCESS] –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏
            logger.info(f"[EMBED] –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(paragraphs)} –∞–±–∑–∞—Ü–µ–≤...")
            
            # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: SBERT –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ GPU –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏!
            if hasattr(self.sbert_model, 'device'):
                logger.info(f"[DEVICE] SBERT device: {self.sbert_model.device}")
                if str(self.sbert_model.device) == 'cpu':
                    logger.warning("[WARN] SBERT –Ω–∞ CPU - –ø–µ—Ä–µ–≤–æ–¥–∏–º –Ω–∞ GPU –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏!")
                    self.sbert_model.to("cuda")
            else:
                logger.warning("[WARN] SBERT device –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ GPU!")
                self.sbert_model.to("cuda")
            
            # üî¨ –°–£–ü–ï–†-DEBUG –î–õ–Ø SBERT
            import time
            logger.info(f"[DEBUG_SBERT] Input paragraphs count: {len(paragraphs)}")
            logger.info(f"[DEBUG_SBERT] Expected batch_size: 32")
            logger.info(f"[DEBUG_SBERT] SBERT device after check: {self.sbert_model.device}")
            logger.info(f"[DEBUG_SBERT] Starting encode at {time.time()}")
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ FP16 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å SBERT
            import torch
            if isinstance(paragraphs, list):
                # –î–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ - —Å–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –∏ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ FP16
                paragraph_embeddings = self.sbert_model.encode(paragraphs, show_progress_bar=False, batch_size=32)
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ FP16
                if hasattr(paragraph_embeddings, 'to'):
                    paragraph_embeddings = paragraph_embeddings.to(torch.float16)
            else:
                paragraph_embeddings = self.sbert_model.encode(paragraphs, show_progress_bar=False, batch_size=32)
            
            logger.info(f"[DEBUG_SBERT] Encode completed at {time.time()}")
            logger.info(f"[DEBUG_SBERT] Embeddings shape: {paragraph_embeddings.shape}")
            
            # –ü—Ä–æ—Å—Ç–æ–µ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
            clusters = self._simple_clustering(paragraph_embeddings, min_clusters=5, max_clusters=15)
            
            # –°–æ–∑–¥–∞–µ–º —Å–µ–∫—Ü–∏–∏ –∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
            clustered_sections = []
            for cluster_id in set(clusters):
                cluster_paragraphs = [paragraphs[i] for i, c in enumerate(clusters) if c == cluster_id]
                cluster_content = '\n\n'.join(cluster_paragraphs)
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title = self._generate_cluster_title(cluster_paragraphs[0])
                
                clustered_sections.append({
                    'title': title,
                    'content': cluster_content,
                    'level': 1,
                    'semantic_type': 'semantic_cluster',
                    'confidence': 0.6,
                    'cluster_id': cluster_id
                })
            
            return clustered_sections
            
        except Exception as e:
            logger.error(f"Semantic clustering failed: {e}")
            return self._fallback_paragraph_splitting(content)
    
    def _simple_clustering(self, embeddings, min_clusters=5, max_clusters=15):
        """–ü—Ä–æ—Å—Ç–æ–µ –∫–ª–∞—Å—Ç–µ—Ä–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Å—Ö–æ–¥—Å—Ç–≤—É"""
        
        n_samples = len(embeddings)
        n_clusters = max(min_clusters, min(max_clusters, n_samples // 3))
        
        # –ü—Ä–æ—Å—Ç–æ–µ k-means —á–µ—Ä–µ–∑ numpy
        try:
            from sklearn.cluster import KMeans
            from sklearn.metrics.pairwise import cosine_similarity
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            clusters = kmeans.fit_predict(embeddings)
            return clusters
        except ImportError:
            # Fallback - —Å–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø–æ—Ä—è–¥–∫—É
            cluster_size = max(1, n_samples // n_clusters)
            return [i // cluster_size for i in range(n_samples)]
    
    def _generate_cluster_title(self, first_paragraph: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        first_sentence = first_paragraph.split('.')[0].strip()
        
        if len(first_sentence) < 100 and len(first_sentence.split()) > 3:
            return first_sentence
        else:
            # –ü–µ—Ä–≤—ã–µ 50 —Å–∏–º–≤–æ–ª–æ–≤
            return first_paragraph[:50].strip() + "..."
    
    def _fallback_paragraph_splitting(self, content: str) -> List[Dict]:
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –∞–±–∑–∞—Ü–∞–º"""
        
        paragraphs = [p.strip() for p in content.split('\n\n') if len(p.strip()) > 50]
        
        sections = []
        current_content = ""
        current_words = 0
        section_num = 1
        target_words = 500  # –¶–µ–ª—å - 500 —Å–ª–æ–≤ –Ω–∞ —Å–µ–∫—Ü–∏—é
        
        for paragraph in paragraphs:
            para_words = len(paragraph.split())
            
            if current_words + para_words > target_words and current_content:
                sections.append({
                    'title': f'–°–µ–∫—Ü–∏—è {section_num}',
                    'content': current_content.strip(),
                    'level': 1,
                    'semantic_type': 'paragraph_section',
                    'confidence': 0.4
                })
                current_content = paragraph + "\n\n"
                current_words = para_words
                section_num += 1
            else:
                current_content += paragraph + "\n\n"
                current_words += para_words
        
        if current_content.strip():
            sections.append({
                'title': f'–°–µ–∫—Ü–∏—è {section_num}',
                'content': current_content.strip(),
                'level': 1,
                'semantic_type': 'paragraph_section',
                'confidence': 0.4
            })
        
        return sections
    
    def _fallback_sentence_splitting(self, content: str) -> List[Dict]:
        """–ö—Ä–∞–π–Ω–∏–π fallback - —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º"""
        
        sentences = re.split(r'[.!?]+\s+', content)
        sentences = [s.strip() for s in sentences if len(s.strip()) > 30]
        
        sections = []
        current_content = ""
        current_sentences = 0
        section_num = 1
        target_sentences = 10  # –¶–µ–ª—å - 10 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ —Å–µ–∫—Ü–∏—é
        
        for sentence in sentences:
            if current_sentences >= target_sentences and current_content:
                sections.append({
                    'title': f'–§—Ä–∞–≥–º–µ–Ω—Ç {section_num}',
                    'content': current_content.strip(),
                    'level': 1,
                    'semantic_type': 'sentence_fragment',
                    'confidence': 0.3
                })
                current_content = sentence + ". "
                current_sentences = 1
                section_num += 1
            else:
                current_content += sentence + ". "
                current_sentences += 1
        
        if current_content.strip():
            sections.append({
                'title': f'–§—Ä–∞–≥–º–µ–Ω—Ç {section_num}',
                'content': current_content.strip(),
                'level': 1,
                'semantic_type': 'sentence_fragment',
                'confidence': 0.3
            })
        
        return sections
    
    def _validate_sections(self, sections: List[Dict], content: str) -> List[Dict]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å–µ–∫—Ü–∏–π - —É–±–∏—Ä–∞–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ"""
        
        validated = []
        total_length = len(content)
        
        for section in sections:
            section_content = section.get('content', '')
            section_words = len(section_content.split())
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
            if section_words >= 20:  # –ú–∏–Ω–∏–º—É–º 20 —Å–ª–æ–≤
                validated.append(section)
        
        # –ï—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Å–µ–∫—Ü–∏–π - –¥–æ–±–∞–≤–ª—è–µ–º fallback
        if len(validated) < 3:
            validated.extend(self._fallback_paragraph_splitting(content))
        
        return validated[:50]  # –ú–∞–∫—Å–∏–º—É–º 50 —Å–µ–∫—Ü–∏–π –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∞–±–∑–∞—Ü–µ–≤ —Å batch_size –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è RAM overflow
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ FP16 –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å SBERT
            import torch
            paragraph_embeddings = self.sbert_model.encode(
                paragraphs, 
                batch_size=32,  # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è SBERT
                show_progress_bar=False,
                convert_to_tensor=True
            )
            
            # –®–∞–±–ª–æ–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–µ–∫—Ü–∏–π
            section_templates = {
                'introduction': '–≤–≤–µ–¥–µ–Ω–∏–µ –æ–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è –Ω–∞—á–∞–ª–æ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è',
                'technical': '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∞—Ç–µ—Ä–∏–∞–ª—ã',
                'procedure': '–ø–æ—Ä—è–¥–æ–∫ –º–µ—Ç–æ–¥–∏–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —ç—Ç–∞–ø—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ',
                'control': '–∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø—ã—Ç–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–∫–∞',
                'conclusion': '–∑–∞–∫–ª—é—á–µ–Ω–∏–µ –≤—ã–≤–æ–¥—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–∫–æ–Ω—á–∞–Ω–∏–µ'
            }
            
            template_embeddings = self.sbert_model.encode(list(section_templates.values()))
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ FP16
            if hasattr(template_embeddings, 'to'):
                template_embeddings = template_embeddings.to(torch.float16)
            
            sections = []
            current_section_type = None
            current_content = ""
            section_confidence = 0.0
            
            for i, (paragraph, emb) in enumerate(zip(paragraphs, paragraph_embeddings)):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–µ–∫—Ü–∏–∏ –¥–ª—è –∞–±–∑–∞—Ü–∞
                similarities = []
                for template_emb in template_embeddings:
                    sim = np.dot(emb, template_emb) / (
                        np.linalg.norm(emb) * np.linalg.norm(template_emb)
                    )
                    similarities.append(sim)
                
                best_idx = max(range(len(similarities)), key=lambda i: similarities[i])
                best_type = list(section_templates.keys())[best_idx]
                best_sim = similarities[best_idx]
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é –µ—Å–ª–∏ —Ç–∏–ø —Å–º–µ–Ω–∏–ª—Å—è
                if current_section_type != best_type and current_content:
                    sections.append({
                        'title': self._generate_section_title(current_section_type, current_content),
                        'content': current_content.strip(),
                        'level': 1,
                        'semantic_type': current_section_type,
                        'confidence': section_confidence / max(len(current_content.split('\n\n')), 1)
                    })
                    current_content = ""
                    section_confidence = 0.0
                
                current_section_type = best_type
                current_content += paragraph + "\n\n"
                section_confidence += best_sim
            
            # –ü–æ—Å–ª–µ–¥–Ω—è—è —Å–µ–∫—Ü–∏—è
            if current_content:
                sections.append({
                    'title': self._generate_section_title(current_section_type, current_content),
                    'content': current_content.strip(),
                    'level': 1,
                    'semantic_type': current_section_type,
                    'confidence': section_confidence / max(len(current_content.split('\n\n')), 1)
                })
            
            return sections if sections else [{
                'title': '–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ',
                'content': content,
                'level': 1,
                'semantic_type': 'main_content',
                'confidence': 0.5
            }]
            
        except Exception as e:
            logger.error(f"SBERT section detection failed: {e}")
            return [{
                'title': '–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ',
                'content': content,
                'level': 1,
                'semantic_type': 'fallback',
                'confidence': 0.1
            }]
    
    def _generate_section_title(self, section_type: str, content: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ç–∏–ø–∞"""
        
        title_map = {
            'introduction': '–û–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è',
            'technical': '–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è',
            'procedure': '–ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è',
            'control': '–ö–æ–Ω—Ç—Ä–æ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞',
            'conclusion': '–ó–∞–∫–ª—é—á–µ–Ω–∏–µ',
            'main_content': '–û—Å–Ω–æ–≤–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ',
            'fallback': '–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ'
        }
        
        base_title = title_map.get(section_type, '–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ')
        
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏
        first_sentence = content.split('.')[0].strip()
        if len(first_sentence) < 100 and len(first_sentence.split()) > 2:
            # –ü–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–µ –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫
            return first_sentence
        
        return base_title
    
    def _sbert_table_detection(self, content: str, sbert_model) -> List[Dict]:
        """ENTERPRISE RAG 3.0: –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü —Å VLM –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π"""
        
        tables = []
        
        # ENTERPRISE: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–∞–±–ª–∏—Ü
        table_patterns = [
            r'–¢–∞–±–ª–∏—Ü–∞\s+\d+',
            r'Table\s+\d+', 
            r'\|[^|\n]*\|[^|\n]*\|',  # –ú–∞—Ä–∫–¥–∞—É–Ω —Ç–∞–±–ª–∏—Ü—ã
            r'\s{2,}\w+\s{2,}\w+\s{2,}\w+\s{2,}',  # –¢–∞–±—É–ª—è—Ü–∏—è
            r'^\s*\d+\.\d+\.\d+\s+.*\n.*\n.*\n',  # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
            r'^\s*[A-–Ø–Å]{2,}\s+\d+.*\n.*\n.*\n',  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
        ]
        
        for pattern in table_patterns:
            for match in re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE):
                # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç–∞–±–ª–∏—Ü—ã
                start = max(0, match.start() - 100)
                end = min(len(content), match.end() + 300)
                table_content = content[start:end]
                
                # ENTERPRISE: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç–∞–±–ª–∏—Ü—ã
                table_type = self._classify_table_type(table_content)
                
                tables.append({
                    'title': match.group()[:50],
                    'position': match.start(),
                    'content': table_content,
                    'detection_method': 'regex',
                    'table_type': table_type,
                    'metadata': {
                        'data_type': 'TABLE',
                        'structured': True,
                        'parent_section': self._find_parent_section(match.start(), content)
                    }
                })
        
        return tables
    
    def _enhanced_list_detection(self, content: str) -> List[Dict]:
        """ENTERPRISE RAG 3.0: –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        
        lists = []
        
        # ENTERPRISE: –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–ø–∏—Å–∫–æ–≤
        list_patterns = [
            # –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            (r'^\s*[‚Äì‚Äî‚Ä¢]\s+([^–∞-—è]*[A-–Ø–∞-—è—ë][^–Ω–∞-—è—ë]*)', 'BULLET_LIST'),
            (r'^\s*[‚Ä¢¬∑]\s+([^–∞-—è]*[A-–Ø–∞-—è—ë][^–Ω–∞-—è—ë]*)', 'BULLET_LIST'),
            
            # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            (r'^\s*(\d+\))\s+([A-–Ø–∞-—è—ë].*)', 'NUMBERED_LIST'),
            (r'^\s*(\d+\.)\s+([A-–Ø–∞-—è—ë].*)', 'NUMBERED_LIST'),
            
            # –ë—É–∫–≤–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            (r'^\s*([a-z–∞-—è]\))\s+([A-–Ø–∞-—è—ë].*)', 'LETTER_LIST'),
            (r'^\s*([A-–ê-–Ø]\))\s+([A-–Ø–∞-—è—ë].*)', 'LETTER_LIST'),
            
            # –†–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã
            (r'^\s*([IVX]+\))\s+([A-–Ø–∞-—è—ë].*)', 'ROMAN_LIST'),
        ]
        
        for pattern, list_type in list_patterns:
            for match in re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE):
                # –†–∞—Å—à–∏—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ø–∏—Å–∫–∞
                start = max(0, match.start() - 50)
                end = min(len(content), match.end() + 200)
                list_content = content[start:end]
                
                # ENTERPRISE: –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–ø–∏—Å–∫–∞
                list_category = self._classify_list_type(list_content)
                
                lists.append({
                    'title': match.group()[:50],
                    'position': match.start(),
                    'content': list_content,
                    'list_type': list_type,
                    'list_category': list_category,
                    'metadata': {
                        'data_type': 'LIST',
                        'structured': True,
                        'parent_section': self._find_parent_section(match.start(), content),
                        'hierarchy_level': self._determine_list_level(match.group())
                    }
                })
        
        return lists
    
    def _classify_list_type(self, list_content: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ —Å–ø–∏—Å–∫–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è"""
        
        if '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ' in list_content.lower() or '—É—Å–ª–æ–≤–∏–µ' in list_content.lower():
            return 'REQUIREMENT_LIST'
        elif '–º–∞—Ç–µ—Ä–∏–∞–ª' in list_content.lower() or '—Ä–µ—Å—É—Ä—Å' in list_content.lower():
            return 'MATERIAL_LIST'
        elif '—Ä–∞—Å—á–µ—Ç' in list_content.lower() or '—Ñ–æ—Ä–º—É–ª–∞' in list_content.lower():
            return 'CALCULATION_LIST'
        elif '–Ω–æ—Ä–º–∞' in list_content.lower() or '—Å—Ç–∞–Ω–¥–∞—Ä—Ç' in list_content.lower():
            return 'NORM_LIST'
        else:
            return 'GENERAL_LIST'
    
    def _determine_list_level(self, list_item: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ —Å–ø–∏—Å–∫–∞"""
        
        if re.match(r'^\s*\d+\.', list_item):
            return 1
        elif re.match(r'^\s*\d+\.\d+', list_item):
            return 2
        elif re.match(r'^\s*\d+\.\d+\.\d+', list_item):
            return 3
        elif re.match(r'^\s*[a-z–∞-—è]\)', list_item):
            return 3
        elif re.match(r'^\s*[A-–ê-–Ø]\)', list_item):
            return 2
        else:
            return 1
    
    def _classify_table_type(self, table_content: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è"""
        
        if '—Å—Ç–æ–∏–º–æ—Å—Ç—å' in table_content.lower() or '—Ü–µ–Ω–∞' in table_content.lower():
            return 'COST_TABLE'
        elif '–º–∞—Ç–µ—Ä–∏–∞–ª' in table_content.lower() or '—Ä–µ—Å—É—Ä—Å' in table_content.lower():
            return 'MATERIAL_TABLE'
        elif '–Ω–æ—Ä–º–∞' in table_content.lower() or '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ' in table_content.lower():
            return 'NORM_TABLE'
        elif '—Ä–∞—Å—á–µ—Ç' in table_content.lower() or '—Ñ–æ—Ä–º—É–ª–∞' in table_content.lower():
            return 'CALCULATION_TABLE'
        else:
            return 'GENERAL_TABLE'
    
    def _find_parent_section(self, position: int, content: str) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫—É—é —Å–µ–∫—Ü–∏—é –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã"""
        
        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤—ã—à–µ –ø–æ–∑–∏—Ü–∏–∏
        lines = content[:position].split('\n')
        for line in reversed(lines):
            if re.match(r'^\s*\d+\.', line.strip()) or re.match(r'^\s*[A-–Ø–Å]{4,}', line.strip()):
                return line.strip()[:50]
        return "Unknown Section"
    
    def _calculate_structure_complexity(self, sections: List[Dict], tables: List[Dict], lists: List[Dict]) -> float:
        """ENTERPRISE RAG 3.0: –†–∞—Å—á–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        sections_count = len(sections)
        tables_count = len(tables)
        lists_count = len(lists)
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ —É—Ä–æ–≤–Ω—è–º –∏–µ—Ä–∞—Ä—Ö–∏–∏
        max_level = max([s.get('level', 1) for s in sections], default=1)
        avg_level = sum([s.get('level', 1) for s in sections]) / max(sections_count, 1)
        
        # –°–ª–æ–∂–Ω–æ—Å—Ç—å –ø–æ —Ç–∏–ø–∞–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        structured_content_ratio = (tables_count + lists_count) / max(sections_count, 1)
        
        # –û–±—â–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å (0-1)
        complexity = min(1.0, (
            (sections_count * 0.1) +  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–µ–∫—Ü–∏–π
            (max_level * 0.2) +      # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å
            (avg_level * 0.15) +     # –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
            (structured_content_ratio * 0.3) +  # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            (tables_count * 0.1) +   # –¢–∞–±–ª–∏—Ü—ã
            (lists_count * 0.05)     # –°–ø–∏—Å–∫–∏
        ) / 2.0)
        
        return round(complexity, 3)
    
    def _build_hierarchical_tree(self, sections: List[Dict], tables: List[Dict], lists: List[Dict]) -> Dict:
        """ENTERPRISE RAG 3.0: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ –¥–µ—Ä–µ–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        tree = {
            'root': {
                'type': 'DOCUMENT',
                'children': [],
                'metadata': {
                    'total_sections': len(sections),
                    'total_tables': len(tables),
                    'total_lists': len(lists)
                }
            }
        }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ –ø–æ–∑–∏—Ü–∏–∏
        all_elements = []
        
        for section in sections:
            all_elements.append({
                'type': 'SECTION',
                'data': section,
                'position': section.get('position', 0),
                'level': section.get('level', 1)
            })
        
        for table in tables:
            all_elements.append({
                'type': 'TABLE',
                'data': table,
                'position': table.get('position', 0),
                'level': 0  # –¢–∞–±–ª–∏—Ü—ã –≤—Å–µ–≥–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            })
        
        for list_item in lists:
            all_elements.append({
                'type': 'LIST',
                'data': list_item,
                'position': list_item.get('position', 0),
                'level': list_item.get('metadata', {}).get('hierarchy_level', 1)
            })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏
        all_elements.sort(key=lambda x: x['position'])
        
        # –°—Ç—Ä–æ–∏–º –∏–µ—Ä–∞—Ä—Ö–∏—é
        current_path = []
        for element in all_elements:
            self._add_to_hierarchy(tree['root'], element, current_path)
        
        return tree
    
    def _add_to_hierarchy(self, parent: Dict, element: Dict, current_path: List[str]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —ç–ª–µ–º–µ–Ω—Ç –≤ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–µ –¥–µ—Ä–µ–≤–æ"""
        
        element_level = element['level']
        element_type = element['type']
        element_data = element['data']
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—É—Ç—å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Ä–æ–≤–Ω—è
        if element_level <= len(current_path):
            current_path = current_path[:element_level-1]
        
        current_path.append(element_data.get('title', f'{element_type}_{len(current_path)}'))
        
        # –°–æ–∑–¥–∞–µ–º —É–∑–µ–ª
        node = {
            'type': element_type,
            'title': element_data.get('title', ''),
            'content': element_data.get('content', ''),
            'position': element_data.get('position', 0),
            'level': element_level,
            'path': ' > '.join(current_path),
            'children': [],
            'metadata': element_data.get('metadata', {})
        }
        
        parent['children'].append(node)
    
    def _sbert_hierarchy_analysis(self, sections: List[Dict], content: str) -> Dict:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        
        if not sections:
            return {'levels': 1, 'structure': 'flat', 'complexity': 'simple'}
        
        try:
            # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å–µ–∫—Ü–∏—è–º–∏
            section_embeddings = []
            for section in sections:
                if self.sbert_model:
                    section_text = section['title'] + ' ' + section['content'][:200]
                    emb = self.sbert_model.encode([section_text])[0]
                    section_embeddings.append(emb)
            
            # –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞
            similarity_matrix = []
            for i, emb1 in enumerate(section_embeddings):
                row = []
                for j, emb2 in enumerate(section_embeddings):
                    if i != j:
                        sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
                        row.append(float(sim))
                    else:
                        row.append(1.0)
                similarity_matrix.append(row)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            avg_similarity = np.mean([np.mean(row) for row in similarity_matrix])
            
            if avg_similarity > 0.7:
                complexity = 'high_coherence'
            elif avg_similarity > 0.4:
                complexity = 'medium_coherence' 
            else:
                complexity = 'low_coherence'
            
            return {
                'levels': len(set(s.get('level', 1) for s in sections)),
                'structure': 'hierarchical' if len(sections) > 3 else 'flat',
                'complexity': complexity,
                'avg_similarity': float(avg_similarity),
                'sections_count': len(sections)
            }
            
        except Exception as e:
            logger.debug(f"Hierarchy analysis failed: {e}")
            return {
                'levels': 1,
                'structure': 'simple',
                'complexity': 'unknown',
                'sections_count': len(sections)
            }
    
    def _structural_analysis_fallback(self, content: str) -> Dict[str, Any]:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ SBERT"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —á–∞–Ω–∫–µ—Ä
        hierarchical_chunks = self.chunker.create_hierarchical_chunks(content)
        
        sections = []
        for i, chunk in enumerate(hierarchical_chunks):
            if hasattr(chunk, 'content') and chunk.content:
                sections.append({
                    'title': getattr(chunk, 'title', f'–†–∞–∑–¥–µ–ª {i+1}'),
                    'content': chunk.content,
                    'level': getattr(chunk, 'level', 1),
                    'semantic_type': 'fallback',
                    'confidence': 0.3
                })
        
        return {
            'sections': sections,
            'paragraphs_count': sum(len(s['content'].split('\n')) for s in sections),
            'tables': [],
            'hierarchy': {'levels': 1, 'structure': 'fallback', 'complexity': 'simple'},
            'structural_completeness': 0.5,
            'analysis_method': 'fallback_chunker'
        }
    
    def _calculate_structural_completeness(self, sections: List[Dict]) -> float:
        """–†–∞—Å—á–µ—Ç –ø–æ–ª–Ω–æ—Ç—ã —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        
        if not sections:
            return 0.0
        
        score = 0.0
        
        # –ü–æ–ª–Ω–æ—Ç–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–µ–∫—Ü–∏–π
        sections_score = min(len(sections) / 5.0, 0.4)
        score += sections_score
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        good_titles = sum(1 for s in sections if len(s.get('title', '')) > 5)
        titles_score = min(good_titles / len(sections), 0.3)
        score += titles_score
        
        # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence_score = sum(s.get('confidence', 0.0) for s in sections) / len(sections)
        score += confidence_score * 0.3
        
        return min(score, 1.0)
    
    def _log_progress(self, current: int, total: int, operation: str):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–∞–∂–¥—ã–µ 10%"""
        
        if total == 0:
            return
            
        percent = (current * 100) // total
        prev_percent = ((current - 1) * 100) // total if current > 0 else -1
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–µ 10%
        if percent // 10 > prev_percent // 10 or current == total:
            logger.info(f"{operation}: {percent}% ({current}/{total})")
    
    def _enhance_norms_structure(self, structural_data: Dict, content: str) -> Dict:
        """–£–ª—É—á—à–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        norm_elements = []
        
        punkt_pattern = r'(\d+\.(?:\d+\.)*)\s+([–ê-–Ø–Å–∞-—è—ë].*?)(?=\n\d+\.|\n[–ê-–Ø–Å]{5,}|\Z)'
        punkts = re.findall(punkt_pattern, content, re.DOTALL)
        
        for nummer, text in punkts:
            norm_elements.append({
                'type': 'punkt',
                'number': nummer,
                'text': text.strip()[:200],
                'level': nummer.count('.')
            })
        
        structural_data['norm_elements'] = norm_elements
        structural_data['punkts_count'] = len(punkts)
        
        return structural_data
    
    def _enhance_ppr_structure(self, structural_data: Dict, content: str) -> Dict:
        """–£–ª—É—á—à–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è –ü–ü–†"""
        
        stages_pattern = r'(–≠—Ç–∞–ø\s+\d+|–°—Ç–∞–¥–∏—è\s+\d+)[:\.]?\s+([A-–Ø–Å–∞-—è—ë].*?)(?=–≠—Ç–∞–ø\s+\d+|–°—Ç–∞–¥–∏—è\s+\d+|\Z)'
        stages = re.findall(stages_pattern, content, re.DOTALL | re.IGNORECASE)
        
        ppr_stages = []
        for stage_num, stage_text in stages:
            ppr_stages.append({
                'type': 'work_stage',
                'number': stage_num,
                'description': stage_text.strip()[:300]
            })
        
        structural_data['ppr_stages'] = ppr_stages
        structural_data['stages_count'] = len(ppr_stages)
        
        return structural_data
    
    def _enhance_smeta_structure(self, structural_data: Dict, content: str) -> Dict:
        """–£–ª—É—á—à–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–ª—è —Å–º–µ—Ç"""
        
        smeta_pattern = r'(\d+(?:\.\d+)*)\s+([A-–Ø–Å–∞-—è—ë].*?)\s+(\d+[,.]?\d*)\s+(\d+[,.]?\d*)\s+(\d+[,.]?\d*)'
        positions = re.findall(smeta_pattern, content)
        
        smeta_items = []
        for pos_num, description, quantity, rate, sum_val in positions:
            smeta_items.append({
                'type': 'smeta_item',
                'position': pos_num,
                'description': description.strip()[:200],
                'quantity': quantity,
                'rate': rate,
                'sum': sum_val
            })
        
        structural_data['smeta_items'] = smeta_items
        structural_data['items_count'] = len(smeta_items)
        
        return structural_data
    
    def _stage6_regex_to_sbert(self, content: str, doc_type_info: Dict, structural_data: Dict) -> List[str]:
        """STAGE 6: SBERT-First Works Extraction (SBERT –æ—Å–Ω–æ–≤–Ω–æ–π + regex –ø–æ–º–æ—â–Ω–∏–∫)"""
        
        logger.info(f"[Stage 6/14] SBERT-FIRST WORKS EXTRACTION - Type: {doc_type_info['doc_type']}")
        start_time = time.time()
        
        # üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫—Ä–∞—Ö–∞
        if structural_data is None:
            logger.error("[ERROR] Stage 6 received None structural_data. Returning empty works list.")
            return []
        
        if not isinstance(structural_data, dict):
            logger.error(f"[ERROR] Stage 6 received invalid structural_data type: {type(structural_data)}. Returning empty works list.")
            return []
        
        # –®–ê–ì–ê 1: SBERT —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç (–û–°–ù–û–í–ù–û–ô)
        sbert_works = self._sbert_works_extraction(content, doc_type_info, structural_data)
        logger.info(f"SBERT found {len(sbert_works)} semantic works")
        
        # –®–ê–ì 2: Regex –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç (–î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô)
        regex_works = self._regex_works_extraction(content, doc_type_info, structural_data)
        logger.info(f"Regex found {len(regex_works)} pattern-based works")
        
        # –®–ê–ì 3: –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ SBERT
        all_works = sbert_works + regex_works
        validated_works = self._validate_and_rank_works(all_works, content)
        
        # –®–ê–ì 4: –§–∏–Ω–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π
        final_works = self._filter_seed_works(validated_works)
        
        # üöÄ –£–õ–£–ß–®–ï–ù–ò–ï –¢–û–ß–ù–û–°–¢–ò: –ü–æ—Å—Ç-–≤–∞–ª–∏–¥–∞—Ü–∏—è seed-—Ä–∞–±–æ—Ç
        STOP_WORDS = {'—Å–æ–≥–ª–∞—Å–Ω–æ', '–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Ç–∞–±–ª–∏—Ü–∞', '—Ä–∏—Å—É–Ω–æ–∫', '—Å–ø–∏—Å–æ–∫', '–ø–µ—Ä–µ—á–µ–Ω—å'}
        filtered_works = []
        
        for work in final_works:
            if not any(sw in work.lower() for sw in STOP_WORDS):
                filtered_works.append(work)
            else:
                logger.warning(f"[WARN] Seed-—Ä–∞–±–æ—Ç–∞ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞ (—Å—Ç–æ–ø-—Å–ª–æ–≤–∞): {work}")
        
        logger.info(f"[ACCURACY] Seed-—Ä–∞–±–æ—Ç—ã –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(filtered_works)}/{len(final_works)}")
        final_works = filtered_works
        
        elapsed = time.time() - start_time
        logger.info(f"[Stage 6/14] COMPLETE - SBERT: {len(sbert_works)}, Regex: {len(regex_works)}, Final: {len(final_works)} ({elapsed:.2f}s)")
        
        return final_works
    
    def _sbert_works_extraction(self, content: str, doc_type_info: Dict, structural_data: Dict) -> List[str]:
        """–û—Å–Ω–æ–≤–Ω–æ–π SBERT —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç"""
        
        if not self.sbert_model or not HAS_ML_LIBS:
            logger.warning("SBERT not available - semantic search disabled")
            return []
        
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            sentences = []
            for section in structural_data.get('sections', []):
                section_sentences = self._split_into_sentences(section.get('content', ''))
                sentences.extend(section_sentences)
            
            if not sentences:
                # Fallback - —Ä–∞–∑–±–∏–≤–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç
                sentences = self._split_into_sentences(content)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            filtered_sentences = [s for s in sentences if len(s.split()) > 5 and len(s) < 300]
            
            if not filtered_sentences:
                return []
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
            sentence_embeddings = self.sbert_model.encode(filtered_sentences, show_progress_bar=False)
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ FP16
            if hasattr(sentence_embeddings, 'to'):
                sentence_embeddings = sentence_embeddings.to(torch.float16)
            
            # –®–∞–±–ª–æ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–∞–±–æ—Ç (—Ä–∞–∑–ª–∏—á–Ω—ã–µ –¥–ª—è —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
            work_templates = self._get_work_templates_by_type(doc_type_info['doc_type'])
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —à–∞–±–ª–æ–Ω–æ–≤
            template_embeddings = self.sbert_model.encode(list(work_templates.values()), show_progress_bar=False)
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ FP16
            if hasattr(template_embeddings, 'to'):
                template_embeddings = template_embeddings.to(torch.float16)
            
            # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —à–∞–±–ª–æ–Ω—ã —Ä–∞–±–æ—Ç
            semantic_works = []
            
            for sentence, sentence_emb in zip(filtered_sentences, sentence_embeddings):
                max_similarity = 0.0
                best_work_type = None
                
                for work_type, template_emb in zip(work_templates.keys(), template_embeddings):
                    similarity = np.dot(sentence_emb, template_emb) / (
                        np.linalg.norm(sentence_emb) * np.linalg.norm(template_emb)
                    )
                    
                    if similarity > max_similarity:
                        max_similarity = similarity
                        best_work_type = work_type
                
                # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–æ–µ
                if max_similarity > 0.4:  # –ü–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
                    semantic_works.append(sentence)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
            keyword_works = self._find_works_by_keywords(filtered_sentences, doc_type_info['doc_type'])
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_semantic_works = list(set(semantic_works + keyword_works))
            
            return all_semantic_works
            
        except Exception as e:
            logger.error(f"SBERT works extraction failed: {e}")
            return []
    
    def _get_work_templates_by_type(self, doc_type: str) -> Dict[str, str]:
        """–ü–æ–ª—É—á–∞–µ–º —à–∞–±–ª–æ–Ω—ã —Ä–∞–±–æ—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        base_templates = {
            'construction': '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ –≤–æ–∑–≤–µ–¥–µ–Ω–∏–µ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–µ –ø–æ—Å—Ç—Ä–æ–π–∫–∞',
            'installation': '—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –º–æ–Ω—Ç–∞–∂ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –∫—Ä–µ–ø–ª–µ–Ω–∏–µ',
            'preparation': '–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—á–∏—Å—Ç–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–Ω—Ç–æ–≤–∫–∞',
            'inspection': '–∫–æ–Ω—Ç—Ä–æ–ª—å –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø—ã—Ç–∞–Ω–∏–µ –æ—Å–º–æ—Ç—Ä –∏–∑–º–µ—Ä–µ–Ω–∏–µ',
            'finishing': '–æ—Ç–¥–µ–ª–∫–∞ –ø–æ–∫—Ä–∞—Å–∫–∞ —à—Ç—É–∫–∞—Ç—É—Ä–∫–∞ –æ–±–ª–∏—Ü–æ–≤–∫–∞'
        }
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ç–∏–ø–æ–≤
        if doc_type == 'ppr':
            base_templates.update({
                'earthworks': '–∑–µ–º–ª—è–Ω—ã–µ —Ä–∞–±–æ—Ç—ã –≤—ã–µ–º–∫–∞ –∫–æ—Ç–ª–æ–≤–∞–Ω —Ç—Ä–∞–Ω—à–µ—è',
                'concrete': '–±–µ—Ç–æ–Ω–Ω—ã–µ —Ä–∞–±–æ—Ç—ã –∑–∞–ª–∏–≤–∫–∞ –±–µ—Ç–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ',
                'reinforcement': '–∞—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä–º–∞—Ç—É—Ä–∞ —Å–≤—è–∑–∫–∞'
            })
        elif doc_type == 'smeta':
            base_templates.update({
                'materials': '–º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ—Å—Ç–∞–≤–∫–∞ –∑–∞–∫—É–ø–∫–∞',
                'transport': '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–µ—Ä–µ–≤–æ–∑–∫–∞ –¥–æ—Å—Ç–∞–≤–∫–∞',
                'machinery': '–º–∞—à–∏–Ω—ã –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫–∞'
            })
        elif doc_type == 'norms':
            base_templates.update({
                'requirements': '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–æ—Ä–º—ã —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è',
                'testing': '–∏—Å–ø—ã—Ç–∞–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∞',
                'standards': '—Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –º–µ—Ç–æ–¥–∏–∫–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞'
            })
        
        return base_templates
    
    def _find_works_by_keywords(self, sentences: List[str], doc_type: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ä–∞–±–æ—Ç –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ SBERT)"""
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —Ä–∞–±–æ—Ç
        work_keywords = {
            'actions': ['–≤—ã–ø–æ–ª–Ω', '–ø—Ä–æ–∏–∑–≤–æ–¥', '–æ—Å—É—â–µ—Å—Ç–≤–ª', '—Ä–µ–∞–ª–∏–∑'],
            'processes': ['–ø—Ä–æ—Ü–µ—Å—Å', '–æ–ø–µ—Ä–∞—Ü–∏', '—ç—Ç–∞–ø', '—Å—Ç–∞–¥–∏'],
            'construction': ['—Å—Ç—Ä–æ–∏—Ç–µ–ª—å', '—Å–æ–æ—Ä—É–∂', '–º–æ–Ω—Ç–∞–∂', '—É—Å—Ç–∞–Ω–æ–≤'],
            'technical': ['–æ–±—Ä–∞–±–æ—Ç', '–∏–∑–≥–æ—Ç–æ–≤', '–ø–æ–¥–≥–æ—Ç–æ–≤', '–æ–±–µ—Å–ø–µ—á']
        }
        
        keyword_works = []
        
        for sentence in sentences:
            sentence_lower = sentence.lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            has_work_keywords = False
            for category, keywords in work_keywords.items():
                if any(kw in sentence_lower for kw in keywords):
                    has_work_keywords = True
                    break
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (—Å–æ–¥–µ—Ä–∂–∏—Ç –≥–ª–∞–≥–æ–ª)
            has_action_structure = any(word in sentence_lower for word in ['–µ—Ç—Å—è', '—Ç—å—Å—è', '–ª—è–µ—Ç', '–∞–µ—Ç'])
            
            if has_work_keywords and has_action_structure:
                keyword_works.append(sentence)
        
        return keyword_works
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–æ—á–∫–∞–º, –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º –∏ –≤–æ–ø—Ä–æ—Å–∏—Ç–µ–ª—å–Ω—ã–º –∑–Ω–∞–∫–∞–º
        sentences = re.split(r'[.!?]+\s+', text)
        
        # –£–±–∏—Ä–∞–µ–º –ø—É—Å—Ç—ã–µ –∏ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20:  # –ú–∏–Ω–∏–º—É–º 20 —Å–∏–º–≤–æ–ª–æ–≤
                cleaned_sentences.append(sentence)
        
        return cleaned_sentences
    
    def _regex_works_extraction(self, content: str, doc_type_info: Dict, structural_data: Dict) -> List[str]:
        """–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π Regex –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç (–±—ã–≤—à–∏–π –æ—Å–Ω–æ–≤–Ω–æ–π)"""
        
        regex_works = []
        
        if doc_type_info['doc_type'] == 'norms':
            regex_works = self._extract_norms_seeds(content, structural_data)
        elif doc_type_info['doc_type'] == 'ppr':
            regex_works = self._extract_ppr_seeds(content, structural_data)
        elif doc_type_info['doc_type'] == 'smeta':
            regex_works = self._extract_smeta_seeds(content, structural_data)
        else:
            regex_works = self._extract_generic_seeds(content, structural_data)
        
        return regex_works
    
    def _validate_and_rank_works(self, all_works: List[str], content: str) -> List[str]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç —á–µ—Ä–µ–∑ SBERT"""
        
        if not all_works or not self.sbert_model:
            return all_works
        
        try:
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏
            unique_works = []
            normalized_works = set()
            
            for work in all_works:
                normalized = self._normalize_russian_text(work.lower())
                if normalized not in normalized_works:
                    normalized_works.add(normalized)
                    unique_works.append(work)
            
            if not unique_works:
                return []
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ä–∞–±–æ—Ç
            work_embeddings = self.sbert_model.encode(unique_works, show_progress_bar=False)
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ FP16
            if hasattr(work_embeddings, 'to'):
                work_embeddings = work_embeddings.to(torch.float16)
            
            # –û—Ü–µ–Ω–∏–≤–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É –∫–æ–Ω—Ç–µ–Ω—Ç—É
            content_sample = ' '.join(content.split()[:2000])  # –ü–µ—Ä–≤—ã–µ 2000 —Å–ª–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            content_embedding = self.sbert_model.encode([content_sample], show_progress_bar=False)[0]
            # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ FP16
            if hasattr(content_embedding, 'to'):
                content_embedding = content_embedding.to(torch.float16)
            
            # –†–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            work_scores = []
            for i, (work, work_emb) in enumerate(zip(unique_works, work_embeddings)):
                relevance = np.dot(work_emb, content_embedding) / (
                    np.linalg.norm(work_emb) * np.linalg.norm(content_embedding)
                )
                work_scores.append((work, float(relevance)))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            work_scores.sort(key=lambda x: x[1], reverse=True)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –í–°–ï —Ä–∞–±–æ—Ç—ã —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å—é (–Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ç–æ–ø-50!)
            validated_works = [work for work, score in work_scores if score > 0.15]  # –ü–æ–Ω–∏–∂–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥
            
            logger.info(f"SBERT validation: {len(unique_works)} -> {len(validated_works)} works (threshold: 0.15)")
            
            return validated_works  # –í–°–ï –≤–∞–ª–∏–¥–Ω—ã–µ —Ä–∞–±–æ—Ç—ã, –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ–ø-50!
            
        except Exception as e:
            logger.error(f"Work validation failed: {e}")
            return all_works
    
    def _extract_norms_seeds(self, content: str, structural_data: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ seeds –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        seeds = []
        
        if 'norm_elements' in structural_data:
            for element in structural_data['norm_elements']:
                if element['type'] == 'punkt' and len(element['text']) > 20:
                    seeds.append(f"–ø.{element['number']} {element['text']}")
        
        gost_patterns = [
            r'—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è?\s+–∫\s+[–ê-–Ø–Å–∞-—è—ë\s]+',
            r'–º–µ—Ç–æ–¥—ã?\s+[–ê-–Ø–Å–∞-—è—ë\s]+',
            r'–∫–æ–Ω—Ç—Ä–æ–ª—å\s+[–ê-–Ø–Å–∞-—è—ë\s]+',
            r'–∏—Å–ø—ã—Ç–∞–Ω–∏—è?\s+[–ê-–Ø–Å–∞-—è—ë\s]+'
        ]
        
        for pattern in gost_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            seeds.extend([match[:100] for match in matches if len(match) > 10])
        
        return seeds
    
    def _extract_ppr_seeds(self, content: str, structural_data: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ seeds –∏–∑ –ü–ü–†"""
        
        seeds = []
        
        if 'ppr_stages' in structural_data:
            for stage in structural_data['ppr_stages']:
                seeds.append(f"{stage['number']}: {stage['description']}")
        
        work_patterns = [
            r'–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ\s+[–ê-–Ø–Å–∞-—è—ë\s]+',
            r'—É—Å—Ç–∞–Ω–æ–≤–∫–∞\s+[–ê-–Ø–Å–∞-—è—ë\s]+',
            r'–º–æ–Ω—Ç–∞–∂\s+[–ê-–Ø–Å–∞-—è—ë\s]+',
            r'–¥–µ–º–æ–Ω—Ç–∞–∂\s+[–ê-–Ø–Å–∞-—è—ë\s]+',
            r'—É–∫–ª–∞–¥–∫–∞\s+[–ê-–Ø–Å–∞-—è—ë\s]+',
            r'–±–µ—Ç–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ\s+[–ê-–Ø–Å–∞-—è—ë\s]+',
            r'—Å–≤–∞—Ä–∫–∞\s+[–ê-–Ø–Å–∞-—è—ë\s]+'
        ]
        
        for pattern in work_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            seeds.extend([match[:80] for match in matches if len(match) > 8])
        
        return seeds
    
    def _extract_smeta_seeds(self, content: str, structural_data: Dict) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ seeds –∏–∑ —Å–º–µ—Ç"""
        
        seeds = []
        
        if 'smeta_items' in structural_data:
            for item in structural_data['smeta_items']:
                seeds.append(f"–ü–æ–∑.{item['position']}: {item['description']}")
        
        work_types = [
            r'–∑–µ–º–ª—è–Ω—ã–µ\s+—Ä–∞–±–æ—Ç—ã',
            r'–±–µ—Ç–æ–Ω–Ω—ã–µ\s+—Ä–∞–±–æ—Ç—ã',
            r'–∂–µ–ª–µ–∑–æ–±–µ—Ç–æ–Ω–Ω—ã–µ\s+—Ä–∞–±–æ—Ç—ã',
            r'–∫–∞–º–µ–Ω–Ω—ã–µ\s+—Ä–∞–±–æ—Ç—ã',
            r'–º–æ–Ω—Ç–∞–∂–Ω—ã–µ\s+—Ä–∞–±–æ—Ç—ã',
            r'–æ—Ç–¥–µ–ª–æ—á–Ω—ã–µ\s+—Ä–∞–±–æ—Ç—ã'
        ]
        
        for pattern in work_types:
            matches = re.findall(pattern, content, re.IGNORECASE)
            seeds.extend([match for match in matches])
        
        return seeds
    
    def _extract_generic_seeds(self, content: str, structural_data: Dict) -> List[str]:
        """–û–±—â–µ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ seeds –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤"""
        
        seeds = []
        
        for section in structural_data.get('sections', []):
            if len(section['content']) > 30:
                sentences = re.split(r'[.!?]\s+', section['content'])
                for sentence in sentences[:2]:
                    if len(sentence.strip()) > 15:
                        seeds.append(sentence.strip()[:100])
        
        return seeds
    
    def _filter_seed_works(self, seeds: List[str]) -> List[str]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è seed works —Å —Ä—É—Å—Å–∫–æ–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π"""
        
        filtered = []
        seen = set()
        
        for seed in seeds:
            seed = seed.strip()
            seed = re.sub(r'\s+', ' ', seed)
            
            if len(seed) < 10 or len(seed) > 200:
                continue
            if re.match(r'^\d+$', seed):
                continue
            if len(seed.split()) < 3:
                continue
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å —É—á–µ—Ç–æ–º —Ä—É—Å—Å–∫–∏—Ö –æ–∫–æ–Ω—á–∞–Ω–∏–π
            normalized_seed = self._normalize_russian_text(seed)
            
            if normalized_seed.lower() in seen:
                continue
            
            seen.add(normalized_seed.lower())
            filtered.append(seed)  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª, –Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π
        
        logger.info(f"Morphology filter: {len(seeds)} -> {len(filtered)} unique works (no limit!)")
        
        return filtered  # –í–°–ï —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–∞–±–æ—Ç—ã, –ë–ï–ó –û–ì–†–ê–ù–ò–ß–ï–ù–ò–ô!
    
    def _normalize_russian_text(self, text: str) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –±–µ–∑ pymorphy2"""
        
        # –°–ª–æ–≤–∞—Ä—å –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ–∫–æ–Ω—á–∞–Ω–∏–π –∏ –∏—Ö –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–æ—Ä–º
        endings_map = {
            # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
            '—Ä–∞–±–æ—Ç–∞': '—Ä–∞–±–æ—Ç', '—Ä–∞–±–æ—Ç—É': '—Ä–∞–±–æ—Ç', '—Ä–∞–±–æ—Ç—ã': '—Ä–∞–±–æ—Ç', '—Ä–∞–±–æ—Ç–µ': '—Ä–∞–±–æ—Ç',
            '–ø—Ä–æ–µ–∫—Ç–∞': '–ø—Ä–æ–µ–∫—Ç', '–ø—Ä–æ–µ–∫—Ç—É': '–ø—Ä–æ–µ–∫—Ç', '–ø—Ä–æ–µ–∫—Ç–µ': '–ø—Ä–æ–µ–∫—Ç', '–ø—Ä–æ–µ–∫—Ç–æ–º': '–ø—Ä–æ–µ–∫—Ç',
            '–∑–¥–∞–Ω–∏—è': '–∑–¥–∞–Ω–∏', '–∑–¥–∞–Ω–∏—é': '–∑–¥–∞–Ω–∏', '–∑–¥–∞–Ω–∏–µ': '–∑–¥–∞–Ω–∏', '–∑–¥–∞–Ω–∏–µ–º': '–∑–¥–∞–Ω–∏',
            '–º–∞—Ç–µ—Ä–∏–∞–ª–∞': '–º–∞—Ç–µ—Ä–∏–∞–ª', '–º–∞—Ç–µ—Ä–∏–∞–ª—É': '–º–∞—Ç–µ—Ä–∏–∞–ª', '–º–∞—Ç–µ—Ä–∏–∞–ª–µ': '–º–∞—Ç–µ—Ä–∏–∞–ª',
            '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏': '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏', '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é': '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏', '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π': '–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏',
            
            # –ì–ª–∞–≥–æ–ª—ã
            '–≤—ã–ø–æ–ª–Ω—è—Ç—å': '–≤—ã–ø–æ–ª–Ω', '–≤—ã–ø–æ–ª–Ω—è–µ—Ç': '–≤—ã–ø–æ–ª–Ω', '–≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ': '–≤—ã–ø–æ–ª–Ω',
            '—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å': '—É—Å—Ç–∞–Ω–æ–≤', '—É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç': '—É—Å—Ç–∞–Ω–æ–≤', '—É—Å—Ç–∞–Ω–æ–≤–∫–∞': '—É—Å—Ç–∞–Ω–æ–≤',
            '–º–æ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å': '–º–æ–Ω—Ç–∏—Ä', '–º–æ–Ω—Ç–∏—Ä—É–µ—Ç': '–º–æ–Ω—Ç–∏—Ä', '–º–æ–Ω—Ç–∞–∂': '–º–æ–Ω—Ç–∏—Ä',
            '–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å': '–∫–æ–Ω—Ç—Ä–æ–ª', '–∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ—Ç': '–∫–æ–Ω—Ç—Ä–æ–ª', '–∫–æ–Ω—Ç—Ä–æ–ª—å': '–∫–æ–Ω—Ç—Ä–æ–ª',
            
            # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ  
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π': '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è': '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫', '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ': '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫',
            '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π': '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–∞—è': '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ': '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω',
        }
        
        words = text.split()
        normalized_words = []
        
        for word in words:
            word_lower = word.lower()
            normalized = word_lower
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if word_lower in endings_map:
                normalized = endings_map[word_lower]
            else:
                # –ü—Ä–æ—Å—Ç–æ–µ –æ—Ç—Å–µ—á–µ–Ω–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏–π
                for ending in ['–∞–º–∏', '–∞–º–∏', '—ã—Ö', '–∏—Ö', '–æ–π', '–∞—è', '–∏–µ', '—ã–µ', '–æ–º', '–µ–º', '–µ—Ç', '—é—Ç', '–∞—Ç']:
                    if word_lower.endswith(ending) and len(word_lower) > len(ending) + 2:
                        normalized = word_lower[:-len(ending)]
                        break
            
            normalized_words.append(normalized)
        
        return ' '.join(normalized_words)
    
    def _stage7_sbert_markup(self, content: str, seed_works: List[str], 
                            doc_type_info: Dict, structural_data: Dict) -> Dict[str, Any]:
        """STAGE 7: SBERT Markup (–ü–û–õ–ù–ê–Ø —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ + –≥—Ä–∞—Ñ)"""
        
        logger.info(f"[Stage 7/14] SBERT MARKUP - Processing {len(seed_works)} seeds")
        start_time = time.time()
        
        # üöÄ CONTEXT SWITCHING: –ó–∞–≥—Ä—É–∂–∞–µ–º SBERT —Ç–æ–ª—å–∫–æ –¥–ª—è Stage 7
        try:
            sbert_model = self._load_sbert_model()
            logger.info(f"[VRAM MANAGER] SBERT loaded for Stage 7")
        except Exception as e:
            logger.error(f"[VRAM MANAGER] Failed to load SBERT for Stage 7: {e}")
            return self._sbert_markup_fallback(seed_works, structural_data)
        
        try:
            if not self.sbert_model or not HAS_ML_LIBS:
                logger.warning("SBERT not available, using fallback")
                return self._sbert_markup_fallback(seed_works, structural_data)
            
            # üöÄ –£–õ–£–ß–®–ï–ù–ò–ï –¢–û–ß–ù–û–°–¢–ò: –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –ø–æ–¥—Å–∫–∞–∑–∫—É –¥–ª—è Rubern
            doc_type = doc_type_info.get('doc_type', 'unknown')
            context_hint = ""
            
            if doc_type == 'sp':
                context_hint = "[–°–ü] –î–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã. –ò–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–∞–∑–¥–µ–ª–æ–≤ –∏ –ø—É–Ω–∫—Ç–æ–≤."
            elif doc_type == 'estimate':
                context_hint = "[–°–ú–ï–¢–ê] –î–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞—Å—Ü–µ–Ω–∫–∏. –ò–∑–≤–ª–µ–∫–∏ –∫–æ–¥—ã —Ä–∞–±–æ—Ç –∏ –º–∞—Ç–µ—Ä–∏–∞–ª—ã."
            elif doc_type == 'ppr':
                context_hint = "[–ü–ü–†] –î–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —ç—Ç–∞–ø—ã —Ä–∞–±–æ—Ç. –ò–∑–≤–ª–µ–∫–∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫–∞—Ä—Ç—ã –∏ —Ä–µ—Å—É—Ä—Å—ã."
            elif doc_type == 'drawing':
                context_hint = "[–ß–ï–†–¢–ï–ñ] –î–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é. –ò–∑–≤–ª–µ–∫–∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è."
            elif doc_type in ['gost', 'snip', 'iso']:
                context_hint = "[–ù–¢–î] –î–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è. –ò–∑–≤–ª–µ–∫–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏."
            
            if context_hint:
                content_with_hint = context_hint + "\n\n" + content
                logger.info(f"[ACCURACY] Rubern context hint added: {doc_type}")
            else:
                content_with_hint = content
            
            if seed_works:
                seed_embeddings = self.sbert_model.encode(seed_works)
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ FP16
                if hasattr(seed_embeddings, 'to'):
                    seed_embeddings = seed_embeddings.to(torch.float16)
            else:
                seed_embeddings = []
            
            # üöÄ RUBERN (SBERT) –ü–û–õ–ù–ê–Ø –°–¢–†–£–ö–¢–£–†–ù–ê–Ø –†–ê–ó–ú–ï–¢–ö–ê
            work_dependencies = self._analyze_work_dependencies(seed_works, seed_embeddings)
            work_graph = self._build_work_graph(seed_works, work_dependencies)
            validated_works = self._validate_works_with_sbert(seed_works, seed_embeddings, content)
            
            # üöÄ –ü–û–õ–ù–ê–Ø –ò–ï–†–ê–†–•–ò–ß–ï–°–ö–ê–Ø –°–¢–†–£–ö–¢–£–†–ê –ß–ï–†–ï–ó RUBERN (—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)
            doc_structure = self._build_hierarchical_structure(structural_data, validated_works, content_with_hint)
            enhanced_structure = self._enhance_structure_with_sbert(structural_data, validated_works)
            
            # üöÄ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ú–ê–¢–ï–†–ò–ê–õ–û–í –ò –†–ï–°–£–†–°–û–í –ß–ï–†–ï–ó RUBERN (—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)
            materials = self._extract_materials_with_sbert(content_with_hint, validated_works, seed_embeddings)
            resources = self._extract_resources_with_sbert(content_with_hint, validated_works, seed_embeddings)
            
            sbert_result = {
                'works': validated_works,
                'dependencies': work_dependencies,
                'work_graph': work_graph,
                'doc_structure': doc_structure,  # –ü–æ–ª–Ω–æ–µ –¥–µ—Ä–µ–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                'enhanced_structure': enhanced_structure,
                'materials': materials,  # –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
                'resources': resources,  # –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
                'embeddings_count': len(seed_embeddings) if seed_embeddings is not None else 0,
                'analysis_method': 'rubern_sbert'
            }
            
            logger.info(f"‚úÖ [Stage 7/14] Rubern —Ä–∞–∑–º–µ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(validated_works)} —Ä–∞–±–æ—Ç, {len(work_dependencies)} –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")
            
        except Exception as e:
            logger.error(f"SBERT markup failed: {e}")
            sbert_result = self._sbert_markup_fallback(seed_works, structural_data)
        finally:
            # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –û—Å—Ç–∞–≤–ª—è–µ–º SBERT –≤ –ø–∞–º—è—Ç–∏ –¥–ª—è Stage 13
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–∞–º—è—Ç–∏ –∏ —Ä–µ–∂–∏–º LLM
            try:
                import torch
                if torch.cuda.is_available():
                    vram_used = torch.cuda.memory_allocated() / 1024**3  # –ì–ë
                    vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    vram_usage = vram_used / vram_total
                    
                    # –û—Å—Ç–∞–≤–ª—è–µ–º SBERT –µ—Å–ª–∏ –ø–∞–º—è—Ç–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏ LLM –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
                    if vram_usage < 0.7 and not self.use_llm:
                        logger.info(f"[PERF] Stage 7-13 SBERT reused: True (VRAM: {vram_usage:.2f})")
                        # –ù–ï –≤—ã–≥—Ä—É–∂–∞–µ–º SBERT - –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è Stage 13
                    else:
                        self._unload_sbert_model()
                        logger.info(f"[PERF] Stage 7-13 SBERT reused: False (VRAM: {vram_usage:.2f})")
                else:
                    # CPU —Ä–µ–∂–∏–º - –æ—Å—Ç–∞–≤–ª—è–µ–º SBERT –≤ –ø–∞–º—è—Ç–∏
                    logger.info(f"[PERF] Stage 7-13 SBERT reused: True (CPU mode)")
                    # –ù–ï –≤—ã–≥—Ä—É–∂–∞–µ–º SBERT
            except Exception as e:
                logger.warning(f"[PERF] SBERT reuse decision failed: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤—ã–≥—Ä—É–∂–∞–µ–º –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
                self._unload_sbert_model()
        
        elapsed = time.time() - start_time
        works_count = len(sbert_result.get('works', []))
        deps_count = len(sbert_result.get('dependencies', []))
        
        # üöÄ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò
        logger.info(f"[PERF] Stage 7 performance: {elapsed:.2f}s, works: {works_count}, deps: {deps_count}")
        
        logger.info(f"[Stage 7/14] COMPLETE - Works: {works_count}, "
                   f"Dependencies: {deps_count} ({elapsed:.2f}s)")
        
        return sbert_result
    
    def _sbert_markup_fallback(self, seed_works: List[str], structural_data: Dict) -> Dict[str, Any]:
        """Fallback –∫–æ–≥–¥–∞ SBERT –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        
        validated_works = []
        for i, work in enumerate(seed_works):
            validated_works.append({
                'id': f"work_{i}",
                'name': work,
                'confidence': 0.5,
                'type': 'generic',
                'section': 'unknown'
            })
        
        dependencies = []
        for i in range(len(validated_works) - 1):
            dependencies.append({
                'from': f"work_{i}",
                'to': f"work_{i+1}",
                'type': 'sequence',
                'confidence': 0.3
            })
        
        return {
            'works': validated_works,
            'dependencies': dependencies,
            'work_graph': {'nodes': validated_works, 'edges': dependencies},
            'enhanced_structure': structural_data,
            'analysis_method': 'fallback'
        }
    
    def _analyze_work_dependencies(self, works: List[str], embeddings) -> List[Dict]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –º–µ–∂–¥—É —Ä–∞–±–æ—Ç–∞–º–∏ —á–µ—Ä–µ–∑ SBERT"""
        
        if not works or embeddings is None or len(embeddings) == 0:
            return []
        
        dependencies = []
        
        try:
            sequence_keywords = ['–ø–æ—Å–ª–µ', '–∑–∞—Ç–µ–º', '–¥–∞–ª–µ–µ', '—Å–ª–µ–¥—É—é—â–∏–π', '—ç—Ç–∞–ø']
            prerequisite_keywords = ['—Ç—Ä–µ–±—É–µ—Ç', '–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ', '–ø–µ—Ä–µ–¥', '–¥–æ']
            
            for i, work1 in enumerate(works):
                for j, work2 in enumerate(works):
                    if i >= j:
                        continue
                    
                    similarity = np.dot(embeddings[i], embeddings[j]) / (
                        np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j])
                    )
                    
                    if similarity > 0.7:
                        dep_type = 'related'
                        confidence = float(similarity)
                        
                        work1_lower = work1.lower()
                        work2_lower = work2.lower()
                        
                        if any(kw in work1_lower for kw in sequence_keywords):
                            dep_type = 'sequence'
                        elif any(kw in work1_lower for kw in prerequisite_keywords):
                            dep_type = 'prerequisite'
                        
                        dependencies.append({
                            'from': f"work_{i}",
                            'to': f"work_{j}",
                            'type': dep_type,
                            'confidence': confidence,
                            'similarity': float(similarity)
                        })
            
        except Exception as e:
            logger.warning(f"Dependency analysis failed: {e}")
        
        return dependencies
    
    def _build_work_graph(self, works: List[str], dependencies: List[Dict]) -> Dict:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Ä–∞–±–æ—Ç"""
        
        nodes = []
        for i, work in enumerate(works):
            nodes.append({
                'id': f"work_{i}",
                'label': work[:50] + ('...' if len(work) > 50 else ''),
                'full_text': work,
                'type': 'work'
            })
        
        edges = []
        for dep in dependencies:
            edges.append({
                'from': dep['from'],
                'to': dep['to'],
                'label': dep['type'],
                'weight': dep['confidence']
            })
        
        return {
            'nodes': nodes,
            'edges': edges,
            'stats': {
                'nodes_count': len(nodes),
                'edges_count': len(edges),
                'avg_connectivity': len(edges) / max(len(nodes), 1)
            }
        }
    
    def _validate_works_with_sbert(self, works: List[str], embeddings, content: str) -> List[Dict]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–∞–±–æ—Ç —Å –ø–æ–º–æ—â—å—é SBERT"""
        
        validated = []
        
        content_sample = ' '.join(content.split()[:1000])  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        
        try:
            if self.sbert_model:
                content_embedding = self.sbert_model.encode([content_sample])[0]
                # –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–ö–°: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ FP16
                if hasattr(content_embedding, 'to'):
                    content_embedding = content_embedding.to(torch.float16)
            else:
                content_embedding = None
        except:
            content_embedding = None
        
        for i, work in enumerate(works):
            work_info = {
                'id': f"work_{i}",
                'name': work,
                'confidence': 0.5,
                'type': 'generic',
                'section': 'unknown',
                'relevance': 0.5
            }
            
            if embeddings is not None and content_embedding is not None and i < len(embeddings):
                try:
                    relevance = np.dot(embeddings[i], content_embedding) / (
                        np.linalg.norm(embeddings[i]) * np.linalg.norm(content_embedding)
                    )
                    work_info['relevance'] = float(relevance)
                    work_info['confidence'] = min(float(relevance) + 0.3, 1.0)
                    
                except:
                    pass
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–∞–±–æ—Ç—ã
            work_lower = work.lower()
            if any(word in work_lower for word in ['–º–æ–Ω—Ç–∞–∂', '—É—Å—Ç–∞–Ω–æ–≤–∫–∞', '—Å–±–æ—Ä–∫–∞']):
                work_info['type'] = 'assembly'
            elif any(word in work_lower for word in ['–±–µ—Ç–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∑–∞–ª–∏–≤–∫–∞']):
                work_info['type'] = 'concrete'
            elif any(word in work_lower for word in ['–∫–æ–Ω—Ç—Ä–æ–ª—å', '–ø—Ä–æ–≤–µ—Ä–∫–∞', '–∏—Å–ø—ã—Ç–∞–Ω–∏–µ']):
                work_info['type'] = 'control'
            elif any(word in work_lower for word in ['–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞', '—Ä–∞–∑–º–µ—Ç–∫–∞']):
                work_info['type'] = 'preparation'
            
            validated.append(work_info)
        
        return validated
    
    def _enhance_structure_with_sbert(self, structural_data: Dict, works: List[Dict]) -> Dict:
        """–î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –∏–∑ SBERT –∞–Ω–∞–ª–∏–∑–∞"""
        
        enhanced = structural_data.copy()
        
        if 'sections' in enhanced:
            for section in enhanced['sections']:
                section['related_works'] = []
                
                section_text = section.get('content', '').lower()
                
                for work in works:
                    work_name = work['name'].lower()
                    
                    if any(word in section_text for word in work_name.split()[:3]):
                        section['related_works'].append(work['id'])
        
        enhanced['sbert_analysis'] = {
            'total_works': len(works),
            'avg_confidence': sum(w['confidence'] for w in works) / max(len(works), 1),
            'work_types': list(set(w['type'] for w in works))
        }
        
        return enhanced
    
    def _stage8_metadata_extraction(self, content: str, structural_data: Dict, 
                                    doc_type_info: Dict, rubern_data: Dict = None) -> DocumentMetadata:
        """üöÄ –ú–ï–¢–ê–î–ê–ù–ù–´–ï –¢–û–õ–¨–ö–û –ò–ó –°–¢–†–£–ö–¢–£–†–´ RUBERN (—Å–∏–º–±–∏–æ—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥—Ö–æ–¥)"""
        
        logger.info(f"[Stage 8/14] RUBERN STRUCTURE METADATA EXTRACTION - Type: {doc_type_info['doc_type']}")
        start_time = time.time()
        
        doc_type = doc_type_info['doc_type']
        metadata = DocumentMetadata()
        
        # üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –ü–†–ò–ù–¶–ò–ü: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¢–û–õ–¨–ö–û –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Rubern
        if not rubern_data:
            logger.warning("[STAGE8_DEBUG] No Rubern data provided - using fallback")
            return self._extract_metadata_fallback(content, doc_type_info)
        
        logger.info("‚úÖ [Stage 8/14] –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã –¢–û–õ–¨–ö–û –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã Rubern")
        
        # üöÄ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ú–ï–¢–ê–î–ê–ù–ù–´–• –ò–ó –°–¢–†–£–ö–¢–£–†–´ RUBERN
        metadata = self._extract_metadata_from_rubern_structure(rubern_data, doc_type_info)
        
        elapsed = time.time() - start_time
        logger.info(f"[Stage 8/14] COMPLETE - Canonical ID: {metadata.canonical_id}, "
                   f"Method: rubern_structure, Confidence: {metadata.confidence:.2f} ({elapsed:.2f}s)")
        
        return metadata
    
    
    def _extract_number_with_vlm_titulnik(self, pdf_path: str, doc_type_info: Dict) -> Optional[str]:
        """üöÄ VLM-OCR –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        
        if not self.vlm_available or not self.vlm_processor:
            return None
        
        try:
            logger.info(f"[Stage 8/14] [VLM] VLM-OCR –∞–Ω–∞–ª–∏–∑ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {pdf_path}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            from pdf2image import convert_from_path
            images = convert_from_path(pdf_path, first_page=1, last_page=1, dpi=300)
            
            if not images:
                logger.warning("[Stage 8/14] –ù–µ —É–¥–∞–ª–æ—Å—å –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
                return None
            
            titulnik_image = images[0]
            
            # VLM –∞–Ω–∞–ª–∏–∑ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            vlm_result = self._analyze_titulnik_with_vlm(titulnik_image, doc_type_info)
            
            if vlm_result:
                logger.info(f"[Stage 8/14] [SUCCESS] VLM-OCR –Ω–∞—à–µ–ª –Ω–æ–º–µ—Ä: {vlm_result}")
                return vlm_result
            
            return None
            
        except Exception as e:
            logger.error(f"[Stage 8/14] VLM-OCR –∞–Ω–∞–ª–∏–∑ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã failed: {e}")
            return None
    
    def _analyze_titulnik_with_vlm(self, image, doc_type_info: Dict) -> Optional[str]:
        """VLM –∞–Ω–∞–ª–∏–∑ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º BLIP –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            inputs = self.vlm_processor.blip_processor(image, return_tensors="pt").to(self.vlm_processor.device)
            
            with torch.no_grad():
                outputs = self.vlm_processor.blip_model.generate(
                    **inputs, 
                    max_length=100,
                    num_beams=4,
                    do_sample=False
                )
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            description = self.vlm_processor.blip_processor.decode(outputs[0], skip_special_tokens=True)
            logger.debug(f"[Stage 8/14] VLM –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {description}")
            
            # –ò—â–µ–º –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
            doc_type = doc_type_info.get('doc_type', '')
            patterns = self._get_document_patterns_for_type(doc_type)
            
            for pattern in patterns:
                import re
                matches = re.findall(pattern, description, re.IGNORECASE)
                if matches:
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä
                    return matches[0]
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ OCR —Ç–µ–∫—Å—Ç–µ
            import pytesseract
            ocr_text = pytesseract.image_to_string(image, lang='rus')
            logger.debug(f"[Stage 8/14] OCR —Ç–µ–∫—Å—Ç —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {ocr_text[:200]}...")
            
            for pattern in patterns:
                matches = re.findall(pattern, ocr_text, re.IGNORECASE)
                if matches:
                    return matches[0]
            
            return None
            
        except Exception as e:
            logger.error(f"[Stage 8/14] VLM –∞–Ω–∞–ª–∏–∑ —Ç–∏—Ç—É–ª—å–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã failed: {e}")
            return None
    
    def _get_document_patterns_for_type(self, doc_type: str) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç regex –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        patterns = {
            'gost': [
                r'–ì–û–°–¢\s+(\d+[\.\-]\d+[\.\-]\d+)',
                r'–ì–û–°–¢\s+(\d+[\.\-]\d+)',
                r'–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π\s+—Å—Ç–∞–Ω–¥–∞—Ä—Ç\s+(\d+[\.\-]\d+)',
            ],
            'sp': [
                r'–°–ü\s+(\d+[\.\-]\d+[\.\-]\d+)',
                r'–°–ü\s+(\d+[\.\-]\d+)',
                r'—Å–≤–æ–¥\s+–ø—Ä–∞–≤–∏–ª\s+(\d+[\.\-]\d+)',
            ],
            'snip': [
                r'–°–ù–∏–ü\s+(\d+[\.\-]\d+[\.\-]\d+)',
                r'–°–ù–∏–ü\s+(\d+[\.\-]\d+)',
                r'—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ\s+–Ω–æ—Ä–º—ã\s+(\d+[\.\-]\d+)',
            ],
            'sanpin': [
                r'–°–∞–Ω–ü–∏–ù\s+(\d+[\.\-]\d+[\.\-]\d+)',
                r'–°–∞–Ω–ü–∏–ù\s+(\d+[\.\-]\d+)',
            ],
            'vsn': [
                r'–í–°–ù\s+(\d+[\.\-]\d+)',
                r'–≤–µ–¥–æ–º—Å—Ç–≤–µ–Ω–Ω—ã–µ\s+—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ\s+–Ω–æ—Ä–º—ã\s+(\d+[\.\-]\d+)',
            ],
            'mds': [
                r'–ú–î–°\s+(\d+[\.\-]\d+)',
                r'–º–µ—Ç–æ–¥–∏—á–µ—Å–∫–∞—è\s+–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è\s+(\d+[\.\-]\d+)',
            ]
        }
        
        return patterns.get(doc_type, [
            r'(\d+[\.\-]\d+[\.\-]\d+)',
            r'(\d+[\.\-]\d+)',
        ])
    
    def _extract_title_based_metadata(self, content: str, doc_type_info: Dict, structural_data: Dict) -> DocumentMetadata:
        """üöÄ –°–¢–†–ê–¢–ï–ì–ò–Ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π/–∞–≤—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        logger.info(f"[Stage 8/14] üìö –°–¢–†–ê–¢–ï–ì–ò–Ø 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è {doc_type_info['doc_type']}")
        metadata = DocumentMetadata()
        
        # 2.1. –ü–†–ò–û–†–ò–¢–ï–¢: SBERT-–ª–æ–∫–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
        title_info = self._extract_title_and_author_with_sbert(content, structural_data)
        if title_info['title']:
            metadata.canonical_id = self._generate_title_based_id(title_info['title'], title_info.get('author', ''))
            metadata.title = title_info['title']
            metadata.source_author = title_info.get('author', '')  # –ò–°–ü–†–ê–í–õ–ï–ù–û: source_author
            metadata.extraction_method = "sbert_title_extraction"
            metadata.confidence = 0.9
            logger.info(f"[Stage 8/14] [SUCCESS] –ù–ê–ó–í–ê–ù–ò–ï –ò–ó SBERT: {title_info['title']}")
            return metadata
        
        # 2.2. FALLBACK: LLM-–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if hasattr(self, 'llm_client') and self.llm_client:
            try:
                llm_title_info = self._extract_title_with_llm_fallback(content, doc_type_info)
                if llm_title_info:
                    metadata.canonical_id = self._generate_title_based_id(llm_title_info['title'], llm_title_info.get('author', ''))
                    metadata.title = llm_title_info['title']
                    metadata.source_author = llm_title_info.get('author', '')  # –ò–°–ü–†–ê–í–õ–ï–ù–û: source_author
                    metadata.extraction_method = "llm_title_extraction"
                    metadata.confidence = 0.8
                    logger.info(f"[Stage 8/14] [AI] LLM-–ù–ê–ó–í–ê–ù–ò–ï: {llm_title_info['title']}")
                    return metadata
            except Exception as e:
                logger.error(f"[Stage 8/14] [ERROR] LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        
        # 2.3. FALLBACK: –≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        heuristic_title = self._extract_title_heuristic(content)
        if heuristic_title:
            metadata.canonical_id = self._generate_title_based_id(heuristic_title, '')
            metadata.title = heuristic_title
            metadata.extraction_method = "heuristic_title"
            metadata.confidence = 0.6
            logger.warning(f"[Stage 8/14] [WARN] –≠–í–†–ò–°–¢–ò–ß–ï–°–ö–û–ï –ù–ê–ó–í–ê–ù–ò–ï: {heuristic_title}")
            return metadata
        
        # 2.4. –ö–†–ê–ô–ù–ò–ô FALLBACK: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        emergency_title = self._generate_emergency_title(content, doc_type_info)
        metadata.canonical_id = emergency_title
        metadata.title = emergency_title
        metadata.extraction_method = "emergency_title"
        metadata.confidence = 0.4
        logger.error(f"[Stage 8/14] –≠–ö–°–¢–†–ï–ù–ù–û–ï –ù–ê–ó–í–ê–ù–ò–ï: {emergency_title}")
        
        return metadata
    
    def _extract_hybrid_metadata(self, content: str, doc_type_info: Dict, structural_data: Dict) -> DocumentMetadata:
        """üöÄ –°–¢–†–ê–¢–ï–ì–ò–Ø 3: –ì–∏–±—Ä–∏–¥–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–ª—è —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤"""
        
        logger.info(f"[Stage 8/14] [STRATEGY] –°–¢–†–ê–¢–ï–ì–ò–Ø 3: –ì–∏–±—Ä–∏–¥–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–ª—è {doc_type_info['doc_type']}")
        
        # –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞
        number_metadata = self._extract_number_based_metadata(content, doc_type_info, structural_data)
        if number_metadata.canonical_id and number_metadata.confidence > 0.7:
            return number_metadata
        
        # –ï—Å–ª–∏ –Ω–æ–º–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        title_metadata = self._extract_title_based_metadata(content, doc_type_info, structural_data)
        if title_metadata.canonical_id and title_metadata.confidence > 0.7:
            return title_metadata
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if number_metadata.confidence > title_metadata.confidence:
            return number_metadata
        else:
            return title_metadata
    
    def _extract_title_and_author_with_sbert(self, content: str, structural_data: Dict) -> Dict[str, str]:
        """üöÄ SBERT-–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∞–≤—Ç–æ—Ä–∞ –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        if not hasattr(self, 'sbert_model') or not self.sbert_model:
            return {'title': '', 'author': ''}
        
        try:
            # –ò—â–µ–º –±–ª–æ–∫–∏ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ —á–µ—Ä–µ–∑ SBERT
            title_blocks = self._find_title_blocks_with_sbert(content, structural_data)
            
            for block in title_blocks:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
                title = self._extract_title_from_block(block)
                if title:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–≤—Ç–æ—Ä–∞
                    author = self._extract_author_from_block(block)
                    return {'title': title, 'author': author}
            
            return {'title': '', 'author': ''}
            
        except Exception as e:
            logger.warning(f"SBERT title extraction failed: {e}")
            return {'title': '', 'author': ''}
    
    def _find_title_blocks_with_sbert(self, content: str, structural_data: Dict) -> List[str]:
        """–ü–æ–∏—Å–∫ –±–ª–æ–∫–æ–≤ —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ —á–µ—Ä–µ–∑ SBERT"""
        
        title_blocks = []
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
        if structural_data and 'sections' in structural_data:
            for section in structural_data['sections']:
                section_text = section.get('content', '')
                if self._is_title_section(section_text):
                    title_blocks.append(section_text)
        
        # –ï—Å–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º SBERT
        if not title_blocks and hasattr(self, 'sbert_model'):
            try:
                blocks = self._split_content_into_blocks(content)
                title_query = "–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∏—Ç—É–ª—å–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"
                
                for block in blocks:
                    if len(block) > 50:
                        similarity = self._calculate_semantic_similarity(block, title_query)
                        if similarity > 0.6:
                            title_blocks.append(block)
            except Exception as e:
                logger.warning(f"SBERT title block detection failed: {e}")
        
        return title_blocks
    
    def _is_title_section(self, section_text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–µ–∫—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–º"""
        title_keywords = [
            '–Ω–∞–∑–≤–∞–Ω–∏–µ', '–∑–∞–≥–æ–ª–æ–≤–æ–∫', '—Ç–∏—Ç—É–ª', '–∞–≤—Ç–æ—Ä', '—Å–æ—Å—Ç–∞–≤–∏—Ç–µ–ª—å',
            '—É—á–µ–±–Ω–∏–∫', '–ø–æ—Å–æ–±–∏–µ', '—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ', '–ª–µ–∫—Ü–∏—è', '–∫—É—Ä—Å'
        ]
        
        text_lower = section_text.lower()
        keyword_count = sum(1 for keyword in title_keywords if keyword in text_lower)
        
        return keyword_count >= 2 or len(section_text) < 500  # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–µ–∫—Ü–∏–∏ —á–∞—Å—Ç–æ —Å–æ–¥–µ—Ä–∂–∞—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏
    
    def _extract_title_from_block(self, block: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –±–ª–æ–∫–∞"""
        
        lines = block.split('\n')
        for line in lines:
            line = line.strip()
            if len(line) > 10 and len(line) < 200:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å–ª—É–∂–µ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                if not any(skip_word in line.lower() for skip_word in ['—Å—Ç—Ä–∞–Ω–∏—Ü–∞', '—Å—Ç—Ä.', '–≥–ª–∞–≤–∞', '—Ä–∞–∑–¥–µ–ª']):
                    return line
        
        return ''
    
    def _extract_author_from_block(self, block: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä–∞ –∏–∑ –±–ª–æ–∫–∞"""
        
        author_patterns = [
            r'–∞–≤—Ç–æ—Ä[:\s]+([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+)',
            r'—Å–æ—Å—Ç–∞–≤–∏—Ç–µ–ª—å[:\s]+([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+)',
            r'–ø–æ–¥\s+—Ä–µ–¥\.\s+([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+)'
        ]
        
        for pattern in author_patterns:
            match = re.search(pattern, block, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return ''
    
    def _generate_title_based_id(self, title: str, author: str = '') -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ ID –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∞–≤—Ç–æ—Ä–∞"""
        
        import hashlib
        
        # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ
        clean_title = re.sub(r'[^\w\s]', '', title.lower())
        clean_title = re.sub(r'\s+', '_', clean_title)
        clean_title = clean_title[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–≤—Ç–æ—Ä–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        if author:
            clean_author = re.sub(r'[^\w\s]', '', author.lower())
            clean_author = re.sub(r'\s+', '_', clean_author)
            clean_author = clean_author[:20]
            clean_title = f"{clean_title}_by_{clean_author}"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ö–µ—à –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        content_hash = hashlib.md5(title.encode()).hexdigest()[:6]
        
        return f"{clean_title}_{content_hash}"
    
    def _extract_title_with_llm_fallback(self, content: str, doc_type_info: Dict) -> Dict[str, str]:
        """LLM-–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∏ –∞–≤—Ç–æ—Ä–∞ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ"""
        
        if not hasattr(self, 'llm_client') or not self.llm_client:
            return {'title': '', 'author': ''}
        
        try:
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            content_preview = content[:2000]
            
            prompt = f"""
            –ò–∑–≤–ª–µ–∫–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ –∞–≤—Ç–æ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞: '{content_preview}'
            
            –û—Ç–≤–µ—Ç—å –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
            {{
                "title": "–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "author": "–ê–≤—Ç–æ—Ä (–µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω)"
            }}
            
            –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–µ—Ä–Ω–∏ "title": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ"
            """
            
            response = self.llm_client.generate_text(
                prompt=prompt,
                temperature=0.1,
                max_tokens=100,
                model="qwen/qwen3-coder-30b"
            ).strip()
            
            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            import json
            try:
                result = json.loads(response)
                return {
                    'title': result.get('title', ''),
                    'author': result.get('author', '')
                }
            except json.JSONDecodeError:
                # –ï—Å–ª–∏ JSON –Ω–µ –ø–∞—Ä—Å–∏—Ç—Å—è, –∏–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞
                title_match = re.search(r'"title":\s*"([^"]+)"', response)
                author_match = re.search(r'"author":\s*"([^"]+)"', response)
                
                return {
                    'title': title_match.group(1) if title_match else '',
                    'author': author_match.group(1) if author_match else ''
                }
                
        except Exception as e:
            logger.error(f"LLM title extraction failed: {e}")
            return {'title': '', 'author': ''}
    
    def _extract_title_heuristic(self, content: str) -> str:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        
        lines = content.split('\n')[:20]  # –ü–µ—Ä–≤—ã–µ 20 —Å—Ç—Ä–æ–∫
        
        for line in lines:
            line = line.strip()
            if len(line) > 15 and len(line) < 150:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å–ª—É–∂–µ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
                if not any(skip_word in line.lower() for skip_word in [
                    '—Å—Ç—Ä–∞–Ω–∏—Ü–∞', '—Å—Ç—Ä.', '–≥–ª–∞–≤–∞', '—Ä–∞–∑–¥–µ–ª', '—Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ', '–æ–≥–ª–∞–≤–ª–µ–Ω–∏–µ'
                ]):
                    return line
        
        return ''
    
    def _generate_emergency_title(self, content: str, doc_type_info: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è"""
        
        from datetime import datetime
        import hashlib
        
        doc_type = doc_type_info.get('doc_type', 'unknown')
        timestamp = datetime.now().strftime('%Y%m%d')
        content_hash = hashlib.md5(content[:500].encode()).hexdigest()[:6]
        
        return f"EMERGENCY_{doc_type}_{timestamp}_{content_hash}"
    
    def _stage9_quality_control(self, content: str, doc_type_info: Dict, 
                               structural_data: Dict, sbert_data: Dict, 
                               metadata: DocumentMetadata) -> Dict[str, Any]:
        """üöÄ –†–ê–°–®–ò–†–ï–ù–ù–´–ô STAGE 9: –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π Quality Control –ø–æ ID-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏"""
        
        logger.info(f"[Stage 9/14] ADAPTIVE QUALITY CONTROL - ID Type: {doc_type_info.get('id_type', 'UNKNOWN')}")
        start_time = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
        id_type = doc_type_info.get('id_type', 'TITLE')
        doc_type = doc_type_info.get('doc_type', 'unknown')
        
        issues = []
        recommendations = []
        quality_score = 100.0
        
        # ============================================================
        # –°–¢–†–ê–¢–ï–ì–ò–Ø 1: –î–û–ö–£–ú–ï–ù–¢–´ –° –ù–û–ú–ï–†–û–ú (–°–ü, –ì–û–°–¢, –°–ù–∏–ü...)
        # ============================================================
        if id_type == 'NUMBER':
            logger.info(f"[Stage 9/14] [FOUND] –°–¢–†–ê–¢–ï–ì–ò–Ø 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({doc_type})")
            
            # 1.1. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π ID
            if not metadata.canonical_id or len(metadata.canonical_id) < 5:
                issues.append(f"CRITICAL: Missing or too short canonical ID for normative document")
                quality_score -= 40
                
                # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª–æ "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –Ω–æ—Ä–º–∞"
                metadata.quality_status = 'WARNING: UNKNOWN_CANONICAL_ID (Normative)'
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –¥–ª—è –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞
                from datetime import datetime
                import hashlib
                file_hash = hashlib.md5(content[:500].encode()).hexdigest()[:6]
                temp_id = f"UNKNOWN_{doc_type.upper()}_{datetime.now().strftime('%Y%m%d')}_{file_hash}"
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ID
                metadata.title = temp_id 
                metadata.canonical_id = temp_id
                logger.warning(f"EMERGENCY NORMATIVE ID: {temp_id}")
            
            # 1.2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            elif metadata.canonical_id:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–æ–º–µ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç —Ü–∏—Ñ—Ä—ã
                if not any(c.isdigit() for c in metadata.canonical_id):
                    issues.append(f"WARNING: Canonical ID lacks numbers: {metadata.canonical_id}")
                    quality_score -= 15
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–ª–æ—Ö–∏–µ –∏–º–µ–Ω–∞
                if any(bad_name in metadata.canonical_id.lower() for bad_name in ['–¥–æ–∫—É–º–µ–Ω—Ç', 'document', '—Ñ–∞–π–ª', 'file']):
                    issues.append(f"ERROR: Poor canonical ID: {metadata.canonical_id}")
                    quality_score -= 30
                    recommendations.append("Improve metadata extraction logic")
        
        # ============================================================
        # –°–¢–†–ê–¢–ï–ì–ò–Ø 2: –î–û–ö–£–ú–ï–ù–¢–´ –ë–ï–ó –ù–û–ú–ï–†–ê (–ü–ü–†, –ö–Ω–∏–≥–∏, –ê–ª—å–±–æ–º—ã...)
        # ============================================================
        elif id_type == 'TITLE':
            logger.info(f"[Stage 9/14] üìö –°–¢–†–ê–¢–ï–ì–ò–Ø 2: –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö/–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({doc_type})")
            
            # 2.1. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ù–∞–ª–∏—á–∏–µ –∏ –¥–ª–∏–Ω–∞ –ó–∞–≥–æ–ª–æ–≤–∫–∞
            if not metadata.title or len(metadata.title) < 20:
                issues.append(f"CRITICAL: Title too short or missing for {doc_type} document")
                quality_score -= 35
                
                metadata.quality_status = 'ERROR: TITLE_TOO_SHORT/MISSING (Non-Normative)'
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –¥–ª—è –∫–∞—Ä–∞–Ω—Ç–∏–Ω–∞
                from datetime import datetime
                import hashlib
                file_hash = hashlib.md5(content[:500].encode()).hexdigest()[:6]
                temp_id = f"ERROR_{doc_type.upper()}_TITLE_{file_hash}"
                metadata.canonical_id = temp_id
                metadata.title = temp_id
                logger.error(f"[ALERT] EMERGENCY TITLE ID: {temp_id}")
            
            # 2.2. –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ê–≤—Ç–æ—Ä–∞/–î–∞—Ç—ã (–ù–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç, –Ω–æ –ø–æ–º–µ—á–∞–µ—Ç)
            # Safe access to source_author with default
            source_author = getattr(metadata, 'source_author', None)
            if not source_author:
                issues.append("WARNING: Author information missing")
                quality_score -= 10
                if metadata.quality_status:
                    metadata.quality_status += '| WARNING: AUTHOR_MISSING'
                else:
                    metadata.quality_status = 'WARNING: AUTHOR_MISSING'
            
            # Safe access to publication_date with default
            publication_date = getattr(metadata, 'publication_date', None)
            if not publication_date:
                issues.append("WARNING: Publication date missing")
                quality_score -= 5
                if metadata.quality_status:
                    metadata.quality_status += '| WARNING: DATE_MISSING'
                else:
                    metadata.quality_status = 'WARNING: DATE_MISSING'
        
        # ============================================================
        # –û–ë–©–ò–ï –ü–†–û–í–ï–†–ö–ò (–¥–ª—è –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
        # ============================================================
        
        # 3.1. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        if len(content) < 500:
            issues.append("ERROR: Content too short")
            quality_score -= 25
            metadata.quality_status = 'ERROR: CONTENT_TOO_SHORT'
        
        # 3.2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Ç–∏–ø–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        if doc_type_info['confidence'] < 0.7:
            issues.append(f"Low document type confidence: {doc_type_info['confidence']:.2f}")
            quality_score -= 15
        
        # 3.3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        sections_count = len(structural_data.get('sections', []))
        if sections_count < 2:
            issues.append(f"Too few sections found: {sections_count}")
            quality_score -= 20
            recommendations.append("Consider manual section markup")
        
        # 3.4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        works_count = len(sbert_data.get('works', []))
        if works_count < 3:
            issues.append(f"Too few works extracted: {works_count}")
            quality_score -= 10
            recommendations.append("Review seed extraction patterns")
        
        deps_count = len(sbert_data.get('dependencies', []))
        if deps_count == 0 and works_count > 1:
            issues.append("No work dependencies found")
            quality_score -= 5
            recommendations.append("Check dependency extraction logic")
        
        # 3.5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
        vector_quality = self._check_vector_quality_safe(sbert_data)
        if vector_quality < 0.5:
            issues.append(f"Low vector quality: {vector_quality:.2f}")
            quality_score -= 20
            recommendations.append("Check SBERT model and embeddings")
        
        # ============================================================
        # –§–ò–ù–ê–õ–¨–ù–ê–Ø –ì–ê–†–ê–ù–¢–ò–Ø: –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ô CANONICAL_ID
        # ============================================================
        
        # [ALERT] –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —É –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –µ—Å—Ç—å canonical_id
        if not metadata.canonical_id or len(metadata.canonical_id) < 3:
            from datetime import datetime
            import hashlib
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π ID
            file_hash = hashlib.md5(content[:500].encode()).hexdigest()[:6]
            emergency_id = f"EMERGENCY_{doc_type.upper()}_{datetime.now().strftime('%Y%m%d')}_{file_hash}"
            
            metadata.canonical_id = emergency_id
            metadata.title = emergency_id
            metadata.quality_status = 'CRITICAL: EMERGENCY_ID_GENERATED'
            
            issues.append("CRITICAL: Emergency canonical ID generated")
            quality_score -= 50
            
            logger.error(f"[ALERT] FINAL EMERGENCY ID: {emergency_id}")
        
        # ============================================================
        # –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê
        # ============================================================
        
        quality_score = max(quality_score, 0.0)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å –∫–∞—á–µ—Å—Ç–≤–∞
        if quality_score >= 80:
            final_status = "High Quality"
        elif quality_score >= 60:
            final_status = "Medium Quality"
        elif quality_score >= 40:
            final_status = "Low Quality"
        else:
            final_status = "Poor Quality"
        
        if metadata.quality_status:
            metadata.quality_status = f"{final_status} | {metadata.quality_status}"
        else:
            metadata.quality_status = final_status
        
        result = {
            'quality_score': quality_score,
            'issues': issues,
            'recommendations': recommendations,
            'id_type': id_type,
            'final_status': final_status,
            'stats': {
                'doc_type_confidence': doc_type_info['confidence'],
                'sections_count': sections_count,
                'works_count': works_count,
                'dependencies_count': deps_count,
                'vector_quality': vector_quality,
                'canonical_id_length': len(metadata.canonical_id) if metadata.canonical_id else 0
            }
        }
        
        elapsed = time.time() - start_time
        logger.info(f"[Stage 9/14] COMPLETE - Quality Score: {quality_score:.1f}, "
                   f"ID Type: {id_type}, Issues: {len(issues)} ({elapsed:.2f}s)")
        
        return result
    
    def _stage10_type_specific_processing(self, content: str, doc_type_info: Dict, 
                                         structural_data: Dict, sbert_data: Dict) -> Dict[str, Any]:
        """STAGE 10: Type-specific Processing"""
        
        logger.info(f"[Stage 10/14] TYPE-SPECIFIC PROCESSING - Type: {doc_type_info['doc_type']}")
        start_time = time.time()
        
        doc_type = doc_type_info['doc_type']
        result = {}
        
        # üöÄ –£–°–ò–õ–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ù–¢–î - –ü–£–ù–ö–¢–´ –ù–û–†–ú –ò –í–ê–õ–ò–î–ê–¶–ò–Ø
        if doc_type in ['gost', 'sp', 'snip', 'iso', 'sanpin', 'vsn', 'mds', 'pnst', 'sto', 'tu']:
            # –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
            result['processing_type'] = 'normative'
            result['priority'] = 'high'
            result['vlm_enhanced'] = True
            
            # üöÄ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ü–£–ù–ö–¢–û–í –ù–û–†–ú –ò–ó –°–¢–†–£–ö–¢–£–†–´ RUBERN
            norm_elements = self._extract_norm_elements_from_rubern(sbert_data, doc_type)
            if norm_elements:
                result['norm_elements'] = norm_elements
                result['norm_elements_count'] = len(norm_elements)
                logger.info(f"‚úÖ [Stage 10/14] –ù–¢–î: {len(norm_elements)} –ø—É–Ω–∫—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
            
            # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∏–∑–æ–ª—è—Ü–∏—è –ø—Ä–∏–∫–∞–∑–∞ –¥–ª—è –°–ü –∏ –ì–û–°–¢
            if doc_type in ['sp', 'gost']:
                order_isolation = self._isolate_order_content(content, doc_type)
                if order_isolation:
                    result['order_isolation'] = order_isolation
                    result['content_trimmed'] = True
                    logger.info(f"[Stage 10/14] [SUCCESS] –ü–†–ò–ö–ê–ó –ò–ó–û–õ–ò–†–û–í–ê–ù: {order_isolation.get('order_length', 0)} —Å–∏–º–≤–æ–ª–æ–≤")
        elif doc_type in ['project', 'design', 'plan', 'estimate']:
            # üöÄ –í–ê–õ–ò–î–ê–¶–ò–Ø –ü–†–û–ï–ö–¢–û–í - –ü–û–ò–°–ö –°–°–´–õ–û–ö –ù–ê –ù–¢–î
            result['processing_type'] = 'project'
            result['priority'] = 'high'
            result['vlm_enhanced'] = True
            
            # üöÄ –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–°–´–õ–û–ö –ù–ê –ù–¢–î –ò–ó –°–¢–†–£–ö–¢–£–†–´ RUBERN
            norm_references = self._extract_norm_references_from_rubern(sbert_data, content)
            if norm_references:
                result['norm_references'] = norm_references
                result['norm_references_count'] = len(norm_references)
                logger.info(f"‚úÖ [Stage 10/14] –ü—Ä–æ–µ–∫—Ç: {len(norm_references)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –ù–¢–î")
                
                # üöÄ –í–ê–õ–ò–î–ê–¶–ò–Ø –°–û–û–¢–í–ï–¢–°–¢–í–ò–Ø - –ü–†–û–í–ï–†–ö–ê –ù–ê–õ–ò–ß–ò–Ø –ù–û–†–ú –í –ë–î
                validated_refs = self._validate_norm_references(norm_references)
                if validated_refs:
                    result['validated_norms'] = validated_refs
                    result['compliance_score'] = len(validated_refs) / len(norm_references)
                    logger.info(f"‚úÖ [Stage 10/14] –í–∞–ª–∏–¥–∞—Ü–∏—è: {len(validated_refs)}/{len(norm_references)} –Ω–æ—Ä–º –Ω–∞–π–¥–µ–Ω–æ –≤ –ë–î")
        
        elif doc_type == 'drawing':
            # üöÄ VLM-–ê–ù–ê–õ–ò–ó –ß–ï–†–¢–ï–ñ–ï–ô - –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–ü–ï–¶–ò–§–ò–ö–ê–¶–ò–ô
            result['processing_type'] = 'drawing'
            result['priority'] = 'high'
            result['vlm_enhanced'] = True
            
            # üöÄ –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô VLM-–ê–ù–ê–õ–ò–ó –ß–ï–†–¢–ï–ñ–ï–ô
            if self.vlm_available and self._current_file_path.endswith('.pdf'):
                drawing_analysis = self._extract_specifications_from_drawing_vlm(self._current_file_path)
                if drawing_analysis:
                    result['specifications'] = drawing_analysis.get('specifications', [])
                    result['drawing_number'] = drawing_analysis.get('drawing_number', '')
                    result['drawing_stamps'] = drawing_analysis.get('stamps', {})
                    result['equipment_notations'] = drawing_analysis.get('equipment_notations', [])
                    
                    logger.info(f"‚úÖ [Stage 10/14] –ß–ï–†–¢–ï–ñ: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(result['specifications'])} –ø–æ–∑–∏—Ü–∏–π —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏")
                    logger.info(f"‚úÖ [Stage 10/14] –ß–ï–†–¢–ï–ñ: –Ω–æ–º–µ—Ä {result['drawing_number']}, —à—Ç–∞–º–ø—ã: {len(result['drawing_stamps'])}")
        
        elif doc_type == 'estimate':
            # üöÄ –ü–ê–†–°–ò–ù–ì EXCEL-–°–ú–ï–¢ - –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –†–ê–°–¶–ï–ù–û–ö –ò –û–ë–™–Å–ú–û–í
            result['processing_type'] = 'estimate'
            result['priority'] = 'high'
            result['vlm_enhanced'] = False  # –ù–µ –Ω—É–∂–µ–Ω VLM –¥–ª—è Excel
            
            # üöÄ –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –°–ú–ï–¢
            if self._current_file_path.endswith(('.xlsx', '.xls')):
                estimate_analysis = self._extract_works_from_estimate_excel(self._current_file_path)
                if estimate_analysis:
                    result['estimate_items'] = estimate_analysis.get('items', [])
                    result['estimate_number'] = estimate_analysis.get('estimate_number', '')
                    result['total_cost'] = estimate_analysis.get('total_cost', 0.0)
                    result['items_count'] = len(result['estimate_items'])
                    
                    logger.info(f"‚úÖ [Stage 10/14] –°–ú–ï–¢–ê: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(result['estimate_items'])} —Ä–∞—Å—Ü–µ–Ω–æ–∫")
                    logger.info(f"‚úÖ [Stage 10/14] –°–ú–ï–¢–ê: –Ω–æ–º–µ—Ä {result['estimate_number']}, –æ–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: {result['total_cost']}")
        
        elif doc_type == 'ppr':
            # üöÄ –ü–ê–†–°–ò–ù–ì –ü–ü–† - –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –≠–¢–ê–ü–û–í –ò –¢–ï–•–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–• –ö–ê–†–¢
            result['processing_type'] = 'ppr'
            result['priority'] = 'high'
            result['vlm_enhanced'] = False  # –ù–µ –Ω—É–∂–µ–Ω VLM –¥–ª—è DOCX
            
            # üöÄ –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì –ü–ü–†
            if self._current_file_path.endswith(('.docx', '.doc')):
                ppr_analysis = self._extract_stages_from_ppr_docx(content)
                if ppr_analysis:
                    result['ppr_stages'] = ppr_analysis.get('stages', [])
                    result['technology_cards'] = ppr_analysis.get('technology_cards', [])
                    result['stages_count'] = len(result['ppr_stages'])
                    result['cards_count'] = len(result['technology_cards'])
                    
                    logger.info(f"‚úÖ [Stage 10/14] –ü–ü–†: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(result['ppr_stages'])} —ç—Ç–∞–ø–æ–≤")
                    logger.info(f"‚úÖ [Stage 10/14] –ü–ü–†: –Ω–∞–π–¥–µ–Ω–æ {len(result['technology_cards'])} —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ä—Ç")
        
        # üöÄ –°–ü–ï–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –ù–û–í–´–• –§–û–†–ú–ê–¢–û–í
        elif doc_type == 'drawing_cad':
            # AutoCAD —á–µ—Ä—Ç–µ–∂–∏
            result['processing_type'] = 'cad_drawing'
            result['priority'] = 'high'
            result['vlm_enhanced'] = True
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è –∏ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑ CAD
            cad_data = self._extract_cad_specifications(content)
            if cad_data:
                result['cad_specifications'] = cad_data.get('specifications', [])
                result['cad_blocks'] = cad_data.get('blocks', [])
                result['cad_layers'] = cad_data.get('layers', [])
                logger.info(f"üèóÔ∏è [Stage 10/14] CAD: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(result['cad_specifications'])} —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–π")
        
        elif doc_type == 'bim':
            # BIM –º–æ–¥–µ–ª–∏
            result['processing_type'] = 'bim_model'
            result['priority'] = 'high'
            result['vlm_enhanced'] = False
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º BIM –¥–∞–Ω–Ω—ã–µ
            bim_data = self._extract_bim_data(content)
            if bim_data:
                result['bim_objects'] = bim_data.get('objects', [])
                result['bim_properties'] = bim_data.get('properties', [])
                result['bim_relationships'] = bim_data.get('relationships', [])
                logger.info(f"üèóÔ∏è [Stage 10/14] BIM-–¥–∞–Ω–Ω—ã–µ –∏–∑–≤–ª–µ—á–µ–Ω—ã: {len(result['bim_objects'])} –æ–±—ä–µ–∫—Ç–æ–≤")
        
        elif doc_type == '1c_exchange':
            # –î–∞–Ω–Ω—ã–µ –æ–±–º–µ–Ω–∞ —Å 1–°
            result['processing_type'] = '1c_data'
            result['priority'] = 'medium'
            result['vlm_enhanced'] = False
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ 1–°
            exchange_data = self._extract_1c_data(content)
            if exchange_data:
                result['1c_objects'] = exchange_data.get('objects', [])
                result['1c_transactions'] = exchange_data.get('transactions', [])
                result['1c_metadata'] = exchange_data.get('metadata', {})
                logger.info(f"üìä [Stage 10/14] 1–°: –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(result['1c_objects'])} –æ–±—ä–µ–∫—Ç–æ–≤")
        
        elif doc_type == 'scan_image':
            # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            result['processing_type'] = 'scan_analysis'
            result['priority'] = 'medium'
            result['vlm_enhanced'] = True
            
            # VLM –∞–Ω–∞–ª–∏–∑ —Å–∫–∞–Ω–æ–≤
            if self.vlm_available:
                scan_analysis = self._analyze_scan_with_vlm(self._current_file_path)
                if scan_analysis:
                    result['scan_text'] = scan_analysis.get('text', '')
                    result['scan_tables'] = scan_analysis.get('tables', [])
                    result['scan_quality'] = scan_analysis.get('quality', 0.0)
                    logger.info(f"üñºÔ∏è [Stage 10/14] –°–∫–∞–Ω: –∫–∞—á–µ—Å—Ç–≤–æ {result['scan_quality']:.2f}, —Ç–∞–±–ª–∏—Ü: {len(result['scan_tables'])}")
        
        elif doc_type == 'archive':
            # –ê—Ä—Ö–∏–≤—ã –ø—Ä–æ–µ–∫—Ç–æ–≤
            result['processing_type'] = 'archive_analysis'
            result['priority'] = 'low'
            result['vlm_enhanced'] = False
            
            # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –∞—Ä—Ö–∏–≤–∞
            archive_analysis = self._analyze_archive_content(content)
            if archive_analysis:
                result['archive_files'] = archive_analysis.get('files', [])
                result['archive_structure'] = archive_analysis.get('structure', {})
                result['archive_size'] = archive_analysis.get('total_size', 0)
                logger.info(f"üì¶ [Stage 10/14] –ê—Ä—Ö–∏–≤: {len(result['archive_files'])} —Ñ–∞–π–ª–æ–≤, {result['archive_size']} –±–∞–π—Ç")
        
        # üöÄ –ì–ï–ù–ï–†–ê–¶–ò–Ø Q&A-–ü–ê–† –î–õ–Ø FINE-TUNING
        if doc_type in ['sp', 'gost', 'snip', 'iso', 'estimate', 'ppr', 'drawing', 'drawing_cad', 'bim']:
            # –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π metadata –¥–ª—è Q&A –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            metadata_dict = {}
            qa_pairs = self._generate_qa_pairs(sbert_data, metadata_dict, doc_type_info)
            if qa_pairs:
                result['qa_pairs'] = qa_pairs
                result['qa_count'] = len(qa_pairs)
                logger.info(f"‚úÖ [Stage 10/14] Q&A: —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(qa_pairs)} –ø–∞—Ä –¥–ª—è fine-tuning")
        
        elif doc_type in ['book', 'manual', 'lecture', 'journal']:
            # –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å VLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ñ–æ—Ä–º—É–ª
            result['processing_type'] = 'educational'
            result['priority'] = 'medium'
            result['vlm_enhanced'] = True
            
            # VLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª –¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
            if self.vlm_available and self._current_file_path.endswith('.pdf'):
                vlm_formulas = self._extract_formulas_with_vlm(self._current_file_path, doc_type)
                if vlm_formulas:
                    result['formulas'] = vlm_formulas
                    logger.info(f"[Stage 10/14] [SUCCESS] VLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª: {len(vlm_formulas)} —Ñ–æ—Ä–º—É–ª –Ω–∞–π–¥–µ–Ω–æ")
                    
        elif doc_type in ['ppr', 'ttk', 'form', 'album']:
            # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å VLM –∞–Ω–∞–ª–∏–∑–æ–º —à—Ç–∞–º–ø–æ–≤
            result['processing_type'] = 'organizational'
            result['priority'] = 'medium'
            result['vlm_enhanced'] = True
            
            # VLM –∞–Ω–∞–ª–∏–∑ —à—Ç–∞–º–ø–æ–≤ –¥–ª—è –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
            if self.vlm_available and self._current_file_path.endswith('.pdf'):
                vlm_stamp_analysis = self._analyze_stamps_with_vlm(self._current_file_path, doc_type)
                if vlm_stamp_analysis:
                    result['stamp_data'] = vlm_stamp_analysis
                    logger.info(f"[Stage 10/14] [SUCCESS] VLM –∞–Ω–∞–ª–∏–∑ —à—Ç–∞–º–ø–æ–≤: {len(vlm_stamp_analysis.get('stamps', []))} —à—Ç–∞–º–ø–æ–≤ –Ω–∞–π–¥–µ–Ω–æ")
        else:
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã
            result['processing_type'] = 'other'
            result['priority'] = 'low'
            result['vlm_enhanced'] = False
        
        elapsed = time.time() - start_time
        logger.info(f"[Stage 10/14] COMPLETE - Type: {doc_type}, "
                   f"Processing: {result['processing_type']} ({elapsed:.2f}s)")
        
        return result
    
    def _analyze_stamps_with_vlm(self, pdf_path: str, doc_type: str) -> Optional[Dict]:
        """VLM –∞–Ω–∞–ª–∏–∑ —à—Ç–∞–º–ø–æ–≤ –≤ –ø—Ä–æ–µ–∫—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏"""
        
        if not self.vlm_available or not self.vlm_processor:
            return None
        
        try:
            logger.info(f"[Stage 10/14] [FOUND] VLM –∞–Ω–∞–ª–∏–∑ —à—Ç–∞–º–ø–æ–≤: {pdf_path}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            from pdf2image import convert_from_path
            images = convert_from_path(pdf_path, dpi=300)
            
            stamps = []
            for page_num, image in enumerate(images):
                # –ò—â–µ–º —à—Ç–∞–º–ø—ã –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                page_stamps = self._detect_stamps_on_page(image, page_num, doc_type)
                stamps.extend(page_stamps)
            
            if stamps:
                return {
                    'stamps': stamps,
                    'total_stamps': len(stamps),
                    'analysis_method': 'VLM_STAMP_DETECTION'
                }
            
            return None
            
        except Exception as e:
            logger.error(f"[Stage 10/14] VLM –∞–Ω–∞–ª–∏–∑ —à—Ç–∞–º–ø–æ–≤ failed: {e}")
            return None
    
    def _detect_stamps_on_page(self, image, page_num: int, doc_type: str) -> List[Dict]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —à—Ç–∞–º–ø–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        
        try:
            import cv2
            import numpy as np
            import pytesseract
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PIL –≤ OpenCV
            cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # –ò—â–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ (—à—Ç–∞–º–ø—ã)
            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            stamps = []
            for contour in contours:
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏ —Ñ–æ—Ä–º–µ
                area = cv2.contourArea(contour)
                if area > 1000:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –ø–ª–æ—â–∞–¥—å —à—Ç–∞–º–ø–∞
                    x, y, w, h = cv2.boundingRect(contour)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –æ–±–ª–∞—Å—Ç–∏ —à—Ç–∞–º–ø–∞
                    roi = image.crop((x, y, x + w, y + h))
                    stamp_text = pytesseract.image_to_string(roi, lang='rus')
                    
                    if self._is_stamp_content(stamp_text, doc_type):
                        stamps.append({
                            'page': page_num,
                            'position': (x, y, w, h),
                            'text': stamp_text.strip(),
                            'stamp_type': self._classify_stamp_type(stamp_text),
                            'confidence': 0.8
                        })
            
            return stamps
            
        except Exception as e:
            logger.error(f"[Stage 10/14] –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —à—Ç–∞–º–ø–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ failed: {e}")
            return []
    
    def _is_stamp_content(self, text: str, doc_type: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —à—Ç–∞–º–ø–æ–º"""
        
        stamp_keywords = [
            '–ª–∏—Å—Ç', '—á–µ—Ä—Ç–µ–∂', '–ø—Ä–æ–µ–∫—Ç', '–æ–±—ä–µ–∫—Ç', '—Å—Ç–∞—Ç—É—Å',
            '–ø–æ–¥–ø–∏—Å—å', '–¥–∞—Ç–∞', '–Ω–æ–º–µ—Ä', '–º–∞—Å—à—Ç–∞–±', '–º–∞—Ç–µ—Ä–∏–∞–ª'
        ]
        
        text_lower = text.lower()
        return any(keyword in text_lower for keyword in stamp_keywords)
    
    def _classify_stamp_type(self, text: str) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —à—Ç–∞–º–ø–∞"""
        
        if '–ª–∏—Å—Ç' in text.lower() or '—á–µ—Ä—Ç–µ–∂' in text.lower():
            return 'DRAWING_STAMP'
        elif '–ø—Ä–æ–µ–∫—Ç' in text.lower() or '–æ–±—ä–µ–∫—Ç' in text.lower():
            return 'PROJECT_STAMP'
        elif '–ø–æ–¥–ø–∏—Å—å' in text.lower() or '–¥–∞—Ç–∞' in text.lower():
            return 'SIGNATURE_STAMP'
        else:
            return 'GENERAL_STAMP'
    
    def _extract_formulas_with_vlm(self, pdf_path: str, doc_type: str) -> Optional[List[Dict]]:
        """VLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª –∏–∑ –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤"""
        
        if not self.vlm_available or not self.vlm_processor:
            return None
        
        try:
            logger.info(f"[Stage 10/14] [FOUND] VLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª: {pdf_path}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            from pdf2image import convert_from_path
            images = convert_from_path(pdf_path, dpi=300)
            
            formulas = []
            for page_num, image in enumerate(images):
                # –ò—â–µ–º —Ñ–æ—Ä–º—É–ª—ã –Ω–∞ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                page_formulas = self._detect_formulas_on_page(image, page_num, doc_type)
                formulas.extend(page_formulas)
            
            if formulas:
                logger.info(f"[Stage 10/14] [SUCCESS] VLM –∏–∑–≤–ª–µ—á–µ–Ω–æ {len(formulas)} —Ñ–æ—Ä–º—É–ª")
                return formulas
            
            return None
            
        except Exception as e:
            logger.error(f"[Stage 10/14] VLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª failed: {e}")
            return None
    
    def _detect_formulas_on_page(self, image, page_num: int, doc_type: str) -> List[Dict]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        
        try:
            import cv2
            import numpy as np
            import pytesseract
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PIL –≤ OpenCV
            cv_image = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
            
            # –ò—â–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
            
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
            formula_patterns = [
                r'[a-zA-Z]\s*=\s*[a-zA-Z0-9\+\-\*/\(\)\^]+',  # –ü—Ä–æ—Å—Ç—ã–µ —Ñ–æ—Ä–º—É–ª—ã
                r'[a-zA-Z]\s*\(\s*[a-zA-Z0-9\+\-\*/\(\)\^]+\s*\)',  # –§—É–Ω–∫—Ü–∏–∏
                r'[a-zA-Z]\s*[0-9]+\s*[a-zA-Z0-9\+\-\*/\(\)\^]+',  # –§–æ—Ä–º—É–ª—ã —Å —á–∏—Å–ª–∞–º–∏
            ]
            
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_text = pytesseract.image_to_string(image, lang='rus')
            
            formulas = []
            import re
            for pattern in formula_patterns:
                matches = re.finditer(pattern, page_text)
                for match in matches:
                    formula_text = match.group()
                    if self._is_formula_content(formula_text):
                        formulas.append({
                            'page': page_num,
                            'formula': formula_text,
                            'latex': self._convert_to_latex(formula_text),
                            'confidence': 0.7
                        })
            
            return formulas
            
        except Exception as e:
            logger.error(f"[Stage 10/14] –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ failed: {e}")
            return []
    
    def _is_formula_content(self, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Ñ–æ—Ä–º—É–ª–æ–π"""
        
        formula_indicators = ['=', '+', '-', '*', '/', '^', '(', ')', 'sin', 'cos', 'tan', 'log', 'ln']
        return any(indicator in text for indicator in formula_indicators)
    
    def _convert_to_latex(self, formula: str) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—É–ª—É –≤ LaTeX —Ñ–æ—Ä–º–∞—Ç"""
        
        # –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        latex = formula
        latex = latex.replace('^', '^')
        latex = latex.replace('*', '\\cdot')
        latex = latex.replace('sin', '\\sin')
        latex = latex.replace('cos', '\\cos')
        latex = latex.replace('tan', '\\tan')
        latex = latex.replace('log', '\\log')
        latex = latex.replace('ln', '\\ln')
        
        return f"${latex}$"
    
    def _extract_order_data_with_vlm(self, pdf_path: str, doc_type_info: Dict) -> Optional[Dict]:
        """VLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∫–∞–∑–∞ –¥–ª—è –°–ü –∏ –ì–û–°–¢"""
        
        if not self.vlm_available or not self.vlm_processor:
            return None
        
        try:
            logger.info(f"[Stage 8/14] [FOUND] VLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∫–∞–∑–∞: {pdf_path}")
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∏–∫–∞–∑–∞
            max_pages = self._determine_order_search_pages(pdf_path, doc_type_info)
            from pdf2image import convert_from_path
            images = convert_from_path(pdf_path, first_page=1, last_page=max_pages, dpi=300)
            
            order_data = {}
            
            for page_num, image in enumerate(images):
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∫–∞–∑–∞
                page_order_data = self._analyze_order_on_page(image, page_num, doc_type_info)
                if page_order_data:
                    order_data.update(page_order_data)
            
            if order_data:
                logger.info(f"[Stage 8/14] [SUCCESS] VLM –∏–∑–≤–ª–µ–∫ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏–∫–∞–∑–∞: {order_data.get('order_number', 'N/A')}")
                return order_data
            
            return None
            
        except Exception as e:
            logger.error(f"[Stage 8/14] VLM –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∫–∞–∑–∞ failed: {e}")
            return None
    
    def _determine_order_search_pages(self, pdf_path: str, doc_type_info: Dict) -> int:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–∏–∫–∞–∑–∞"""
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –≤ PDF
            from pdf2image import convert_from_path
            images = convert_from_path(pdf_path, first_page=1, last_page=1, dpi=150)  # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞
            doc_type = doc_type_info.get('doc_type', '')
            
            if doc_type == 'sp':
                # –î–ª—è –°–ü –æ–±—ã—á–Ω–æ –ø—Ä–∏–∫–∞–∑ 1-3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ 5
                return 5
            elif doc_type == 'gost':
                # –î–ª—è –ì–û–°–¢ –ø—Ä–∏–∫–∞–∑ –æ–±—ã—á–Ω–æ 1-2 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                return 3
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ - –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
                return 2
                
        except Exception as e:
            logger.warning(f"[Stage 8/14] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {e}")
            return 3  # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _analyze_order_on_page(self, image, page_num: int, doc_type_info: Dict) -> Optional[Dict]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∫–∞–∑–∞"""
        
        try:
            import pytesseract
            import re
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            page_text = pytesseract.image_to_string(image, lang='rus')
            
            # –ò—â–µ–º –Ω–æ–º–µ—Ä –ø—Ä–∏–∫–∞–∑–∞
            order_patterns = [
                r'–ø—Ä–∏–∫–∞–∑\s+–º–∏–Ω—Å—Ç—Ä–æ—è\s+—Ä–æ—Å—Å–∏–∏\s+–æ—Ç\s+(\d{2}\.\d{2}\.\d{4})\s+‚Ññ\s*(\d+[\/\w]*)',
                r'–ø—Ä–∏–∫–∞–∑\s+–æ—Ç\s+(\d{2}\.\d{2}\.\d{4})\s+‚Ññ\s*(\d+[\/\w]*)',
                r'‚Ññ\s*(\d+[\/\w]*)\s+–æ—Ç\s+(\d{2}\.\d{2}\.\d{4})',
            ]
            
            order_number = None
            effective_date = None
            
            for pattern in order_patterns:
                matches = re.finditer(pattern, page_text, re.IGNORECASE)
                for match in matches:
                    if len(match.groups()) >= 2:
                        date_part = match.group(1)
                        number_part = match.group(2)
                        order_number = f"–ü—Ä–∏–∫–∞–∑ –æ—Ç {date_part} ‚Ññ {number_part}"
                        effective_date = date_part
                        break
                if order_number:
                    break
            
            # –ò—â–µ–º –≤–≤–æ–¥–Ω—É—é —á–∞—Å—Ç—å –ø—Ä–∏–∫–∞–∑–∞
            order_intro = None
            intro_patterns = [
                r'–≤\s+—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏\s+—Å[^.]*\.',
                r'–≤\s+—Ü–µ–ª—è—Ö[^.]*\.',
                r'–Ω–∞\s+–æ—Å–Ω–æ–≤–∞–Ω–∏–∏[^.]*\.',
                r'–ø—Ä–∏–∫–∞–∑—ã–≤–∞—é[^.]*\.',
            ]
            
            for pattern in intro_patterns:
                matches = re.finditer(pattern, page_text, re.IGNORECASE | re.DOTALL)
                for match in matches:
                    if len(match.group()) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –≤–≤–æ–¥–Ω–æ–π —á–∞—Å—Ç–∏
                        order_intro = match.group().strip()
                        break
                if order_intro:
                    break
            
            if order_number or order_intro:
                return {
                    'order_number': order_number,
                    'effective_date': effective_date,
                    'order_intro': order_intro,
                    'page': page_num
                }
            
            return None
            
        except Exception as e:
            logger.error(f"[Stage 8/14] –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∫–∞–∑–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ failed: {e}")
            return None
    
    def _isolate_order_content(self, content: str, doc_type: str) -> Optional[Dict]:
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∏–∑–æ–ª—è—Ü–∏—è –ø—Ä–∏–∫–∞–∑–∞ –¥–ª—è –°–ü –∏ –ì–û–°–¢"""
        
        try:
            logger.info(f"[Stage 10/14] [FOUND] –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∏–∑–æ–ª—è—Ü–∏—è –ø—Ä–∏–∫–∞–∑–∞ –¥–ª—è {doc_type}")
            
            # SBERT-–¥–µ—Ç–µ–∫—Ç–æ—Ä —è–∫–æ—Ä—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—á–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            anchor_phrases = [
                "1. –û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è",
                "1 –û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è", 
                "1. –ù–ê–ó–ù–ê–ß–ï–ù–ò–ï",
                "1 –ù–ê–ó–ù–ê–ß–ï–ù–ò–ï",
                "1. –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø",
                "1 –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø",
                "1. –¢–ï–†–ú–ò–ù–´ –ò –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø",
                "1 –¢–ï–†–ú–ò–ù–´ –ò –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø"
            ]
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            content_lines = content.split('\n')
            search_window_size = self._determine_search_window_size(content, doc_type)
            search_window = content_lines[:search_window_size]
            
            best_match = None
            best_similarity = 0.0
            anchor_line = None
            
            # –ò—â–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–∏–±–æ–ª–µ–µ –±–ª–∏–∑–∫—É—é —Å—Ç—Ä–æ–∫—É –∫ —è–∫–æ—Ä–Ω—ã–º —Ñ—Ä–∞–∑–∞–º
            for line in search_window:
                line_clean = line.strip()
                if len(line_clean) < 10:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                for anchor in anchor_phrases:
                    if anchor.lower() in line_clean.lower():
                        best_match = line_clean
                        best_similarity = 1.0
                        anchor_line = line_clean
                        break
                
                if best_match:
                    break
                
                # SBERT —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
                if self.sbert_model:
                    try:
                        for anchor in anchor_phrases:
                            similarity = self._calculate_semantic_similarity(line_clean, anchor)
                            if similarity > best_similarity and similarity > 0.7:
                                best_similarity = similarity
                                best_match = line_clean
                                anchor_line = line_clean
                    except:
                        pass
            
            if best_match and best_similarity > 0.7:
                # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å —Å—Ç—Ä–æ–∫–∏ —Å —è–∫–æ—Ä–µ–º
                anchor_index = None
                for i, line in enumerate(content_lines):
                    if anchor_line in line:
                        anchor_index = i
                        break
                
                if anchor_index is not None:
                    # –ò–∑–æ–ª–∏—Ä—É–µ–º –ø—Ä–∏–∫–∞–∑
                    order_lines = content_lines[:anchor_index]
                    main_content_lines = content_lines[anchor_index:]
                    
                    order_content = '\n'.join(order_lines)
                    main_content = '\n'.join(main_content_lines)
                    
                    isolation_result = {
                        'order_content': order_content,
                        'main_content': main_content,
                        'order_length': len(order_content),
                        'main_length': len(main_content),
                        'anchor_line': anchor_line,
                        'similarity': best_similarity,
                        'isolation_method': 'SBERT_SEMANTIC'
                    }
                    
                    logger.info(f"[Stage 10/14] [SUCCESS] –Ø–ö–û–†–¨ –ù–ê–ô–î–ï–ù: '{anchor_line}' (similarity: {best_similarity:.2f})")
                    return isolation_result
            
            # FALLBACK: –ï—Å–ª–∏ —è–∫–æ—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∫–∞–∑–∞
            has_order_indicators = self._check_for_order_indicators(content_lines[:search_window_size])
            if has_order_indicators:
                # –ï—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–∏–∫–∞–∑–∞, –Ω–æ —è–∫–æ—Ä—å –Ω–µ –Ω–∞–π–¥–µ–Ω - –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É
                logger.warning(f"[Stage 10/14] [WARN] –ü–†–ò–ó–ù–ê–ö–ò –ü–†–ò–ö–ê–ó–ê –ù–ê–ô–î–ï–ù–´, –Ω–æ —è–∫–æ—Ä—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω")
                return self._fallback_order_isolation(content_lines, doc_type)
            else:
                # –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏–∫–∞–∑–∞ - –¥–æ–∫—É–º–µ–Ω—Ç –±–µ–∑ –ø—Ä–∏–∫–∞–∑–∞
                logger.info(f"[Stage 10/14] [INFO] –î–û–ö–£–ú–ï–ù–¢ –ë–ï–ó –ü–†–ò–ö–ê–ó–ê: {doc_type}")
                return None
            
        except Exception as e:
            logger.error(f"[Stage 10/14] –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –∏–∑–æ–ª—è—Ü–∏—è –ø—Ä–∏–∫–∞–∑–∞ failed: {e}")
            return None
    
    def _determine_search_window_size(self, content: str, doc_type: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏ –ø—Ä–∏–∫–∞–∑–∞"""
        
        try:
            content_lines = content.split('\n')
            total_lines = len(content_lines)
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            if doc_type == 'sp':
                # –î–ª—è –°–ü –∏—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö 10% –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ –º–∞–∫—Å–∏–º—É–º 200 —Å—Ç—Ä–æ–∫
                return min(200, max(50, total_lines // 10))
            elif doc_type == 'gost':
                # –î–ª—è –ì–û–°–¢ –∏—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö 5% –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ –º–∞–∫—Å–∏–º—É–º 100 —Å—Ç—Ä–æ–∫
                return min(100, max(30, total_lines // 20))
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ - –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥
                return min(50, max(20, total_lines // 25))
                
        except Exception as e:
            logger.warning(f"[Stage 10/14] –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return 100  # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _check_for_order_indicators(self, content_lines: List[str]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∏–∫–∞–∑–∞ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ"""
        
        try:
            order_indicators = [
                '–ø—Ä–∏–∫–∞–∑', '–ø—Ä–∏–∫–∞–∑—ã–≤–∞—é', '–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å', '–Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏',
                '–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ', '–º–∏–Ω—Å—Ç—Ä–æ–π', '—É—Ç–≤–µ—Ä–∂–¥–∞—é', '–≤–≤–æ–∂—É –≤ –¥–µ–π—Å—Ç–≤–∏–µ',
                '–≤ —Ü–µ–ª—è—Ö', '–¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è'
            ]
            
            content_text = ' '.join(content_lines).lower()
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            indicator_count = 0
            for indicator in order_indicators:
                if indicator in content_text:
                    indicator_count += 1
            
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–æ 3+ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –µ—Å—Ç—å –ø—Ä–∏–∫–∞–∑
            return indicator_count >= 3
            
        except Exception as e:
            logger.error(f"[Stage 10/14] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∫–∞–∑–∞ failed: {e}")
            return False
    
    def _fallback_order_isolation(self, content_lines: List[str], doc_type: str) -> Optional[Dict]:
        """Fallback –∏–∑–æ–ª—è—Ü–∏—è –ø—Ä–∏–∫–∞–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏"""
        
        try:
            # –ò—â–µ–º –ø–µ—Ä–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            normative_starters = [
                '1.', '2.', '3.', '4.', '5.',
                '1 ', '2 ', '3 ', '4 ', '5 ',
                '–æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è', '–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ', '–æ–±—â–∏–µ –ø–æ–ª–æ–∂–µ–Ω–∏—è',
                '—Ç–µ—Ä–º–∏–Ω—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è', '–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å—Å—ã–ª–∫–∏'
            ]
            
            # –ò—â–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É —Å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –Ω–∞—á–∞–ª–æ–º
            anchor_index = None
            for i, line in enumerate(content_lines):
                line_clean = line.strip().lower()
                for starter in normative_starters:
                    if starter in line_clean and len(line_clean) > 10:
                        anchor_index = i
                        break
                if anchor_index is not None:
                    break
            
            if anchor_index is not None and anchor_index > 5:  # –ú–∏–Ω–∏–º—É–º 5 —Å—Ç—Ä–æ–∫ –¥–æ —è–∫–æ—Ä—è
                # –ò–∑–æ–ª–∏—Ä—É–µ–º –ø—Ä–∏–∫–∞–∑
                order_lines = content_lines[:anchor_index]
                main_content_lines = content_lines[anchor_index:]
                
                order_content = '\n'.join(order_lines)
                main_content = '\n'.join(main_content_lines)
                
                isolation_result = {
                    'order_content': order_content,
                    'main_content': main_content,
                    'order_length': len(order_content),
                    'main_length': len(main_content),
                    'anchor_line': content_lines[anchor_index].strip(),
                    'similarity': 0.5,  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è fallback
                    'isolation_method': 'HEURISTIC_FALLBACK'
                }
                
                logger.info(f"[Stage 10/14] [SUCCESS] FALLBACK –ò–ó–û–õ–Ø–¶–ò–Ø: {anchor_index} —Å—Ç—Ä–æ–∫ –¥–æ —è–∫–æ—Ä—è")
                return isolation_result
            
            logger.warning(f"[Stage 10/14] [WARN] FALLBACK –ò–ó–û–õ–Ø–¶–ò–Ø –ù–ï –£–î–ê–õ–ê–°–¨")
            return None
            
        except Exception as e:
            logger.error(f"[Stage 10/14] Fallback –∏–∑–æ–ª—è—Ü–∏—è –ø—Ä–∏–∫–∞–∑–∞ failed: {e}")
            return None
    
    def _create_order_header_chunk(self, order_isolation: Dict, metadata: Dict, doc_type_info: Dict) -> Optional[DocumentChunk]:
        """–°–æ–∑–¥–∞–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ –¥–ª—è –ø—Ä–∏–∫–∞–∑–∞"""
        
        try:
            order_content = order_isolation.get('order_content', '')
            if not order_content:
                return None
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫ –ø—Ä–∏–∫–∞–∑–∞
            order_chunk = DocumentChunk(
                content=order_content,
                chunk_id=f"order_header_{doc_type_info['doc_type']}",
                metadata={
                    **metadata,
                    'data_type': 'ORDER_HEADER',
                    'source_section': '–í–í–û–î–ù–´–ô_–ü–†–ò–ö–ê–ó',
                    'canonical_id': metadata.get('canonical_id', 'UNKNOWN'),
                    'order_number': metadata.get('order_number'),
                    'effective_date': metadata.get('effective_date'),
                    'isolation_method': order_isolation.get('isolation_method', 'SBERT_SEMANTIC'),
                    'anchor_line': order_isolation.get('anchor_line'),
                    'similarity': order_isolation.get('similarity', 0.0)
                },
                position=0  # –ü—Ä–∏–∫–∞–∑ –≤—Å–µ–≥–¥–∞ –≤ –Ω–∞—á–∞–ª–µ
            )
            
            logger.info(f"[Stage 13/14] [SUCCESS] –ß–ê–ù–ö –ü–†–ò–ö–ê–ó–ê –°–û–ó–î–ê–ù: {len(order_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            return order_chunk
            
        except Exception as e:
            logger.error(f"[Stage 13/14] –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–∞ –ø—Ä–∏–∫–∞–∑–∞ failed: {e}")
            return None
    
    def _check_vector_quality_safe(self, sbert_data: Dict) -> float:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
            works_count = len(sbert_data.get('works', []))
            dependencies_count = len(sbert_data.get('dependencies', []))
            
            if works_count == 0:
                return 0.0
            
            # –ë–∞–∑–æ–≤–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            quality = min(1.0, (works_count + dependencies_count) / 1000.0)
            
            return quality
            
        except Exception as e:
            logger.warning(f"[Stage 9/14] Vector quality check failed: {e}")
            return 0.5  # –°—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    def _stage11_work_sequence_extraction(self, sbert_data: Dict, doc_type_info: Dict, 
                                         metadata: DocumentMetadata) -> List[WorkSequence]:
        """STAGE 11: Work Sequence Extraction"""
        
        logger.info(f"[Stage 11/14] WORK SEQUENCE EXTRACTION")
        start_time = time.time()
        
        work_sequences = []
        works = sbert_data.get('works', [])
        dependencies = sbert_data.get('dependencies', [])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç
        for work in works:
            sequence = WorkSequence(
                name=work.get('name', ''),
                deps=[dep.get('to', '') for dep in dependencies if dep.get('from') == work.get('id')],
                duration=work.get('duration', 0.0),
                priority=work.get('priority', 0),
                quality_score=work.get('confidence', 0.5),
                doc_type=doc_type_info.get('doc_type', ''),
                section=work.get('section', '')
            )
            work_sequences.append(sequence)
        
        elapsed = time.time() - start_time
        logger.info(f"[Stage 11/14] COMPLETE - Extracted {len(work_sequences)} sequences ({elapsed:.2f}s)")
        
        return work_sequences

    def _stage12_save_work_sequences(self, work_sequences: List[WorkSequence], file_path: str, metadata: Dict = None) -> int:
        """STAGE 12: Save Work Sequences"""
        
        logger.info(f"[Stage 12/14] SAVE WORK SEQUENCES")
        start_time = time.time()
        
        saved_count = 0
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            for sequence in work_sequences:
                # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
                saved_count += 1
            
            elapsed = time.time() - start_time
            logger.info(f"[Stage 12/14] COMPLETE - Saved {saved_count} sequences ({elapsed:.2f}s)")
            
            return saved_count
            
        except Exception as e:
            logger.error(f"[Stage 12/14] ERROR - Failed to save sequences: {e}")
            return 0
    
    def _stage13_smart_chunking(self, content: str, structural_data: Dict,
                               metadata: Dict, doc_type_info: Dict) -> List[DocumentChunk]:
        """STAGE 13: Smart Chunking"""
        
        logger.info(f"[Stage 13/14] SMART CHUNKING")
        start_time = time.time()
        
        # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ SBERT –∏–∑ Stage 7
        sbert_reused = False
        if hasattr(self, 'sbert_model') and self.sbert_model is not None:
            logger.info(f"[PERF] Stage 7-13 SBERT reused: True (already loaded)")
            sbert_reused = True
        else:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º SBERT —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –±—ã–ª –∑–∞–≥—Ä—É–∂–µ–Ω –≤ Stage 7
            sbert_model = self._load_sbert_model()
            logger.info(f"[PERF] Stage 7-13 SBERT reused: False (loaded for Stage 13)")
        
        chunks = []
        
        try:
            # üéØ –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –õ–û–ì–ò–ö–ê –î–õ–Ø –ù–¢–î –î–û–ö–£–ú–ï–ù–¢–û–í
            doc_type = doc_type_info.get('doc_type', '')
            if doc_type in ['sp', 'gost', 'snip']:
                logger.info(f"[Stage 13/14] [NTD] –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –¥–ª—è {doc_type}")
                ntd_chunks = self._create_ntd_chunks(content, structural_data, metadata, doc_type_info)
                chunks.extend(ntd_chunks)
                logger.info(f"[Stage 13/14] [NTD] –°–æ–∑–¥–∞–Ω–æ {len(ntd_chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –ù–¢–î")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–∫–∞–∑–∞
            order_isolation = metadata.get('order_isolation')
            if order_isolation and order_isolation.get('order_content'):
                # –°–æ–∑–¥–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ –¥–ª—è –ø—Ä–∏–∫–∞–∑–∞
                order_chunk = self._create_order_header_chunk(order_isolation, metadata, doc_type_info)
                if order_chunk:
                    chunks.append(order_chunk)
                    logger.info(f"[Stage 13/14] [SUCCESS] –°–û–ó–î–ê–ù –ß–ê–ù–ö –ü–†–ò–ö–ê–ó–ê: {order_chunk.chunk_id}")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞
                content = order_isolation.get('main_content', content)
            
            # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–º–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞
            sections = structural_data.get('sections', [])
            paragraphs = structural_data.get('paragraphs', [])
            
            if sections or paragraphs:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —á–∞–Ω–∫–∏–Ω–≥–∞
                logger.info(f"[PERF] Chunking from structure: {len(sections)} sections, {len(paragraphs)} paragraphs")
                
                # –ß–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–µ–∫—Ü–∏–π
                for i, section in enumerate(sections):
                    section_text = section.get('text', '')
                    section_title = section.get('title', '')
                    logger.info(f"[DEBUG] Section {i}: title='{section_title}', text_length={len(section_text)}")
                    
                    if section_text and len(section_text.strip()) > 50:
                        chunk = DocumentChunk(
                            content=section_text,
                            chunk_id=f"section_{i}",
                            metadata=metadata,
                            section_id=section_title
                        )
                        chunks.append(chunk)
                        logger.info(f"[DEBUG] Created chunk for section {i}: {chunk.chunk_id}")
                    else:
                        logger.warning(f"[DEBUG] Section {i} skipped: text too short or empty")
                
                # –ß–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
                for i, paragraph in enumerate(paragraphs):
                    if paragraph.get('text') and len(paragraph['text'].strip()) > 50:
                        chunk = DocumentChunk(
                            content=paragraph['text'],
                            chunk_id=f"paragraph_{i}",
                            metadata=metadata
                        )
                        chunks.append(chunk)
            else:
                # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É —á–∞–Ω–∫–∏–Ω–≥—É
                logger.info(f"[PERF] Chunking from structure: fallback to text-based chunking")
                chunk_size = 1024
                overlap = 50
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏
                for i in range(0, len(content), chunk_size - overlap):
                    chunk_text = content[i:i + chunk_size]
                    if len(chunk_text.strip()) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                        chunk = DocumentChunk(
                            content=chunk_text,
                            chunk_id=f"chunk_{i}",
                            metadata=metadata
                        )
                        chunks.append(chunk)
            
            # –ï—Å–ª–∏ –Ω–µ —Å–æ–∑–¥–∞–ª–∏ —á–∞–Ω–∫–∏ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            if not chunks and content:
                logger.warning(f"[DEBUG] No chunks created from structure, using fallback chunking")
                chunk_size = 1024
                overlap = 50
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏
                for i in range(0, len(content), chunk_size - overlap):
                    chunk_text = content[i:i + chunk_size]
                    if len(chunk_text.strip()) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                        chunk = DocumentChunk(
                            content=chunk_text,
                            chunk_id=f"fallback_{i}",
                            metadata=metadata
                        )
                        chunks.append(chunk)
                        logger.info(f"[DEBUG] Created fallback chunk {i}: {chunk.chunk_id}")
            
            # üöÄ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ —É—Ä–æ–≤–Ω–µ —ç—Ç–∞–ø–∞
            if hasattr(self, 'sbert_model') and self.sbert_model is not None:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                structural_hash = hash(str(structural_data))
                cache_key = f"stage13_embeddings_{structural_hash}"
                
                if hasattr(self, 'embedding_cache') and self.embedding_cache.get("test_content", "DeepPavlov/rubert-base-cased"):
                    logger.info(f"[PERF] Stage 13 embeddings cached: True")
                else:
                    logger.info(f"[PERF] Stage 13 embeddings cached: False (will compute)")
            
            elapsed = time.time() - start_time
            logger.info(f"[Stage 13/14] COMPLETE - Created {len(chunks)} chunks ({elapsed:.2f}s)")
            
            # üöÄ –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò
            logger.info(f"[PERF] Stage 13 performance: {elapsed:.2f}s, chunks: {len(chunks)}, sbert_reused: {sbert_reused}")
            
            return chunks
            
        except Exception as e:
            logger.error(f"[Stage 13/14] ERROR - Chunking failed: {e}")
            return []
        finally:
            # üöÄ CONTEXT SWITCHING: SBERT –æ—Å—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º –¥–ª—è Stage 14
            logger.info(f"[VRAM MANAGER] SBERT kept loaded for Stage 14")
    
    def _stage14_save_to_qdrant(self, chunks: List[DocumentChunk], file_path: str, file_hash: str, metadata: Dict[str, Any]) -> int:
        """STAGE 14: Save to Qdrant"""
        
        logger.info(f"[Stage 14/14] SAVE TO QDRANT")
        start_time = time.time()
        
        saved_count = 0
        
        try:
            if not self.qdrant:
                logger.warning("[Stage 14/14] Qdrant client not available")
                return 0
            
            if not chunks:
                logger.info("[Stage 14/14] No chunks to save")
                return 0
            
            # üöÄ –ó–ê–ì–†–£–ñ–ê–ï–ú SBERT –î–õ–Ø –≠–ú–ë–ï–î–î–ò–ù–ì–û–í
            if not hasattr(self, 'sbert_model') or self.sbert_model is None:
                logger.info("[Stage 14/14] Loading SBERT for embeddings...")
                self._load_sbert_model()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ç–æ—á–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            points = []
            for i, chunk in enumerate(chunks):
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —á–∞–Ω–∫–∞
                if hasattr(self, 'sbert_model') and self.sbert_model is not None:
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                        embedding = self.sbert_model.encode([chunk.content])[0]
                    except Exception as e:
                        logger.warning(f"[Stage 14/14] Failed to create embedding for chunk {i}: {e}")
                        continue
                else:
                    logger.warning("[Stage 14/14] SBERT model not available for embeddings")
                    continue
                
                # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫—É –¥–ª—è Qdrant —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º
                from qdrant_client.models import PointStruct
                import uuid
                
                point = PointStruct(
                    id=str(uuid.uuid4()),  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è –∫–∞–∂–¥–æ–π —Ç–æ—á–∫–∏
                    vector=embedding.tolist(),
                    payload={
                        "content": chunk.content,
                        "file_path": file_path,
                        "file_hash": file_hash,
                        "chunk_id": chunk.chunk_id,
                        "section_id": chunk.section_id,
                        "chunk_type": chunk.chunk_type,
                        "metadata": chunk.metadata,
                        "doc_type": metadata.get('doc_type', 'unknown'),
                        "canonical_id": metadata.get('canonical_id', ''),
                        "quality_score": metadata.get('quality_score', 0.0)
                    }
                )
                points.append(point)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ—á–∫–∏ –≤ Qdrant
            if points:
                try:
                    self.qdrant.upsert(
                        collection_name="enterprise_docs",
                        points=points
                    )
                    saved_count = len(points)
                    logger.info(f"[Stage 14/14] Successfully saved {saved_count} points to Qdrant")
                except Exception as e:
                    logger.error(f"[Stage 14/14] Failed to upsert points to Qdrant: {e}")
                    return 0
            
            elapsed = time.time() - start_time
            logger.info(f"[Stage 14/14] COMPLETE - Saved {saved_count} chunks to Qdrant ({elapsed:.2f}s)")
            
            return saved_count
                    
        except Exception as e:
            logger.error(f"[Stage 14/14] ERROR - Failed to save to Qdrant: {e}")
            return 0
        finally:
            # üöÄ CONTEXT SWITCHING: –í—ã–≥—Ä—É–∂–∞–µ–º SBERT –ø–æ—Å–ª–µ Stage 14
            self._unload_sbert_model()
            logger.info(f"[VRAM MANAGER] SBERT unloaded after Stage 14")

    def _create_ntd_chunks(self, content: str, structural_data: Dict, metadata: Dict, doc_type_info: Dict) -> List[DocumentChunk]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è –ù–¢–î –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–°–ü, –ì–û–°–¢, –°–ù–∏–ü) —Å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–º –ì–û–°–¢-—á–∞–Ω–∫–∏–Ω–≥–æ–º"""
        chunks = []
        
        try:
            # 1. –ß–∞–Ω–∫ —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞
            title = metadata.get('title', '')
            if title:
                title_chunk = DocumentChunk(
                    content=f"–î–æ–∫—É–º–µ–Ω—Ç: {title}",
                    chunk_id="ntd_title",
                    metadata=metadata,
                    section_id="–ó–∞–≥–æ–ª–æ–≤–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                    chunk_type="title"
                )
                chunks.append(title_chunk)
                logger.info(f"[NTD] –°–æ–∑–¥–∞–Ω —á–∞–Ω–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {title[:50]}...")
            
            # 2. –†–ï–ö–£–†–°–ò–í–ù–´–ô –ì–û–°–¢-–ß–ê–ù–ö–ò–ù–ì: –ò—Å–ø–æ–ª—å–∑—É–µ–º _recursive_gost_chunking –¥–ª—è –ù–¢–î –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            logger.info(f"[NTD] –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ì–û–°–¢-—á–∞–Ω–∫–∏–Ω–≥ –¥–ª—è {doc_type_info.get('doc_type', 'unknown')}")
            recursive_chunks = self._recursive_gost_chunking(content, metadata)
            
            if recursive_chunks:
                chunks.extend(recursive_chunks)
                logger.info(f"[NTD] –°–æ–∑–¥–∞–Ω–æ {len(recursive_chunks)} —á–∞–Ω–∫–æ–≤ —Å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–º –ì–û–°–¢-—á–∞–Ω–∫–∏–Ω–≥–æ–º")
            else:
                logger.warning(f"[NTD] –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –ì–û–°–¢-—á–∞–Ω–∫–∏–Ω–≥ –Ω–µ —Å–æ–∑–¥–∞–ª —á–∞–Ω–∫–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                
                # 3. Fallback: —Ä–∞–∑–±–∏–≤–∞–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ —á–∞–Ω–∫–∏ –µ—Å–ª–∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
                chunk_size = 1024
                overlap = 100
                
                for i in range(0, len(content), chunk_size - overlap):
                    chunk_text = content[i:i + chunk_size]
                    if len(chunk_text.strip()) > 100:
                        chunk = DocumentChunk(
                            content=chunk_text,
                            chunk_id=f"ntd_fallback_{i}",
                            metadata={
                                **metadata,
                                "path": ["misc"],
                                "title": "–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç",
                                "hierarchy_level": 0
                            },
                            section_id="–û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç",
                            chunk_type="content"
                        )
                        chunks.append(chunk)
                        logger.info(f"[NTD] –°–æ–∑–¥–∞–Ω fallback —á–∞–Ω–∫ {i}")
            
            logger.info(f"[NTD] –ò—Ç–æ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –¥–ª—è –ù–¢–î")
            return chunks
            
        except Exception as e:
            logger.error(f"[NTD] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–ª—è –ù–¢–î: {e}")
            return []

    def _recursive_gost_chunking(self, content: str, metadata: Dict) -> List[DocumentChunk]:
        """Recursive –ì–û–°–¢-—á–∞–Ω–∫–∏–Ω–≥ —Å 3-—É—Ä–æ–≤–Ω–µ–≤–æ–π –∏–µ—Ä–∞—Ä—Ö–∏–µ–π (6‚Üí6.2‚Üí6.2.3) –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–µ—Ä–∞—Ä—Ö–∏–∏.
        
        Args:
            content: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
            metadata: –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            
        Returns:
            List[DocumentChunk]: –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–µ—Ä–∞—Ä—Ö–∏–∏
        """
        chunks = []
        
        try:
            # –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π
            # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Ç–æ—á–∫–∏ –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            level1_pattern = r'^(\d+)\.\s+([^\n]+)'
            level2_pattern = r'^(\d+\.\d+)\s+([^\n]+)'
            level3_pattern = r'^(\d+\.\d+\.\d+)\s+([^\n]+)'
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–∞–∑–¥–µ–ª—ã –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π
            level1_matches = list(re.finditer(level1_pattern, content, re.MULTILINE | re.DOTALL))
            level2_matches = list(re.finditer(level2_pattern, content, re.MULTILINE | re.DOTALL))
            level3_matches = list(re.finditer(level3_pattern, content, re.MULTILINE | re.DOTALL))
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–æ–≤ 1 —É—Ä–æ–≤–Ω—è
            for i, match in enumerate(level1_matches):
                section_number = match.group(1)
                section_title = match.group(2).strip()
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
                section_end = match.end()
                if i + 1 < len(level1_matches):
                    section_end = level1_matches[i + 1].start()
                else:
                    section_end = len(content)
                
                section_text = content[match.start():section_end].strip()
                
                if len(section_text) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                    chunk = DocumentChunk(
                        content=section_text,
                        chunk_id=f"gost_section_{section_number}",
                        metadata={
                            **metadata,
                            "path": [section_number],
                            "title": section_title,
                            "hierarchy_level": 1
                        },
                        section_id=section_number,
                        chunk_type="gost_section"
                    )
                    chunks.append(chunk)
                    logger.info(f"[RECURSIVE_GOST] –°–æ–∑–¥–∞–Ω —á–∞–Ω–∫ —Ä–∞–∑–¥–µ–ª–∞ 1 —É—Ä–æ–≤–Ω—è: {section_number}")
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–æ–≤ 2 —É—Ä–æ–≤–Ω—è
            for i, match in enumerate(level2_matches):
                section_number = match.group(1)
                section_title = match.group(2).strip()
                
                # –†–∞–∑–±–∏—Ä–∞–µ–º –ø—É—Ç—å
                path_parts = section_number.split('.')
                level1_part = path_parts[0] if len(path_parts) > 0 else ""
                level2_part = section_number
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
                section_end = match.end()
                # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑–¥–µ–ª —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è
                next_section_found = False
                for j in range(i + 1, len(level2_matches)):
                    if level2_matches[j].group(1).startswith(level1_part + "."):
                        section_end = level2_matches[j].start()
                        next_section_found = True
                        break
                
                if not next_section_found:
                    # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑–¥–µ–ª 1 —É—Ä–æ–≤–Ω—è
                    level1_part_num = int(level1_part)
                    for j, l1_match in enumerate(level1_matches):
                        l1_num = int(l1_match.group(1))
                        if l1_num > level1_part_num:
                            section_end = l1_match.start()
                            next_section_found = True
                            break
                
                if not next_section_found:
                    section_end = len(content)
                
                section_text = content[match.start():section_end].strip()
                
                if len(section_text) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                    chunk = DocumentChunk(
                        content=section_text,
                        chunk_id=f"gost_section_{section_number}",
                        metadata={
                            **metadata,
                            "path": [level1_part, level2_part],
                            "title": section_title,
                            "hierarchy_level": 2
                        },
                        section_id=section_number,
                        chunk_type="gost_section"
                    )
                    chunks.append(chunk)
                    logger.info(f"[RECURSIVE_GOST] –°–æ–∑–¥–∞–Ω —á–∞–Ω–∫ —Ä–∞–∑–¥–µ–ª–∞ 2 —É—Ä–æ–≤–Ω—è: {section_number}")
            
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–æ–≤ 3 —É—Ä–æ–≤–Ω—è
            for i, match in enumerate(level3_matches):
                section_number = match.group(1)
                section_title = match.group(2).strip()
                
                # –†–∞–∑–±–∏—Ä–∞–µ–º –ø—É—Ç—å
                path_parts = section_number.split('.')
                level1_part = path_parts[0] if len(path_parts) > 0 else ""
                level2_part = f"{path_parts[0]}.{path_parts[1]}" if len(path_parts) > 1 else ""
                level3_part = section_number
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–µ—Ü —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
                section_end = match.end()
                # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑–¥–µ–ª —Ç–æ–≥–æ –∂–µ —É—Ä–æ–≤–Ω—è
                next_section_found = False
                for j in range(i + 1, len(level3_matches)):
                    if level3_matches[j].group(1).startswith(f"{level1_part}.{level2_part}."):
                        section_end = level3_matches[j].start()
                        next_section_found = True
                        break
                
                if not next_section_found:
                    # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑–¥–µ–ª 2 —É—Ä–æ–≤–Ω—è –≤ —Ç–æ–π –∂–µ —Å–µ–∫—Ü–∏–∏ 1 —É—Ä–æ–≤–Ω—è
                    for j, l2_match in enumerate(level2_matches):
                        l2_num = l2_match.group(1)
                        if l2_num.startswith(f"{level1_part}.") and l2_num > level2_part:
                            section_end = l2_match.start()
                            next_section_found = True
                            break
                
                if not next_section_found:
                    # –ò—â–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ä–∞–∑–¥–µ–ª 1 —É—Ä–æ–≤–Ω—è
                    level1_part_num = int(level1_part)
                    for j, l1_match in enumerate(level1_matches):
                        l1_num = int(l1_match.group(1))
                        if l1_num > level1_part_num:
                            section_end = l1_match.start()
                            next_section_found = True
                            break
                
                if not next_section_found:
                    section_end = len(content)
                
                section_text = content[match.start():section_end].strip()
                
                if len(section_text) > 50:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                    chunk = DocumentChunk(
                        content=section_text,
                        chunk_id=f"gost_section_{section_number}",
                        metadata={
                            **metadata,
                            "path": [level1_part, level2_part, level3_part],
                            "title": section_title,
                            "hierarchy_level": 3
                        },
                        section_id=section_number,
                        chunk_type="gost_section"
                    )
                    chunks.append(chunk)
                    logger.info(f"[RECURSIVE_GOST] –°–æ–∑–¥–∞–Ω —á–∞–Ω–∫ —Ä–∞–∑–¥–µ–ª–∞ 3 —É—Ä–æ–≤–Ω—è: {section_number}")
            
            logger.info(f"[RECURSIVE_GOST] –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π")
            return chunks
            
        except Exception as e:
            logger.error(f"[RECURSIVE_GOST] –û—à–∏–±–∫–∞ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞: {e}")
            return []


if __name__ == "__main__":
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ Enterprise RAG Trainer"""
    
    import sys
    import os
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('rag_training.log')
        ]
    )
    
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("=== ENTERPRISE RAG TRAINER - STARTING ===")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
        trainer = EnterpriseRAGTrainer()
        
        # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        logger.info("Starting RAG training process...")
        trainer.train()
        
        logger.info("=== ENTERPRISE RAG TRAINER - COMPLETED ===")
        
    except KeyboardInterrupt:
        logger.info("Training interrupted by user")
        sys.exit(0)
    except Exception as e:
        logger.error(f"Training failed: {e}")
        sys.exit(1)
    
    def _extract_from_sections(self, sections: List[Dict], metadata: DocumentMetadata) -> DocumentMetadata:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–µ–∫—Ü–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        try:
            for section in sections:
                section_content = section.get('content', '')
                if section_content:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–µ–∫—Ü–∏–∏
                    if '–Ω–∞–∑–≤–∞–Ω–∏–µ' in section_content.lower():
                        metadata.title = section_content[:100]
                    if '–∞–≤—Ç–æ—Ä' in section_content.lower():
                        metadata.source_author = section_content[:50]
        
            return metadata
            
        except Exception as e:
            logger.error(f"Error extracting from sections: {e}")
            return metadata
    
    def _extract_from_tables(self, tables: List[Dict], metadata: DocumentMetadata) -> DocumentMetadata:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Ç–∞–±–ª–∏—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        try:
            for table in tables:
                table_content = table.get('content', '')
                if table_content:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
                    if '–º–∞—Ç–µ—Ä–∏–∞–ª' in table_content.lower():
                        metadata.materials = [table_content[:50]]
                    if '—Å—Ç–æ–∏–º–æ—Å—Ç—å' in table_content.lower():
                        metadata.finances = [table_content[:50]]
            
            return metadata
            
        except Exception as e:
            logger.error(f"Error extracting from tables: {e}")
            return metadata
    
    def _extract_norms_metadata(self, norm_elements: List[Dict], metadata: DocumentMetadata) -> DocumentMetadata:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        
        try:
            for element in norm_elements:
                element_content = element.get('content', '')
                if element_content:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    if '—Å–ø' in element_content.lower():
                        metadata.doc_numbers.append(element_content[:20])
                    if '—Å–Ω–∏–ø' in element_content.lower():
                        metadata.doc_numbers.append(element_content[:20])
            
            return metadata
            
        except Exception as e:
            logger.error(f"Error extracting norms metadata: {e}")
            return metadata
    
    def _extract_smeta_metadata(self, smeta_items: List[Dict], metadata: DocumentMetadata) -> DocumentMetadata:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —Å–º–µ—Ç–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        
        try:
            for item in smeta_items:
                item_content = item.get('content', '')
                if item_content:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    if '—Å—Ç–æ–∏–º–æ—Å—Ç—å' in item_content.lower():
                        metadata.finances.append(item_content[:50])
                    if '–º–∞—Ç–µ—Ä–∏–∞–ª' in item_content.lower():
                        metadata.materials.append(item_content[:50])
            
            return metadata
            
        except Exception as e:
            logger.error(f"Error extracting smeta metadata: {e}")
            return metadata
    
    def _extract_ppr_metadata(self, ppr_stages: List[Dict], metadata: DocumentMetadata) -> DocumentMetadata:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ —ç—Ç–∞–ø–æ–≤ –ü–ü–†"""
        
        try:
            for stage in ppr_stages:
                stage_content = stage.get('content', '')
                if stage_content:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç—Ç–∞–ø–∞—Ö
                    if '—ç—Ç–∞–ø' in stage_content.lower():
                        metadata.work_sequences.append(stage_content[:50])
                    if '–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å' in stage_content.lower():
                        metadata.work_sequences.append(stage_content[:50])
            
            return metadata
            
        except Exception as e:
            logger.error(f"Error extracting ppr metadata: {e}")
            return metadata
    
    def _calculate_metadata_quality(self, metadata: DocumentMetadata) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        
        try:
            quality_score = 0.0
            
            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            if metadata.canonical_id and len(metadata.canonical_id) > 5:
                quality_score += 30.0
            
            if metadata.title and len(metadata.title) > 10:
                quality_score += 20.0
            
            if metadata.source_author:
                quality_score += 15.0
            
            if metadata.publication_date:
                quality_score += 10.0
            
            if metadata.doc_numbers:
                quality_score += 15.0
            
            if metadata.materials:
                quality_score += 5.0
            
            if metadata.finances:
                quality_score += 5.0
            
            return min(quality_score, 100.0)
            
        except Exception as e:
            logger.error(f"Error calculating metadata quality: {e}")
            return 0.0
    
    def _extract_semantic_title(self, content: str, doc_type_info: Dict, structural_data: Dict) -> DocumentMetadata:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞"""
        
        try:
            metadata = DocumentMetadata()
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º SBERT –¥–ª—è –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            if hasattr(self, 'sbert_model'):
                # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ SBERT
                pass
            
            return metadata
            
        except Exception as e:
            logger.error(f"Error extracting semantic title: {e}")
            return DocumentMetadata()
    
    def _extract_heuristic_fallback(self, content: str, doc_type_info: Dict, structural_data: Dict) -> DocumentMetadata:
        """–≠–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∏–π fallback –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        
        try:
            metadata = DocumentMetadata()
            
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫
            lines = content.split('\n')[:10]
            for line in lines:
                if len(line.strip()) > 10:
                    metadata.title = line.strip()[:100]
                    break
            
            return metadata
            
        except Exception as e:
            logger.error(f"Error in heuristic fallback: {e}")
            return DocumentMetadata()
    
    def _extract_strict_technical_metadata(self, content: str, doc_type_info: Dict, structural_data: Dict) -> DocumentMetadata:
        """–°–¢–†–ê–¢–ï–ì–ò–Ø 1: –°—Ç—Ä–æ–≥–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø–∞—Ä—Å–∏–Ω–≥ (Regex-First) –¥–ª—è –ù–¢–î, —Å–º–µ—Ç, –ø—Ä–∞–≤–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        
        metadata = DocumentMetadata()
        doc_type = doc_type_info['doc_type']
        
        logger.info(f"[Stage 8/14] –°–¢–†–ê–¢–ï–ì–ò–Ø 1: –°—Ç—Ä–æ–≥–∏–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è {doc_type}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        header_text = self._extract_header_text(structural_data)
        if not header_text:
            # [ALERT] –ö–†–ò–¢–ò–ß–ù–û: Fallback - –∏—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø–æ –í–°–ï–ú–£ –¥–æ–∫—É–º–µ–Ω—Ç—É!
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 20000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            header_text = content[:20000] if content else ""  # 20K —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        
        # === –ù–¢–î (–°–ü, –°–ù–∏–ü, –ì–û–°–¢) ===
        if doc_type == 'norms':
            logger.info(f"[Stage 8/14] –ü–∞—Ä—Å–∏–Ω–≥ –ù–¢–î –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ ({len(header_text)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –ò–ó–ú–ï–ù–ï–ù–ò–ï–ú –∫ –°–ü
            amendment_info = self._detect_amendment_to_sp(header_text)
            if amendment_info:
                logger.info(f"[Stage 8/14] [ALERT] –û–ë–ù–ê–†–£–ñ–ï–ù–û –ò–ó–ú–ï–ù–ï–ù–ò–ï: {amendment_info}")
                metadata.canonical_id = amendment_info['full_name']
                metadata.doc_numbers = [amendment_info['full_name']]
                metadata.amendment_number = amendment_info['amendment_num']
                metadata.base_sp_id = amendment_info['base_sp_id']
                metadata.doc_type = 'amendment'
                metadata.title = f"–ò–∑–º–µ–Ω–µ–Ω–∏–µ {amendment_info['amendment_num']} –∫ {amendment_info['base_sp_id']}"
            else:
                # –û–±—ã—á–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ù–¢–î
                main_doc_numbers = self._extract_document_numbers(header_text)
                if main_doc_numbers:
                    metadata.canonical_id = main_doc_numbers[0]
                    metadata.doc_numbers = main_doc_numbers
                    metadata.title = f"–°–≤–æ–¥ –ø—Ä–∞–≤–∏–ª {main_doc_numbers[0]}"
                    logger.info(f"[Stage 8/14] –ù–¢–î –Ω–∞–π–¥–µ–Ω: {main_doc_numbers[0]}")
                else:
                    logger.warning(f"[Stage 8/14] [WARN] –ù–¢–î –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ")
        
        # === –°–ú–ï–¢–ù–´–ï –ù–û–†–ú–ê–¢–ò–í–´ (–ì–≠–°–ù, –§–ï–†, –¢–ï–†) ===
        elif doc_type == 'smeta':
            logger.info(f"[Stage 8/14] –ü–∞—Ä—Å–∏–Ω–≥ —Å–º–µ—Ç–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–æ–≤")
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ì–≠–°–ù/–§–ï–†
            smeta_patterns = [
                r'–ì–≠–°–ù[—Ä]?-[–ê-–Ø–Å]+-\w+\d+-\d+',  # –ì–≠–°–ù—Ä-–û–ü-–†–∞–∑–¥–µ–ª—ã51-69
                r'–ì–≠–°–ù[—Ä]?-\w+-\d+',              # –ì–≠–°–ù-–û–ü-51
                r'–ì–≠–°–ù[—Ä]?\s*-\s*[–ê-–Ø–Å]+',       # –ì–≠–°–ù—Ä-–û–ü
                r'–§–ï–†[—Ä]?-[–ê-–Ø–Å]+-\w+\d+-\d+',    # –§–ï–†—Ä-–û–ü-–†–∞–∑–¥–µ–ª—ã51-69
                r'–§–ï–†[—Ä]?-\w+-\d+',               # –§–ï–†-–û–ü-51
                r'–ì–≠–°–ù\s+\d+\.\d+\.\d{4}',        # –ì–≠–°–ù 81-02-09-2001
                r'–§–ï–†\s+\d+\.\d+\.\d{4}',         # –§–ï–† 81-02-09-2001
                r'–¢–ï–†\s+\d+\.\d+\.\d{4}',         # –¢–ï–† 81-02-09-2001
            ]
            
            for pattern in smeta_patterns:
                matches = re.findall(pattern, header_text, re.IGNORECASE)
                if matches:
                    metadata.canonical_id = matches[0]
                    metadata.doc_numbers = matches
                    metadata.title = f"–°–º–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã {matches[0]}"
                    logger.info(f"[Stage 8/14] –°–º–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã –Ω–∞–π–¥–µ–Ω—ã: {matches[0]}")
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ, –ø—Ä–æ–±—É–µ–º –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            if not metadata.canonical_id:
                file_path = getattr(self, '_current_file_path', '')
                if file_path:
                    filename = Path(file_path).stem
                    # –£–±–∏—Ä–∞–µ–º —Ö–µ—à-–ø—Ä–µ—Ñ–∏–∫—Å—ã
                    if '_' in filename:
                        parts = filename.split('_')
                        for part in reversed(parts):
                            if any(c.isalpha() for c in part):
                                filename = part
                                break
                    
                    for pattern in smeta_patterns:
                        matches = re.findall(pattern, filename, re.IGNORECASE)
                        if matches:
                            metadata.canonical_id = matches[0]
                            metadata.doc_numbers = matches
                            metadata.title = f"–°–º–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã {matches[0]}"
                            logger.info(f"[Stage 8/14] –°–º–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤—ã –Ω–∞–π–¥–µ–Ω—ã –≤ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞: {matches[0]}")
                            break
        
        # === –ü–†–ê–í–û–í–´–ï –î–û–ö–£–ú–ï–ù–¢–´ ===
        elif doc_type == 'legal':
            logger.info(f"[Stage 8/14] –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–∞–≤–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            legal_patterns = [
                r'–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ\s+–ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞\s+–†–§\s+–æ—Ç\s+\d{1,2}\.\d{1,2}\.\d{4}\s+N\s+\d+',
                r'–ü—Ä–∏–∫–∞–∑\s+–æ—Ç\s+\d{1,2}\.\d{1,2}\.\d{4}\s+N\s+\d+',
                r'–§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–π\s+–∑–∞–∫–æ–Ω\s+–æ—Ç\s+\d{1,2}\.\d{1,2}\.\d{4}\s+N\s+\d+',
                r'–ü–ü\s+–†–§\s+–æ—Ç\s+\d{1,2}\.\d{1,2}\.\d{4}\s+N\s+\d+',
            ]
            
            for pattern in legal_patterns:
                matches = re.findall(pattern, header_text, re.IGNORECASE)
                if matches:
                    metadata.canonical_id = matches[0]
                    metadata.doc_numbers = matches
                    metadata.title = matches[0]
                    logger.info(f"[Stage 8/14] –ü—Ä–∞–≤–æ–≤–æ–π –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω: {matches[0]}")
                    break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if metadata.canonical_id:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã
            date_patterns = [r'\d{4}', r'\d{1,2}\.\d{1,2}\.\d{4}']
            for pattern in date_patterns:
                matches = re.findall(pattern, header_text)
                metadata.dates.extend(matches)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Ç–µ—Ä–∏–∞–ª—ã
            material_patterns = [r'–±–µ—Ç–æ–Ω', r'—Å—Ç–∞–ª—å', r'–¥–µ—Ä–µ–≤–æ', r'–∫–∏—Ä–ø–∏—á']
            for pattern in material_patterns:
                if re.search(pattern, header_text, re.IGNORECASE):
                    metadata.materials.append(pattern)
        
        return metadata
    
    def _extract_author_from_content(self, content: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞–≤—Ç–æ—Ä–∞ –∏–∑ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∞–≤—Ç–æ—Ä–∞
        author_patterns = [
            r'–ê–≤—Ç–æ—Ä[:\s]+([–ê-–Ø–Å–∞-—è—ë\s\.]+)',
            r'–°–æ—Å—Ç–∞–≤–∏—Ç–µ–ª—å[:\s]+([–ê-–Ø–Å–∞-—è—ë\s\.]+)',
            r'–ü–æ–¥ —Ä–µ–¥–∞–∫—Ü–∏–µ–π[:\s]+([–ê-–Ø–Å–∞-—è—ë\s\.]+)',
            r'–†–µ–¥–∞–∫—Ç–æ—Ä[:\s]+([–ê-–Ø–Å–∞-—è—ë\s\.]+)',
            r'([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.)',  # –ò.–ò. –ò–≤–∞–Ω–æ–≤
            r'([–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+)',  # –ò–≤–∞–Ω–æ–≤ –ò–≤–∞–Ω
        ]
        
        # [ALERT] –ö–†–ò–¢–ò–ß–ù–û: –ò—â–µ–º –∞–≤—Ç–æ—Ä–∞ –ø–æ –í–°–ï–ú–£ –¥–æ–∫—É–º–µ–Ω—Ç—É - –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –∫–æ–Ω—Ü–µ!
        # –ò—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö 10000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–≤—Ç–æ—Ä–∞
        search_text = content[:10000] if content else ""
        
        for pattern in author_patterns:
            match = re.search(pattern, search_text, re.IGNORECASE)
            if match:
                author = match.group(1).strip()
                if len(author) > 3 and len(author) < 100:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞
                    return author
        
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∞–≤—Ç–æ—Ä"
        
        # 3. –ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–Ω—Å
        metadata.canonical_id = f"–î–û–ö–£–ú–ï–ù–¢_{doc_type.upper()}"
        metadata.title = metadata.canonical_id
        logger.warning(f"[Stage 8/14] [WARN] Fallback: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –æ–±—â–∏–π —à–∞–±–ª–æ–Ω: {metadata.canonical_id}")
        
        return metadata
    
    
    def _extract_document_numbers_from_filename(self, file_path: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        try:
            filename = Path(file_path).stem  # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            
            # –£–±–∏—Ä–∞–µ–º —Ö–µ—à-–ø—Ä–µ—Ñ–∏–∫—Å—ã –µ—Å–ª–∏ –µ—Å—Ç—å
            if '_' in filename:
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —á–∞—Å—Ç—å –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏—è
                parts = filename.split('_')
                if len(parts) > 1:
                    # –ò—â–µ–º —á–∞—Å—Ç—å, –∫–æ—Ç–æ—Ä–∞—è —Å–æ–¥–µ—Ä–∂–∏—Ç –±—É–∫–≤—ã (–Ω–µ —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä—ã)
                    for part in reversed(parts):
                        if any(c.isalpha() for c in part):
                            filename = part
                            break
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞
            document_numbers = self._extract_document_numbers(filename)
            
            logger.info(f"[Stage 8/14] [FOUND] –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞: '{filename}' -> {document_numbers}")
            return document_numbers
            
        except Exception as e:
            logger.error(f"[Stage 8/14] [ERROR] –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞: {e}")
            return []
    
    def _extract_document_numbers(self, text: str) -> List[str]:
        """
        –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
        –°–ü, –°–ù–∏–ü, –ì–û–°–¢, –û–°–¢, –ì–≠–°–ù, –§–ï–†, –¢–£, ISO, IEC, –ü—Ä–∏–∫–∞–∑—ã, –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        """
        document_numbers = []
        
        # 1. –°–ü/–°–ù–∏–ü (–°–≤–æ–¥—ã –ø—Ä–∞–≤–∏–ª –∏ –°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã) - –£–õ–£–ß–®–ï–ù–ù–´–ï –ü–ê–¢–¢–ï–†–ù–´
        sp_snip_patterns = [
            r'–°–ü\s+\d+\.\d+\.\d{4}',      # –°–ü 16.13330.2017
            r'–°–ü\s+\d+\.\d+\.\d{2}',      # –°–ü 16.13330.17
            r'–°–ü\s+\d+\.\d+',             # –°–ü 16.13330
            r'–°–ü\s+\d+',                  # –°–ü 16 (—Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä)
            r'–°–ù–∏–ü\s+\d+\.\d+\.\d{4}',    # –°–ù–∏–ü 2.01.07-85
            r'–°–ù–∏–ü\s+\d+\.\d+',           # –°–ù–∏–ü 2.01.07
            r'–°–ù–∏–ü\s+\d+',                # –°–ù–∏–ü 2.01
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–ª–æ—Ö–∏—Ö PDF
            r'–°–ü\s*\d+\.\d+\.\d{4}',      # –°–ü16.13330.2017 (–±–µ–∑ –ø—Ä–æ–±–µ–ª–∞)
            r'–°–ü\s*\d+\.\d+',             # –°–ü16.13330 (–±–µ–∑ –ø—Ä–æ–±–µ–ª–∞)
            r'–°–ü\s*\d+',                  # –°–ü16 (–±–µ–∑ –ø—Ä–æ–±–µ–ª–∞)
        ]
        
        # 2. –ì–û–°–¢/–û–°–¢ (–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –æ—Ç—Ä–∞—Å–ª–µ–≤—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã)
        gost_ost_patterns = [
            r'–ì–û–°–¢\s+\d+\.\d+\.\d{4}',    # –ì–û–°–¢ 12.1.004-91
            r'–ì–û–°–¢\s+\d+\.\d+',           # –ì–û–°–¢ 12.1.004
            r'–û–°–¢\s+\d+\.\d+\.\d{4}',     # –û–°–¢ 36-118-85
            r'–û–°–¢\s+\d+\.\d+',            # –û–°–¢ 36-118
        ]
        
        # 3. –ì–≠–°–ù/–§–ï–† (–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç–Ω—ã–µ —Å–º–µ—Ç–Ω—ã–µ –Ω–æ—Ä–º—ã)
        gesn_fer_patterns = [
            r'–ì–≠–°–ù\s+\d+\.\d+\.\d{4}',    # –ì–≠–°–ù 81-02-09-2001
            r'–ì–≠–°–ù\s+\d+\.\d+',           # –ì–≠–°–ù 81-02-09
            r'–§–ï–†\s+\d+\.\d+\.\d{4}',     # –§–ï–† 81-02-09-2001
            r'–§–ï–†\s+\d+\.\d+',            # –§–ï–† 81-02-09
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ì–≠–°–ù —Å –¥–µ—Ñ–∏—Å–∞–º–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏
            r'–ì–≠–°–ù[—Ä]?-[–ê-–Ø–Å]+-\w+\d+-\d+',  # –ì–≠–°–ù—Ä-–û–ü-–†–∞–∑–¥–µ–ª—ã51-69
            r'–ì–≠–°–ù[—Ä]?-\w+-\d+',              # –ì–≠–°–ù-–û–ü-51
            r'–ì–≠–°–ù[—Ä]?\s*-\s*[–ê-–Ø–Å]+',       # –ì–≠–°–ù—Ä-–û–ü
            r'–§–ï–†[—Ä]?-[–ê-–Ø–Å]+-\w+\d+-\d+',    # –§–ï–†—Ä-–û–ü-–†–∞–∑–¥–µ–ª—ã51-69
            r'–§–ï–†[—Ä]?-\w+-\d+',               # –§–ï–†-–û–ü-51
        ]
        
        # 4. –¢–£ (–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è)
        tu_patterns = [
            r'–¢–£\s+\d+\.\d+\.\d{4}',      # –¢–£ 48-10-85
            r'–¢–£\s+\d+\.\d+',             # –¢–£ 48-10
        ]
        
        # 5. ISO/IEC (–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã)
        iso_iec_patterns = [
            r'ISO\s+\d+:\s*\d{4}',        # ISO 9001:2015
            r'IEC\s+\d+:\s*\d{4}',        # IEC 61000-4-2:2008
        ]
        
        # 6. –ü—Ä–∏–∫–∞–∑—ã, –ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è, –†–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏—è
        order_patterns = [
            r'–ü—Ä–∏–∫–∞–∑\s+–æ—Ç\s+.*?‚Ññ\s*[\d\.\-]+',
            r'–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ\s+–æ—Ç\s+.*?‚Ññ\s*[\d\.\-]+',
            r'–†–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ\s+–æ—Ç\s+.*?‚Ññ\s*[\d\.\-]+',
        ]
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        all_patterns = (sp_snip_patterns + gost_ost_patterns + gesn_fer_patterns + 
                       tu_patterns + iso_iec_patterns + order_patterns)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        for pattern in all_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            document_numbers.extend(matches)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        unique_numbers = list(set(filter(None, document_numbers)))
        
        return unique_numbers

    def _extract_header_text(self, structural_data: Dict) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞/–æ–±–ª–æ–∂–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –û–°–ù–û–í–ù–û–ì–û –¥–æ–∫—É–º–µ–Ω—Ç–∞
        (–ø–µ—Ä–≤—ã–µ 10-20 —Å—Ç—Ä–æ–∫, –∑–∞–≥–æ–ª–æ–≤–æ–∫, –æ–±–ª–æ–∂–∫–∞)
        """
        header_parts = []
        
        # 1. –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
        if 'header' in structural_data:
            header_parts.append(structural_data['header'])
        
        # 2. –ò—â–µ–º –≤ –ø–µ—Ä–≤—ã—Ö —Å–µ–∫—Ü–∏—è—Ö (–æ–±—ã—á–Ω–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–º)
        if 'sections' in structural_data:
            for section in structural_data['sections'][:3]:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 —Å–µ–∫—Ü–∏–∏
                if section.get('type') in ['header', 'title', 'cover']:
                    header_parts.append(section.get('content', ''))
                elif section.get('level', 0) <= 2:  # –ó–∞–≥–æ–ª–æ–≤–∫–∏ 1-2 —É—Ä–æ–≤–Ω—è
                    header_parts.append(section.get('content', ''))
        
        # 3. –ò—â–µ–º –≤ norm_elements (–¥–ª—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
        if 'norm_elements' in structural_data:
            for element in structural_data['norm_elements'][:5]:  # –ü–µ—Ä–≤—ã–µ 5 —ç–ª–µ–º–µ–Ω—Ç–æ–≤
                if element.get('type') in ['title', 'header', 'document_title']:
                    header_parts.append(element.get('text', ''))
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —á–∞—Å—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞
        header_text = ' '.join(filter(None, header_parts))
        
        # [ALERT] –ö–†–ò–¢–ò–ß–ù–û: –ù–ï –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –∑–∞–≥–æ–ª–æ–≤–∫–∞ - –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!
        # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ–ª–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        # if len(header_text) > 2000:
        #     header_text = header_text[:2000]
        
        return header_text

    
    
    
    def _calculate_metadata_quality(self, metadata: DocumentMetadata) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        
        score = 0.0
        
        if metadata.materials:
            score += min(len(metadata.materials) / 10.0, 0.4)
        
        if metadata.finances:
            score += min(len(metadata.finances) / 5.0, 0.3)
        
        if metadata.dates:
            score += min(len(metadata.dates) / 3.0, 0.2)
        
        if metadata.doc_numbers:
            score += min(len(metadata.doc_numbers) / 3.0, 0.1)
        
        return min(score, 1.0)
    
    # ================================================
    # Helper Methods for Quality Control
    # ================================================
    
    def _check_vector_quality(self, sbert_data: Dict) -> float:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤"""
        
        if not sbert_data.get('embeddings'):
            return 0.0
        
        embeddings = sbert_data['embeddings']
        if not embeddings or len(embeddings) == 0:
            return 0.0
        
        quality_score = 0.0
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        zero_vectors = sum(1 for emb in embeddings if all(x == 0.0 for x in emb))
        if zero_vectors > 0:
            quality_score -= 0.3 * (zero_vectors / len(embeddings))
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (–ø–ª–æ—Ö–∞—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
        unique_vectors = len(set(tuple(emb) for emb in embeddings))
        if unique_vectors < len(embeddings) * 0.8:  # –ú–µ–Ω–µ–µ 80% —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
            quality_score -= 0.2
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (–¥–ª–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å ~1.0)
        return max(quality_score, 0.0)
        
        if not sbert_data.get('embeddings'):
            return 0.0
        
        embeddings = sbert_data['embeddings']
        if not embeddings or len(embeddings) == 0:
            return 0.0
        
        quality_score = 0.0
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω—É–ª–µ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã
        zero_vectors = sum(1 for emb in embeddings if all(x == 0.0 for x in emb))
        if zero_vectors > 0:
            quality_score -= 0.3 * (zero_vectors / len(embeddings))
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –≤–µ–∫—Ç–æ—Ä—ã (–ø–ª–æ—Ö–∞—è –¥–∏–≤–µ—Ä—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
        unique_vectors = len(set(tuple(emb) for emb in embeddings))
        if unique_vectors < len(embeddings) * 0.8:  # –ú–µ–Ω–µ–µ 80% —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
            quality_score -= 0.2
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é (–¥–ª–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å ~1.0)
        vector_lengths = [sum(x**2 for x in emb)**0.5 for emb in embeddings]
        avg_length = sum(vector_lengths) / len(vector_lengths)
        if abs(avg_length - 1.0) > 0.3:  # –û—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –æ—Ç –Ω–æ—Ä–º—ã
            quality_score -= 0.2
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å 768 –¥–ª—è SBERT)
        expected_dim = 768
        wrong_dims = sum(1 for emb in embeddings if len(emb) != expected_dim)
        if wrong_dims > 0:
            quality_score -= 0.3 * (wrong_dims / len(embeddings))
        
        return max(0.0, min(1.0, 1.0 + quality_score))
    
    def _generate_emergency_canonical_id(self, content: str, doc_type_info: Dict) -> str:
        """[ALERT] –≠–ö–°–¢–†–ï–ù–ù–ê–Ø –ì–ï–ù–ï–†–ê–¶–ò–Ø –ö–ê–ù–û–ù–ò–ß–ï–°–ö–û–ì–û ID –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è '–î–æ–∫—É–º–µ–Ω—Ç.pdf'"""
        from datetime import datetime
        import hashlib
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å —á—Ç–æ-—Ç–æ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Å—Ç—Ä–æ–∫
        first_lines = content.split('\n')[:10]
        for line in first_lines:
            line = line.strip()
            if len(line) > 10 and any(keyword in line.lower() for keyword in ['—Å–ø', '—Å–Ω–∏–ø', '–≥–æ—Å—Ç', '–¥–æ–∫—É–º–µ–Ω—Ç', '–ø—Ä–∞–≤–∏–ª–∞']):
                # –û—á–∏—â–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                clean_line = re.sub(r'[^\w\s\.\-]', '', line)
                if len(clean_line) > 5:
                    return f"UNKNOWN_NORM_{clean_line[:30].replace(' ', '_')}"
        
        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        content_hash = hashlib.md5(content[:1000].encode()).hexdigest()[:8]
        timestamp = datetime.now().strftime('%Y%m%d')
        
        return f"UNKNOWN_NORM_{timestamp}_{content_hash}"
    
    # ================================================
    # API METHODS FOR FRONTEND INTEGRATION
    # ================================================
    
    def process_single_file_ad_hoc(self, file_path: str, save_to_db: bool = False) -> Optional[Dict]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–ª–∏ '–¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ª–µ—Ç—É'.
        
        Args:
            file_path: –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.
            save_to_db: –ï—Å–ª–∏ True, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Qdrant/Neo4j.
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏, —á–∞–Ω–∫–∞–º–∏ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏, –∏–ª–∏ None.
        """
        try:
            logger.info(f"[FOUND] API: Processing single file ad-hoc: {os.path.basename(file_path)}")
            
            # 1. –ó–∞–ø—É—Å–∫ core-–ø–∞–π–ø–ª–∞–π–Ω–∞
            processed_data = self._process_document_pipeline(file_path)
            
            if not processed_data:
                logger.warning(f"[ERROR] API: Failed to process {file_path}")
                return None
            
            if save_to_db:
                # 2. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (–µ—Å–ª–∏ —ç—Ç–æ —Ä–µ–∞–ª—å–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ)
                logger.info("[SAVE] API: Saving results to databases...")
                self._save_results_to_dbs(processed_data)
                logger.info("[SUCCESS] API: Results saved to databases")
            
            # 3. –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
                return {
                "success": True,
                "file_name": os.path.basename(file_path),
                "doc_type": processed_data.get('doc_type_info', {}).get('doc_type', 'unknown'),
                "metadata": processed_data.get('metadata', {}),
                "chunks_count": len(processed_data.get('chunks', [])),
                "processing_time": processed_data.get('processing_time', 0),
                "saved_to_db": save_to_db,
                "raw_data": processed_data  # –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                }
                
        except Exception as e:
            logger.error(f"[ERROR] API: Error in process_single_file_ad_hoc: {e}")
            return {
                "success": False,
                "error": str(e),
                "file_name": os.path.basename(file_path)
            }
    
    def analyze_project_context(self, file_list: List[str]) -> List[Dict]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–∞–∫–µ—Ç —Ñ–∞–π–ª–æ–≤ (–ø—Ä–æ–µ–∫—Ç) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
        –ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞ '–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–µ–∫—Ç–∞'.
        """
        try:
            logger.info(f"üìä API: Analyzing project context for {len(file_list)} files")
            results = []
            
            for i, file_path in enumerate(file_list, 1):
                logger.info(f"[DOC] API: Processing project file {i}/{len(file_list)}: {os.path.basename(file_path)}")
                
                # –í —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ –º—ã –ù–ò–ö–û–ì–î–ê –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î, —Ç–æ–ª—å–∫–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º!
                processed_data = self._process_document_pipeline(file_path)
                
                if processed_data:
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–∂–Ω—É—é –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
                    result = {
                        "file_name": os.path.basename(file_path),
                        "file_path": file_path,
                        "doc_type": processed_data.get('doc_type_info', {}).get('doc_type', 'unknown'),
                        "doc_subtype": processed_data.get('doc_type_info', {}).get('doc_subtype', ''),
                        "confidence": processed_data.get('doc_type_info', {}).get('confidence', 0.0),
                        "key_metadata": {
                            "date_approved": processed_data.get('metadata', {}).get('date_approved', ''),
                            "document_number": processed_data.get('metadata', {}).get('document_number', ''),
                            "organization": processed_data.get('metadata', {}).get('organization', ''),
                            "scope": processed_data.get('metadata', {}).get('scope', '')
                        },
                        "chunk_count": len(processed_data.get('chunks', [])),
                        "processing_time": processed_data.get('processing_time', 0),
                        "status": "success"
                    }
            else:
                result = {
                        "file_name": os.path.basename(file_path),
                        "file_path": file_path,
                        "status": "failed",
                        "error": "Processing failed"
                    }
                
                results.append(result)
            
            logger.info(f"[SUCCESS] API: Project analysis completed for {len(results)} files")
            return results
            
        except Exception as e:
            logger.error(f"[ERROR] API: Error in analyze_project_context: {e}")
            return [{
                "error": str(e),
                "status": "failed"
            }]
    
    def _stage9_save_to_neo4j(self, metadata: 'DocumentMetadata', structural_data: Dict) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã stage 9 (quality control) –≤ Neo4j.
        –°–æ–∑–¥–∞–µ—Ç —É–∑–ª—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º.
        """
        if not self.neo4j:
            logger.warning("Neo4j not available, skipping stage 9 save")
            return False
        
        try:
            with self.neo4j.session() as session:
                # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º —É–∑–µ–ª –¥–æ–∫—É–º–µ–Ω—Ç–∞
                session.run("""
                    MERGE (d:Document {canonical_id: $canonical_id})
                    SET d.title = $title,
                        d.doc_type = $doc_type,
                        d.quality_score = $quality_score,
                        d.quality_status = $quality_status,
                        d.processed_at = datetime(),
                        d.updated_at = datetime()
                    RETURN d
                """, 
                canonical_id=metadata.canonical_id,
                title=metadata.title,
                doc_type=metadata.doc_type,
                quality_score=getattr(metadata, 'quality_score', 0.0),
                quality_status=getattr(metadata, 'quality_status', 'unknown')
                )
                
                # –°–æ–∑–¥–∞–µ–º —É–∑–ª—ã —Å–µ–∫—Ü–∏–π
                for i, section in enumerate(structural_data.get('sections', [])):
                    session.run("""
                        MERGE (s:Section {doc_id: $doc_id, section_id: $section_id})
                        SET s.title = $title,
                            s.level = $level,
                            s.semantic_type = $semantic_type,
                            s.confidence = $confidence,
                            s.updated_at = datetime()
                        WITH s
                        MATCH (d:Document {canonical_id: $doc_id})
                        MERGE (d)-[:HAS_SECTION]->(s)
                        RETURN s
                    """,
                    doc_id=metadata.canonical_id,
                    section_id=f"section_{i}",
                    title=section.get('title', f'Section {i}'),
                    level=section.get('level', 1),
                    semantic_type=section.get('semantic_type', 'unknown'),
                    confidence=section.get('confidence', 0.0)
                    )
                
                logger.info(f"[Stage 9/14] Saved document metadata to Neo4j: {metadata.canonical_id}")
                return True
                
        except Exception as e:
            logger.error(f"[Stage 9/14] Error saving to Neo4j: {e}")
            return False
    
    def _save_results_to_dbs(self, processed_data: Dict) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (Neo4j + Qdrant).
        """
        try:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Neo4j
            if processed_data.get('metadata'):
                self._stage9_save_to_neo4j(processed_data['metadata'], processed_data['structural_data'])
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Qdrant
            if processed_data.get('chunks'):
                # –ü–æ–ª—É—á–∞–µ–º file_path –∏ file_hash –∏–∑ processed_data
                file_path = processed_data.get('file_path', 'unknown')
                file_hash = processed_data.get('file_hash', 'unknown')
                self._stage14_save_to_qdrant(processed_data['chunks'], file_path, file_hash, processed_data['metadata'])
            
            logger.info("[SUCCESS] Results saved to databases")
            return True
            
        except Exception as e:
            logger.error(f"[ERROR] Error saving results to databases: {e}")
            return False


# Production test stubs
import pytest

def test_config():
    assert config.base_dir.exists()

def test_text_extraction():
    # TODO: Implement test with actual trainer instance
    trainer = EnterpriseRAGTrainer()
    path = config.base_dir / 'test.pdf'
    content = trainer._stage3_text_extraction(str(path))  # Assume test data
    assert len(content) > 0

def test_chunking():
    # TODO: Implement test with actual trainer instance
    trainer = EnterpriseRAGTrainer()
    chunks = trainer._stage13_smart_chunking("test content", {}, {}, {'doc_type': 'sp'})
    assert len(chunks) > 0  # No hierarchy break

# Test stubs
def test_unified_extraction():
    # TODO: Implement test with actual trainer instance
    trainer = EnterpriseRAGTrainer()
    content = trainer._stage3_text_extraction('test.pdf')  # Your test file
    assert len(content) >= 0  # Allow empty content for test files

def test_custom_chunking():
    # TODO: Implement test with actual trainer instance
    trainer = EnterpriseRAGTrainer()
    chunks = trainer._custom_ntd_chunking("4.1.1 –¢–µ—Å—Ç –ø—É–Ω–∫—Ç–∞.", 'sp')
    assert any('4.1.1' in chunk for chunk in chunks)  # No break

if __name__ == "__main__":
    import pytest
    pytest.main(['-v'])

# üöÄ –ó–ê–ü–£–°–ö API –°–ï–†–í–ï–†–ê
if __name__ == "__main__":
    import sys
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if len(sys.argv) > 1 and sys.argv[1] == "api":
        # –ó–∞–ø—É—Å–∫ API —Å–µ—Ä–≤–µ—Ä–∞
        trainer = EnterpriseRAGTrainer()
        trainer.start_api_server()
    else:
        # –û–±—ã—á–Ω—ã–π –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
        trainer = EnterpriseRAGTrainer()
        trainer.train()
