"""
üß™ –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM –±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
"""

import logging
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_simple_llm():
    """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM"""
    
    try:
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM...")
        
        model_name = "ai-forever/rugpt3large_based_on_gpt2"
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        logger.info("üìù –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ë–ï–ó –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
        logger.info("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        logger.info(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {model.device}")
        
        # –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        test_prompt = """–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞:

–°–ù–∏–ü 2.01.07-85* "–ù–∞–≥—Ä—É–∑–∫–∏ –∏ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è"

–¢–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞:"""
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(test_prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        logger.info(f"üìä –¢–æ–∫–µ–Ω–æ–≤ –≤ –ø—Ä–æ–º–ø—Ç–µ: {inputs['input_ids'].shape[1]}")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
        logger.info("üîÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...")
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=inputs['input_ids'].shape[1] + 50,
                temperature=0.1,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = response[len(test_prompt):].strip()
        
        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {generated_text}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return False

if __name__ == "__main__":
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ç–µ—Å—Ç–∞ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ LLM")
    
    success = test_simple_llm()
    
    if success:
        logger.info("üéâ –¢–µ—Å—Ç –ø—Ä–æ—à–µ–ª —É—Å–ø–µ—à–Ω–æ!")
    else:
        logger.info("‚ùå –¢–µ—Å—Ç –Ω–µ –ø—Ä–æ—à–µ–ª")
    
    logger.info("üèÅ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
