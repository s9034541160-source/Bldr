"""
üá∑üá∫ –ê–Ω—Å–∞–º–±–ª—å —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö LLM –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ RAG –ø–∞–π–ø–ª–∞–π–Ω–∞
Stage 4: RuLongformer (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è) + Stage 8: RuT5 (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ) + Stage 5.5: RuGPT (–∞–Ω–∞–ª–∏–∑)
"""

import logging
import torch
import json
import time
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    AutoModelForSeq2SeqLM,
    pipeline,
    BitsAndBytesConfig
)

logger = logging.getLogger(__name__)

@dataclass
class RussianLLMEnsembleConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö LLM"""
    # Stage 4: RuT5 –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–Ω–∞ CPU)
    classification_model: str = "ai-forever/ruT5-base"  # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô RuT5
    classification_max_length: int = 2048
    
    # Stage 8: RuT5 –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–Ω–∞ CPU)
    extraction_model: str = "ai-forever/ruT5-base"  # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô RuT5
    extraction_max_length: int = 2048
    
    # Stage 5.5: RuT5 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–Ω–∞ CPU)
    analysis_model: str = "ai-forever/ruT5-base"  # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô RuT5
    analysis_max_length: int = 2048
    
    # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    device: str = "cpu"  # ‚úÖ –í–°–ï –ú–û–î–ï–õ–ò –ù–ê CPU
    use_quantization: bool = False  # ‚úÖ –ë–ï–ó –ö–í–ê–ù–¢–ò–ó–ê–¶–ò–ò –î–õ–Ø CPU
    cache_dir: str = "./models_cache"
    temperature: float = 0.1

class RussianLLMEnsemble:
    """
    –ê–Ω—Å–∞–º–±–ª—å —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö LLM –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á RAG –ø–∞–π–ø–ª–∞–π–Ω–∞
    Stage 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (RuLongformer)
    Stage 8: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (RuT5) 
    Stage 5.5: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (RuGPT)
    """
    
    def __init__(self, config: RussianLLMEnsembleConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
        self.classification_model = None
        self.classification_tokenizer = None
        self.extraction_model = None
        self.extraction_tokenizer = None
        self.analysis_model = None
        self.analysis_tokenizer = None
        
        self._initialize_models()
    
    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∞–Ω—Å–∞–º–±–ª—è"""
        try:
            self.logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö LLM...")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
            # –û—Ç–∫–ª—é—á–∞–µ–º –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            quantization_config = None
            # if self.config.use_quantization and torch.cuda.is_available():
            #     quantization_config = BitsAndBytesConfig(
            #         load_in_8bit=True,
            #         llm_int8_threshold=6.0,
            #         llm_int8_has_fp16_weight=False,
            #     )
            
            # Stage 4: –ú–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            self._initialize_classification_model(quantization_config)
            
            # Stage 8: –ú–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            self._initialize_extraction_model(quantization_config)
            
            # Stage 5.5: –ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            self._initialize_analysis_model(quantization_config)
            
            self.logger.info("‚úÖ –ê–Ω—Å–∞–º–±–ª—å —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω—Å–∞–º–±–ª—è: {e}")
            raise
    
    def _initialize_classification_model(self, quantization_config):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (Stage 4)"""
        try:
            self.logger.info(f"üìã –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {self.config.classification_model}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä RuT5 —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º slow tokenizer
            self.classification_tokenizer = AutoTokenizer.from_pretrained(
                self.config.classification_model,
                cache_dir=self.config.cache_dir,
                use_fast=False,  # ‚¨ÖÔ∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–ê–í–ö–ê –î–õ–Ø WINDOWS/RuT5
                trust_remote_code=True
            )
            
            if self.classification_tokenizer.pad_token is None:
                self.classification_tokenizer.pad_token = self.classification_tokenizer.eos_token
            
            # RuT5 –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ CPU, —á—Ç–æ–±—ã Qwen'—É —Ö–≤–∞—Ç–∏–ª–æ VRAM!
            self.classification_model = AutoModelForSeq2SeqLM.from_pretrained(
                self.config.classification_model,
                cache_dir=self.config.cache_dir,
                trust_remote_code=True,
                torch_dtype=torch.float32  # ‚úÖ float32 –¥–ª—è CPU
            ).to("cpu")  # ‚úÖ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –ù–ê CPU
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞
            if self.classification_model is not None:
                self.logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")
                self.logger.info(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: {self.classification_model.device}")
            else:
                self.logger.error("‚ùå –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            self.classification_model = None
    
    def _initialize_extraction_model(self, quantization_config):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (Stage 8) - RuT5 –Ω–∞ CPU"""
        try:
            self.logger.info(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {self.config.extraction_model}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä RuT5 —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º slow tokenizer
            self.extraction_tokenizer = AutoTokenizer.from_pretrained(
                self.config.extraction_model,
                cache_dir=self.config.cache_dir,
                use_fast=False,  # ‚¨ÖÔ∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–ê–í–ö–ê –î–õ–Ø WINDOWS/RuT5
                trust_remote_code=True
            )
            
            if self.extraction_tokenizer.pad_token is None:
                self.extraction_tokenizer.pad_token = self.extraction_tokenizer.eos_token
            
            # RuT5 –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ CPU, —á—Ç–æ–±—ã Qwen'—É —Ö–≤–∞—Ç–∏–ª–æ VRAM!
            self.extraction_model = AutoModelForSeq2SeqLM.from_pretrained(
                self.config.extraction_model,
                cache_dir=self.config.cache_dir,
                trust_remote_code=True,
                torch_dtype=torch.float32  # ‚úÖ float32 –¥–ª—è CPU
            ).to("cpu")  # ‚úÖ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –ù–ê CPU
            
            self.logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (RuT5) –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
            self.extraction_model = None
    
    def _initialize_analysis_model(self, quantization_config):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (Stage 5.5) - RuT5 –Ω–∞ CPU"""
        try:
            self.logger.info(f"üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞: {self.config.analysis_model}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä RuT5 —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º slow tokenizer
            self.analysis_tokenizer = AutoTokenizer.from_pretrained(
                self.config.analysis_model,
                cache_dir=self.config.cache_dir,
                use_fast=False,  # ‚¨ÖÔ∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–ê–í–ö–ê –î–õ–Ø WINDOWS/RuT5
                trust_remote_code=True
            )
            
            if self.analysis_tokenizer.pad_token is None:
                self.analysis_tokenizer.pad_token = self.analysis_tokenizer.eos_token
            
            # RuT5 –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ CPU, —á—Ç–æ–±—ã Qwen'—É —Ö–≤–∞—Ç–∏–ª–æ VRAM!
            self.analysis_model = AutoModelForSeq2SeqLM.from_pretrained(
                self.config.analysis_model,
                cache_dir=self.config.cache_dir,
                trust_remote_code=True,
                torch_dtype=torch.float32  # ‚úÖ float32 –¥–ª—è CPU
            ).to("cpu")  # ‚úÖ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –ù–ê CPU
            
            self.logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞ (RuT5) –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            self.analysis_model = None
    
    def stage_4_classify_document(self, content: str) -> Dict:
        """
        Stage 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å RuLongformer
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (4096+ —Ç–æ–∫–µ–Ω–æ–≤)
        """
        try:
            if self.classification_model is None:
                return {'classification_available': False, 'error': 'Model not loaded'}
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            prompt = self._prepare_classification_prompt(content)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
            classification_result = self._classify_with_llm(prompt, self.classification_model, self.classification_tokenizer)
            
            return {
                'classification_available': True,
                'document_type': classification_result.get('document_type', 'unknown'),
                'confidence': classification_result.get('confidence', 0.0),
                'subtype': classification_result.get('subtype', ''),
                'model': self.config.classification_model,
                'context_length': len(prompt)
            }
            
        except Exception as e:
            self.logger.error(f"Stage 4 classification failed: {e}")
            return {'classification_available': False, 'error': str(e)}
    
    def stage_8_extract_metadata(self, content: str, structural_data: Dict) -> Dict:
        """
        Stage 8: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å RuT5
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç QA/IE –ø–æ–¥—Ö–æ–¥ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        """
        try:
            if self.extraction_model is None:
                return {'extraction_available': False, 'error': 'Model not loaded'}
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é QA
            metadata = self._extract_metadata_with_qa(content, structural_data)
            
            return {
                'extraction_available': True,
                'metadata': metadata,
                'model': self.config.extraction_model,
                'fields_extracted': len(metadata)
            }
            
        except Exception as e:
            self.logger.error(f"Stage 8 extraction failed: {e}")
            return {'extraction_available': False, 'error': str(e)}
    
    def stage_5_5_deep_analysis(self, content: str, vlm_metadata: Dict) -> Dict:
        """
        Stage 5.5: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å RuGPT
        –û–±—ä–µ–¥–∏–Ω—è–µ—Ç VLM + LLM –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è
        """
        try:
            if self.analysis_model is None:
                return {'analysis_available': False, 'error': 'Model not loaded'}
            
            # –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            analysis_result = self._deep_analysis_with_llm(content, vlm_metadata)
            
            return {
                'analysis_available': True,
                'analysis': analysis_result,
                'model': self.config.analysis_model,
                'enhanced_sections': len(analysis_result.get('sections', [])),
                'extracted_entities': len(analysis_result.get('entities', []))
            }
            
        except Exception as e:
            self.logger.error(f"Stage 5.5 analysis failed: {e}")
            return {'analysis_available': False, 'error': str(e)}
    
    def _prepare_classification_prompt(self, content: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        max_length = self.config.classification_max_length - 500
        truncated_content = content[:max_length] if len(content) > max_length else content
        
        prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é.

–î–û–ö–£–ú–ï–ù–¢:
{truncated_content}

–¢–ò–ü–´ –î–û–ö–£–ú–ï–ù–¢–û–í:
- –°–ù–∏–ü (–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –ø—Ä–∞–≤–∏–ª–∞)
- –°–ü (–°–≤–æ–¥—ã –ø—Ä–∞–≤–∏–ª) 
- –ì–û–°–¢ (–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- –ü–ü–† (–ü—Ä–æ–µ–∫—Ç –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç)
- –°–º–µ—Ç–∞ (–õ–æ–∫–∞–ª—å–Ω–∞—è —Å–º–µ—Ç–∞)
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä–µ–≥–ª–∞–º–µ–Ω—Ç
- –ü—Ä–∏–∫–∞–∑/–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
- –ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- –î—Ä—É–≥–æ–µ

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
    "document_type": "–æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø",
    "subtype": "–ø–æ–¥—Ç–∏–ø –∏–ª–∏ –Ω–æ–º–µ—Ä",
    "confidence": 0.95,
    "keywords": ["–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"]
}}

–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ JSON."""
        
        return prompt
    
    def _classify_with_llm(self, prompt: str, model, tokenizer) -> Dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é LLM"""
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            inputs = tokenizer(prompt, return_tensors="pt", max_length=self.config.classification_max_length, truncation=True)
            
            # ‚úÖ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –ü–ï–†–ï–ù–û–°–ò–ú –í–°–ï –ù–ê CPU (RuT5 –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ CPU!)
            inputs = {k: v.to("cpu") for k, v in inputs.items()}
            model = model.to("cpu")  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å –Ω–∞ CPU
            
            # ‚úÖ –õ–æ–≥–∏—Ä—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            self.logger.info(f"üîß RuT5 device: {next(model.parameters()).device}, inputs device: {inputs['input_ids'].device}")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=min(len(inputs['input_ids'][0]) + 200, self.config.classification_max_length),
                    temperature=self.config.temperature,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = generated_text[json_start:json_end]
                return json.loads(json_text)
            else:
                return {"document_type": "unknown", "confidence": 0.0}
                
        except Exception as e:
            self.logger.error(f"Classification error: {e}")
            return {"document_type": "unknown", "confidence": 0.0}
    
    def _extract_metadata_with_qa(self, content: str, structural_data: Dict) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é QA –ø–æ–¥—Ö–æ–¥–∞"""
        metadata = {}
        
        # –°–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        qa_questions = [
            ("–î–∞—Ç–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞", "date_approval"),
            ("–ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞", "document_number"),
            ("–ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏", "organization"),
            ("–î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è –≤ –¥–µ–π—Å—Ç–≤–∏–µ", "date_effective"),
            ("–û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è", "scope"),
            ("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞", "keywords")
        ]
        
        for question, field in qa_questions:
            try:
                # –°–æ–∑–¥–∞–µ–º QA –ø—Ä–æ–º–ø—Ç
                qa_prompt = f"""–ù–∞–π–¥–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}

–î–û–ö–£–ú–ï–ù–¢:
{content[:2000]}

–û—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞):"""
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                inputs = self.extraction_tokenizer(qa_prompt, return_tensors="pt", max_length=self.config.extraction_max_length, truncation=True)
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å)
                if not hasattr(self.extraction_model, 'hf_quantizer'):
                    inputs = {k: v.to(self.extraction_model.device) for k, v in inputs.items()}
                else:
                    inputs = {k: v for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.extraction_model.generate(
                        **inputs,
                        max_length=min(len(inputs['input_ids'][0]) + 100, self.config.extraction_max_length),
                        temperature=0.1,
                        do_sample=True,
                        pad_token_id=self.extraction_tokenizer.eos_token_id
                    )
                
                response = self.extraction_tokenizer.decode(outputs[0], skip_special_tokens=True)
                answer = response[len(qa_prompt):].strip()
                
                if answer and len(answer) < 200:  # –†–∞–∑—É–º–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
                    metadata[field] = answer
                    
            except Exception as e:
                self.logger.warning(f"Failed to extract {field}: {e}")
                continue
        
        return metadata
    
    def _deep_analysis_with_llm(self, content: str, vlm_metadata: Dict) -> Dict:
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å –ø–æ–º–æ—â—å—é LLM"""
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            prompt = self._prepare_analysis_prompt(content, vlm_metadata)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å LLM
            inputs = self.analysis_tokenizer(prompt, return_tensors="pt", max_length=self.config.analysis_max_length, truncation=True)
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ –∫–≤–∞–Ω—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å)
            if not hasattr(self.analysis_model, 'hf_quantizer'):
                inputs = {k: v.to(self.analysis_model.device) for k, v in inputs.items()}
            else:
                inputs = {k: v for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.analysis_model.generate(
                    **inputs,
                    max_length=min(len(inputs['input_ids'][0]) + 500, self.config.analysis_max_length),
                    temperature=self.config.temperature,
                    do_sample=True,
                    pad_token_id=self.analysis_tokenizer.eos_token_id
                )
            
            response = self.analysis_tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = generated_text[json_start:json_end]
                return json.loads(json_text)
            else:
                return {"sections": [], "entities": [], "relations": []}
                
        except Exception as e:
            self.logger.error(f"Deep analysis error: {e}")
            return {"sections": [], "entities": [], "relations": []}
    
    def _prepare_analysis_prompt(self, content: str, vlm_metadata: Dict) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        prompt = f"""–ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

–î–û–ö–£–ú–ï–ù–¢:
{content[:3000]}

VLM –ê–ù–ê–õ–ò–ó:
- –¢–∞–±–ª–∏—Ü—ã: {len(vlm_metadata.get('tables', []))}
- –°–µ–∫—Ü–∏–∏: {len(vlm_metadata.get('sections', []))}

–ó–ê–î–ê–ß–ò:
1. –ò–∑–≤–ª–µ–∫–∏ –≤—Å–µ —Å–µ–∫—Ü–∏–∏ –∏ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
2. –ù–∞–π–¥–∏ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–¥–∞—Ç—ã, –Ω–æ–º–µ—Ä–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)
3. –û–ø—Ä–µ–¥–µ–ª–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (JSON):
{{
    "sections": [
        {{"title": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "content": "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ", "level": 1}}
    ],
    "entities": [
        {{"text": "—Å—É—â–Ω–æ—Å—Ç—å", "type": "—Ç–∏–ø", "confidence": 0.9}}
    ],
    "relations": [
        {{"source": "–∏—Å—Ç–æ—á–Ω–∏–∫", "target": "—Ü–µ–ª—å", "type": "—Å–≤—è–∑—å"}}
    ]
}}

–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ JSON."""
        
        return prompt
    
    def get_ensemble_info(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–Ω—Å–∞–º–±–ª–µ"""
        return {
            'classification_model': self.config.classification_model,
            'extraction_model': self.config.extraction_model,
            'analysis_model': self.config.analysis_model,
            'classification_loaded': self.classification_model is not None,
            'extraction_loaded': self.extraction_model is not None,
            'analysis_loaded': self.analysis_model is not None,
            'device': str(self.classification_model.device) if self.classification_model is not None else 'unknown'
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞–Ω—Å–∞–º–±–ª—è
    config = RussianLLMEnsembleConfig(
        classification_model="ai-forever/rugpt3large_based_on_gpt2",
        extraction_model="ai-forever/rugpt3large_based_on_gpt2", 
        analysis_model="ai-forever/rugpt3large_based_on_gpt2",
        device="auto",
        use_quantization=True
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    ensemble = RussianLLMEnsemble(config)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    test_content = "–°–ù–∏–ü 2.01.07-85* –ù–∞–≥—Ä—É–∑–∫–∏ –∏ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è. –£—Ç–≤–µ—Ä–∂–¥–µ–Ω 29.12.2020."
    
    # Stage 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    classification = ensemble.stage_4_classify_document(test_content)
    print(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {classification}")
    
    # Stage 8: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata = ensemble.stage_8_extract_metadata(test_content, {})
    print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata}")
    
    # Stage 5.5: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
    analysis = ensemble.stage_5_5_deep_analysis(test_content, {})
    print(f"–ê–Ω–∞–ª–∏–∑: {analysis}")
    
    print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω—Å–∞–º–±–ª–µ: {ensemble.get_ensemble_info()}")
