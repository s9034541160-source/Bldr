#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üîß WORKING FRONTEND RAG INTEGRATION V3 
======================================
–†–∞–±–æ—á–∞—è –≤–µ—Ä—Å–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ —Å Enhanced RAG Trainer
—Å –ø–æ–ª–Ω—ã–º fallback –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é

–ö–õ–Æ–ß–ï–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
‚úÖ –†–∞–±–æ—Ç–∞—é—â–∏–π fallback —Ä–µ–∂–∏–º
‚úÖ –ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º  
‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
‚úÖ –ù–∞–¥–µ–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
‚úÖ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ–¥–∞–∫—à–µ–Ω—É
"""

import os
import json
import logging
import re
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
import hashlib

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(levelname)s:%(name)s:%(message)s')
logger = logging.getLogger(__name__)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
try:
    from integrated_structure_chunking_system import IntegratedStructureChunkingSystem
    INTEGRATED_SYSTEM_AVAILABLE = True
    logger.info("‚úÖ Integrated Structure & Chunking System available")
except ImportError:
    INTEGRATED_SYSTEM_AVAILABLE = False
    logger.warning("‚ö†Ô∏è Integrated system not available, using fallback")

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Enhanced RAG Trainer
try:
    from complete_enhanced_bldr_rag_trainer import CompleteEnhancedBldrRAGTrainer
    ENHANCED_TRAINER_AVAILABLE = True
    logger.info("‚úÖ Enhanced RAG Trainer available")
except ImportError:
    ENHANCED_TRAINER_AVAILABLE = False
    logger.warning("‚ö†Ô∏è Enhanced RAG Trainer not available, using fallback")

class WorkingFrontendRAGProcessor:
    """
    üîß –†–∞–±–æ—á–∏–π Frontend-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π RAG –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ä–∞–±–æ—á–∞—è –≤–µ—Ä—Å–∏—è —Å –ø–æ–ª–Ω—ã–º fallback
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –Ω–∞–¥–µ–∂–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É
        if INTEGRATED_SYSTEM_AVAILABLE:
            try:
                self.intelligent_system = IntegratedStructureChunkingSystem()
                self.use_intelligent_chunking = True
                logger.info("üß© Using intelligent structure-based chunking")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to initialize intelligent system: {e}")
                self.intelligent_system = None
                self.use_intelligent_chunking = False
        else:
            self.intelligent_system = None
            self.use_intelligent_chunking = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Enhanced trainer
        if ENHANCED_TRAINER_AVAILABLE:
            try:
                self.enhanced_trainer = CompleteEnhancedBldrRAGTrainer()
                self.use_enhanced_trainer = True
                logger.info("üöÄ Using Enhanced RAG Trainer")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to initialize enhanced trainer: {e}")
                self.enhanced_trainer = None
                self.use_enhanced_trainer = False
        else:
            self.enhanced_trainer = None
            self.use_enhanced_trainer = False
        
        logger.info(f"‚úÖ Processor initialized - Intelligent: {self.use_intelligent_chunking}, Enhanced: {self.use_enhanced_trainer}")
    
    def process_document_for_frontend(self, content: str, file_path: str = "", 
                                    additional_metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """
        üîß –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î: –ù–∞–¥–µ–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        
        –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
        """
        
        try:
            logger.info(f"üìÑ Processing document: {Path(file_path).name if file_path else 'unnamed'}")
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É
            if self.use_intelligent_chunking:
                try:
                    result = self._process_with_intelligent_system(content, file_path, additional_metadata)
                    logger.info("‚úÖ Processed with intelligent system")
                    return self._adapt_for_frontend_api(result, file_path)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Intelligent processing failed: {e}, falling back to basic")
            
            # Fallback –∫ –±–∞–∑–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ
            result = self._process_with_reliable_fallback(content, file_path, additional_metadata)
            logger.info("‚úÖ Processed with fallback system")
            return self._adapt_for_frontend_api(result, file_path)
            
        except Exception as e:
            logger.error(f"‚ùå All processing methods failed: {e}")
            return self._create_error_response(str(e), file_path)
    
    def _process_with_intelligent_system(self, content: str, file_path: str, 
                                       additional_metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É
        result = self.intelligent_system.process_document(content, file_path)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if additional_metadata:
            result['document_info'].update(additional_metadata)
        
        return result
    
    def _process_with_reliable_fallback(self, content: str, file_path: str,
                                      additional_metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """–ù–∞–¥–µ–∂–Ω–∞—è fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)"""
        
        logger.info("Using reliable fallback processing")
        
        # –ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        document_info = self._extract_reliable_metadata(content, file_path)
        if additional_metadata:
            document_info.update(additional_metadata)
        
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤
        sections = self._extract_improved_sections(content)
        
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤
        chunks = self._create_improved_chunks(content, sections)
        
        # –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
        tables = self._extract_improved_tables(content)
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤
        lists = self._extract_lists(content)
        
        return {
            'document_info': document_info,
            'sections': sections,
            'chunks': chunks,
            'tables': tables,
            'lists': lists,
            'statistics': {
                'content_length': len(content),
                'word_count': len(content.split()),
                'paragraph_count': len([p for p in content.split('\n\n') if p.strip()]),
                'chunks_created': len(chunks),
                'avg_chunk_quality': self._calculate_average_chunk_quality(chunks),
                'chunk_types_distribution': self._get_chunk_types_distribution(chunks)
            },
            'processing_info': {
                'extracted_at': datetime.now().isoformat(),
                'processor_version': 'WorkingFallback_v3.0',
                'structure_quality': self._calculate_structure_quality(sections, tables),
                'chunking_quality': self._calculate_chunking_quality(chunks),
                'processing_method': 'improved_fallback'
            }
        }
    
    def _extract_reliable_metadata(self, content: str, file_path: str) -> Dict[str, Any]:
        """–ù–∞–¥–µ–∂–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤
        title_patterns = [
            r'^([–ê-–Ø–Å][–ê-–Ø–Å\s]{15,100})',  # –ó–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–≥–ª–∞–≤–Ω—ã–º–∏
            r'^\d+\.\s*([–ê-–Ø–Å][–ê-–Ø–Å\s]{10,80})',  # –ü–æ—Å–ª–µ –Ω–æ–º–µ—Ä–∞ —Ä–∞–∑–¥–µ–ª–∞
            r'(–°–ü|–ì–û–°–¢|–°–ù–∏–ü)\s+[\d.-]+\s*([–ê-–Ø–Å][^\n]{10,100})'  # –ü–æ—Å–ª–µ –Ω–æ–º–µ—Ä–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞
        ]
        
        title = ''
        for pattern in title_patterns:
            match = re.search(pattern, content, re.MULTILINE)
            if match:
                title = match.group(1) if len(match.groups()) == 1 else match.group(2)
                title = title.strip()
                break
        
        # –ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
        number_patterns = [
            r'(–°–ü\s+[\d.-]+)',
            r'(–ì–û–°–¢\s+[\d.-]+)',
            r'(–°–ù–∏–ü\s+[\d.-]+)',
            r'(?:‚Ññ\s*|–Ω–æ–º–µ—Ä\s*)([\d.-]+)'
        ]
        
        number = ''
        doc_type = 'unknown'
        for pattern in number_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                number = match.group(1)
                if '–°–ü' in number.upper():
                    doc_type = '–°–ü'
                elif '–ì–û–°–¢' in number.upper():
                    doc_type = '–ì–û–°–¢'
                elif '–°–ù–ò–ü' in number.upper():
                    doc_type = '–°–ù–∏–ü'
                break
        
        # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è
        org_patterns = [
            r'(–ú–∏–Ω—Å—Ç—Ä–æ–π\s+–†–æ—Å—Å–∏–∏)',
            r'(–ì–æ—Å—Å—Ç—Ä–æ–π\s+–†–æ—Å—Å–∏–∏)',
            r'(–†–æ—Å—Å—Ç–∞–Ω–¥–∞—Ä—Ç)',
            r'(–§–ì–ë–£\s+[^\\n]+)',
        ]
        
        organization = ''
        for pattern in org_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                organization = match.group(1)
                break
        
        # –î–∞—Ç–∞
        date_patterns = [
            r'(\d{2}\.\d{2}\.\d{4})',
            r'(\d{4}-\d{2}-\d{2})',
            r'(\d{1,2}\s+\w+\s+\d{4})'
        ]
        
        date = ''
        for pattern in date_patterns:
            match = re.search(pattern, content)
            if match:
                date = match.group(1)
                break
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = self._extract_keywords(content)
        
        return {
            'title': title,
            'number': number,
            'type': doc_type,
            'organization': organization,
            'approval_date': date,
            'file_name': Path(file_path).name if file_path else '',
            'file_size': len(content.encode('utf-8')),
            'keywords': keywords
        }
    
    def _extract_improved_sections(self, content: str) -> List[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤"""
        
        sections = []
        
        # –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–æ–≤
        section_patterns = [
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã: "1. –ù–ê–ó–í–ê–ù–ò–ï"
            (r'^(\d+)\.\s+([–ê-–Ø–Å][–ê-–Ø–Å\s]{5,80})', 1),
            # –ü–æ–¥—Ä–∞–∑–¥–µ–ª—ã: "1.1 –ù–∞–∑–≤–∞–Ω–∏–µ"  
            (r'^(\d+\.\d+)\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s]{5,80})', 2),
            # –ü–æ–¥–ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã: "1.1.1 –ù–∞–∑–≤–∞–Ω–∏–µ"
            (r'^(\d+\.\d+\.\d+)\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s]{5,80})', 3),
        ]
        
        for pattern, level in section_patterns:
            matches = re.finditer(pattern, content, re.MULTILINE)
            for match in matches:
                number = match.group(1)
                title = match.group(2).strip()
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—É—é –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ä–∞–∑–¥–µ–ª–∞
                section_start = match.end()
                next_section = re.search(r'^\d+\.', content[section_start:], re.MULTILINE)
                section_end = section_start + next_section.start() if next_section else len(content)
                content_length = section_end - section_start
                
                section = {
                    'id': f'section_{len(sections)+1}',
                    'number': number,
                    'title': title,
                    'level': level,
                    'type': 'section',
                    'content_length': content_length,
                    'has_subsections': any(s['number'].startswith(number + '.') for s in sections),
                    'parent_path': '.'.join(number.split('.')[:-1]) if '.' in number else '',
                    'start_position': match.start(),
                    'end_position': section_end
                }
                
                sections.append(section)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–∑–∏—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ
        sections.sort(key=lambda x: x['start_position'])
        
        return sections[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑—É–º–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
    
    def _create_improved_chunks(self, content: str, sections: List[Dict]) -> List[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å —É—á–µ—Ç–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        
        chunks = []
        
        if sections:
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–¥–µ–ª–æ–≤
            for i, section in enumerate(sections):
                section_start = section['start_position']
                section_end = section['end_position']
                section_content = content[section_start:section_end]
                
                # –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
                if len(section_content) > 1200:
                    sub_chunks = self._split_large_section(section_content, section['number'])
                    chunks.extend(sub_chunks)
                else:
                    chunk = {
                        'id': f'chunk_section_{i+1}',
                        'content': section_content.strip(),
                        'type': 'section_content',
                        'source_elements': [f"Section {section['number']}"],
                        'metadata': {
                            'section_number': section['number'],
                            'section_title': section['title'],
                            'section_level': section['level']
                        },
                        'quality_score': self._assess_chunk_quality(section_content),
                        'word_count': len(section_content.split()),
                        'char_count': len(section_content),
                        'has_tables': '|' in section_content or '–¢–∞–±–ª–∏—Ü–∞' in section_content,
                        'has_lists': bool(re.search(r'^[-‚Ä¢*]\s', section_content, re.MULTILINE)),
                        'technical_terms': len(re.findall(r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü|–ú–ü–∞|–∫–≥|–º¬≤|¬∞C)\b', section_content, re.IGNORECASE))
                    }
                    chunks.append(chunk)
        else:
            # Fallback: —Å–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ —Ä–∞–∑–º–µ—Ä–Ω—ã–º –º–µ—Ç–æ–¥–æ–º
            chunks = self._create_size_based_chunks(content)
        
        return chunks
    
    def _split_large_section(self, section_content: str, section_number: str) -> List[Dict]:
        """–†–∞–∑–±–∏–µ–Ω–∏–µ –±–æ–ª—å—à–∏—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –Ω–∞ –ø–æ–¥—á–∞–Ω–∫–∏"""
        
        sub_chunks = []
        paragraphs = [p.strip() for p in section_content.split('\n\n') if p.strip()]
        
        current_chunk = ""
        chunk_counter = 1
        
        for paragraph in paragraphs:
            if len(current_chunk) + len(paragraph) > 800 and current_chunk:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                chunk = {
                    'id': f'chunk_{section_number}_{chunk_counter}',
                    'content': current_chunk.strip(),
                    'type': 'section_part',
                    'source_elements': [f"Section {section_number}, part {chunk_counter}"],
                    'metadata': {
                        'section_number': section_number,
                        'part_number': chunk_counter
                    },
                    'quality_score': self._assess_chunk_quality(current_chunk),
                    'word_count': len(current_chunk.split()),
                    'char_count': len(current_chunk),
                    'has_tables': '|' in current_chunk or '–¢–∞–±–ª–∏—Ü–∞' in current_chunk,
                    'has_lists': bool(re.search(r'^[-‚Ä¢*]\s', current_chunk, re.MULTILINE)),
                    'technical_terms': len(re.findall(r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü|–ú–ü–∞|–∫–≥|–º¬≤|¬∞C)\b', current_chunk, re.IGNORECASE))
                }
                sub_chunks.append(chunk)
                
                current_chunk = paragraph
                chunk_counter += 1
            else:
                current_chunk += "\n\n" + paragraph if current_chunk else paragraph
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunk = {
                'id': f'chunk_{section_number}_{chunk_counter}',
                'content': current_chunk.strip(),
                'type': 'section_part',
                'source_elements': [f"Section {section_number}, part {chunk_counter}"],
                'metadata': {
                    'section_number': section_number,
                    'part_number': chunk_counter
                },
                'quality_score': self._assess_chunk_quality(current_chunk),
                'word_count': len(current_chunk.split()),
                'char_count': len(current_chunk),
                'has_tables': '|' in current_chunk or '–¢–∞–±–ª–∏—Ü–∞' in current_chunk,
                'has_lists': bool(re.search(r'^[-‚Ä¢*]\s', current_chunk, re.MULTILINE)),
                'technical_terms': len(re.findall(r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü|–ú–ü–∞|–∫–≥|–º¬≤|¬∞C)\b', current_chunk, re.IGNORECASE))
            }
            sub_chunks.append(chunk)
        
        return sub_chunks
    
    def _create_size_based_chunks(self, content: str) -> List[Dict]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω—ã–º –º–µ—Ç–æ–¥–æ–º (fallback)"""
        
        chunks = []
        chunk_size = 800
        overlap = 100
        
        for i in range(0, len(content), chunk_size - overlap):
            chunk_text = content[i:i + chunk_size].strip()
            if len(chunk_text) >= 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
                chunk = {
                    'id': f'chunk_size_{i//chunk_size + 1}',
                    'content': chunk_text,
                    'type': 'size_based',
                    'source_elements': [f"Position {i}-{i+len(chunk_text)}"],
                    'metadata': {
                        'start_position': i,
                        'end_position': i + len(chunk_text)
                    },
                    'quality_score': self._assess_chunk_quality(chunk_text),
                    'word_count': len(chunk_text.split()),
                    'char_count': len(chunk_text),
                    'has_tables': '|' in chunk_text or '–¢–∞–±–ª–∏—Ü–∞' in chunk_text,
                    'has_lists': bool(re.search(r'^[-‚Ä¢*]\s', chunk_text, re.MULTILINE)),
                    'technical_terms': len(re.findall(r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü|–ú–ü–∞|–∫–≥|–º¬≤|¬∞C)\b', chunk_text, re.IGNORECASE))
                }
                chunks.append(chunk)
        
        return chunks
    
    def _extract_improved_tables(self, content: str) -> List[Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
        
        tables = []
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∞–±–ª–∏—Ü
        table_patterns = [
            # "–¢–∞–±–ª–∏—Ü–∞ 1 - –ù–∞–∑–≤–∞–Ω–∏–µ"
            r'–¢–∞–±–ª–∏—Ü–∞\s+(\d+)(?:\s*[-‚Äì‚Äî]\s*([^\n]+))?\n((?:[^\n]*\|[^\n]*\n?)+)',
            # –ü—Ä–æ—Å—Ç–æ —Ç–∞–±–ª–∏—Ü—ã —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏
            r'((?:[^\n]*\|[^\n]*\n?){3,})',
        ]
        
        for pattern_idx, pattern in enumerate(table_patterns):
            matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)
            
            for match_idx, match in enumerate(matches):
                if pattern_idx == 0:  # –° –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                    number = match.group(1)
                    title = match.group(2) if match.group(2) else f'–¢–∞–±–ª–∏—Ü–∞ {number}'
                    table_content = match.group(3)
                else:  # –ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    number = str(len(tables) + 1)
                    title = f'–¢–∞–±–ª–∏—Ü–∞ {number}'
                    table_content = match.group(1)
                
                # –ü–∞—Ä—Å–∏–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–±–ª–∏—Ü—ã
                headers, rows = self._parse_table_content(table_content)
                
                if headers or rows:
                    table = {
                        'id': f'table_{len(tables)+1}',
                        'number': number,
                        'title': title.strip(),
                        'headers': headers,
                        'rows': rows,
                        'page_number': 0,  # –ü–æ–∫–∞ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                        'metadata': {
                            'source_line': content[:match.start()].count('\n'),
                            'table_type': 'structured' if headers else 'simple',
                            'column_count': len(headers) if headers else (max(len(row) for row in rows) if rows else 0),
                            'row_count': len(rows)
                        }
                    }
                    tables.append(table)
        
        return tables
    
    def _parse_table_content(self, table_content: str) -> Tuple[List[str], List[List[str]]]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ç–∞–±–ª–∏—Ü—ã"""
        
        lines = [line.strip() for line in table_content.split('\n') if line.strip() and '|' in line]
        headers = []
        rows = []
        
        for i, line in enumerate(lines):
            # –£–¥–∞–ª—è–µ–º –∫—Ä–∞–π–Ω–∏–µ |
            line = line.strip('|')
            cells = [cell.strip() for cell in line.split('|')]
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —è—á–µ–π–∫–∏
            cells = [cell for cell in cells if cell]
            
            if not cells:
                continue
            
            # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å—á–∏—Ç–∞–µ—Ç—Å—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–º, –µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –±—É–∫–≤—ã
            if i == 0 and any(re.search(r'[–ê-–Ø–∞-—èA-Za-z]', cell) for cell in cells):
                headers = cells
            else:
                rows.append(cells)
        
        return headers, rows
    
    def _extract_lists(self, content: str) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤"""
        
        lists = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ø–∏—Å–∫–æ–≤
        list_patterns = [
            # –ú–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            (r'(?:^[-‚Ä¢*]\s+.+\n?)+', 'bulleted'),
            # –ù—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            (r'(?:^\d+\.\s+.+\n?)+', 'numbered'),
            # –ë—É–∫–≤–µ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
            (r'(?:^[–∞-—è]\)\s+.+\n?)+', 'lettered'),
        ]
        
        for pattern, list_type in list_patterns:
            matches = re.finditer(pattern, content, re.MULTILINE)
            
            for match_idx, match in enumerate(matches):
                list_content = match.group(0)
                items = []
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å–ø–∏—Å–∫–∞
                for line in list_content.split('\n'):
                    line = line.strip()
                    if not line:
                        continue
                    
                    # –£–¥–∞–ª—è–µ–º –º–∞—Ä–∫–µ—Ä—ã
                    if list_type == 'bulleted':
                        item = re.sub(r'^[-‚Ä¢*]\s+', '', line)
                    elif list_type == 'numbered':
                        item = re.sub(r'^\d+\.\s+', '', line)
                    elif list_type == 'lettered':
                        item = re.sub(r'^[–∞-—è]\)\s+', '', line)
                    else:
                        item = line
                    
                    if item:
                        items.append(item)
                
                if items:
                    list_obj = {
                        'id': f'list_{len(lists)+1}',
                        'type': list_type,
                        'items': items,
                        'level': 1,  # –ü–æ–∫–∞ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —É—Ä–æ–≤–µ–Ω—å
                        'metadata': {
                            'source_line': content[:match.start()].count('\n'),
                            'item_count': len(items)
                        }
                    }
                    lists.append(list_obj)
        
        return lists
    
    def _extract_keywords(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        
        keywords = []
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        tech_patterns = [
            r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü)\s+[\d.-]+',
            r'\b\d+(?:\.\d+)?\s*(?:–º–º|—Å–º|–º|–∫–º|–∫–≥|—Ç|–ú–ü–∞|¬∞C|–∫–í—Ç|–í—Ç)\b',
            r'\b(?:—Ç–µ–ø–ª–æ–ø—Ä–æ–≤–æ–¥–Ω–æ—Å—Ç—å|—Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ|–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç|—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞|–≤–ª–∞–∂–Ω–æ—Å—Ç—å)\b',
        ]
        
        for pattern in tech_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            keywords.extend([m.strip() for m in matches])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        return list(dict.fromkeys(keywords))[:15]
    
    def _assess_chunk_quality(self, chunk_content: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–∞"""
        
        quality = 0.5  # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É (–æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è 300-800 —Å–∏–º–≤–æ–ª–æ–≤)
        length = len(chunk_content)
        if 300 <= length <= 800:
            quality += 0.2
        elif 200 <= length < 300 or 800 < length <= 1000:
            quality += 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        tech_terms = len(re.findall(r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü|–ú–ü–∞|–∫–≥|–º¬≤|¬∞C)\b', chunk_content, re.IGNORECASE))
        quality += min(tech_terms * 0.05, 0.2)
        
        # –ë–æ–Ω—É—Å –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
        if any(marker in chunk_content for marker in ['1.', '2.', '–∞)', '–±)', '-', '‚Ä¢']):
            quality += 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ç–∞–±–ª–∏—Ü—ã
        if '|' in chunk_content or '–¢–∞–±–ª–∏—Ü–∞' in chunk_content:
            quality += 0.1
        
        # –®—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏
        if length < 50:
            quality -= 0.3
        
        return min(max(quality, 0.0), 1.0)
    
    def _calculate_average_chunk_quality(self, chunks: List[Dict]) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–æ–≤"""
        if not chunks:
            return 0.0
        
        total_quality = sum(chunk.get('quality_score', 0) for chunk in chunks)
        return total_quality / len(chunks)
    
    def _get_chunk_types_distribution(self, chunks: List[Dict]) -> Dict[str, int]:
        """–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —á–∞–Ω–∫–æ–≤"""
        distribution = {}
        for chunk in chunks:
            chunk_type = chunk.get('type', 'unknown')
            distribution[chunk_type] = distribution.get(chunk_type, 0) + 1
        return distribution
    
    def _calculate_structure_quality(self, sections: List[Dict], tables: List[Dict]) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        quality = 0.5
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        if sections:
            quality += min(len(sections) * 0.02, 0.3)
        
        # –ë–æ–Ω—É—Å –∑–∞ –∏–µ—Ä–∞—Ä—Ö–∏—é —Ä–∞–∑–¥–µ–ª–æ–≤
        levels = set(s.get('level', 1) for s in sections)
        if len(levels) > 1:
            quality += 0.1
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ç–∞–±–ª–∏—Ü—ã
        if tables:
            quality += min(len(tables) * 0.05, 0.2)
        
        return min(quality, 1.0)
    
    def _calculate_chunking_quality(self, chunks: List[Dict]) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–∏–Ω–≥–∞"""
        if not chunks:
            return 0.0
        
        # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ + –±–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ç–∏–ø–æ–≤
        avg_quality = self._calculate_average_chunk_quality(chunks)
        type_diversity = len(set(c.get('type', '') for c in chunks))
        diversity_bonus = min(type_diversity * 0.05, 0.15)
        
        return min(avg_quality + diversity_bonus, 1.0)
    
    def _adapt_for_frontend_api(self, result: Dict[str, Any], file_path: str) -> Dict[str, Any]:
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ API"""
        
        return {
            # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            "document_info": {
                "id": self._generate_document_id(file_path),
                "title": result['document_info'].get('title', ''),
                "number": result['document_info'].get('number', ''),
                "type": result['document_info'].get('type', 'unknown'),
                "organization": result['document_info'].get('organization', ''),
                "date": result['document_info'].get('approval_date', ''),
                "file_name": result['document_info'].get('file_name', Path(file_path).name if file_path else ''),
                "file_size": result['document_info'].get('file_size', 0),
                "keywords": result['document_info'].get('keywords', []),
                "status": "processed",
                "processing_time": datetime.now().isoformat()
            },
            
            # –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–∑–¥–µ–ª–æ–≤
            "sections": self._format_sections_for_navigation(result.get('sections', [])),
            
            # –ß–∞–Ω–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è RAG
            "chunks": self._format_chunks_for_rag(result.get('chunks', [])),
            
            # –¢–∞–±–ª–∏—Ü—ã –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            "tables": self._format_tables_for_frontend(result.get('tables', [])),
            
            # –°–ø–∏—Å–∫–∏
            "lists": result.get('lists', []),
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è UI
            "statistics": {
                "content_stats": {
                    "total_characters": result['statistics'].get('content_length', 0),
                    "total_words": result['statistics'].get('word_count', 0),
                    "total_sections": len(result.get('sections', [])),
                    "total_paragraphs": result['statistics'].get('paragraph_count', 0),
                    "total_tables": len(result.get('tables', [])),
                    "total_lists": len(result.get('lists', []))
                },
                "processing_stats": {
                    "chunks_created": result['statistics'].get('chunks_created', 0),
                    "avg_chunk_quality": result['statistics'].get('avg_chunk_quality', 0),
                    "chunk_types": result['statistics'].get('chunk_types_distribution', {}),
                    "structure_quality": result['processing_info'].get('structure_quality', 0),
                    "chunking_quality": result['processing_info'].get('chunking_quality', 0)
                }
            },
            
            # –ú–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            "processing_info": {
                "processor_version": result['processing_info'].get('processor_version', 'Unknown'),
                "processing_method": result['processing_info'].get('processing_method', 'unknown'),
                "extraction_quality": result['processing_info'].get('structure_quality', 0),
                "processing_time": result['processing_info'].get('extracted_at', ''),
                "features_used": {
                    "intelligent_chunking": self.use_intelligent_chunking,
                    "enhanced_trainer": self.use_enhanced_trainer,
                    "structure_extraction": True,
                    "table_extraction": len(result.get('tables', [])) > 0,
                    "list_extraction": len(result.get('lists', [])) > 0
                }
            }
        }
    
    def _format_sections_for_navigation(self, sections: List[Dict]) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏"""
        return [
            {
                "id": section.get('id', ''),
                "number": section.get('number', ''),
                "title": section.get('title', ''),
                "level": section.get('level', 1),
                "has_content": section.get('content_length', 0) > 0,
                "has_subsections": section.get('has_subsections', False),
                "parent_path": section.get('parent_path', ''),
                "metadata": {
                    "word_count": section.get('content_length', 0) // 5,
                    "section_type": section.get('type', 'section')
                }
            }
            for section in sections
        ]
    
    def _format_chunks_for_rag(self, chunks: List[Dict]) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
        return [
            {
                "id": chunk.get('id', ''),
                "content": chunk.get('content', ''),
                "type": chunk.get('type', 'unknown'),
                "source_elements": chunk.get('source_elements', []),
                "metadata": {
                    **chunk.get('metadata', {}),
                    "word_count": chunk.get('word_count', 0),
                    "char_count": chunk.get('char_count', 0),
                    "quality_score": chunk.get('quality_score', 0),
                    "has_tables": chunk.get('has_tables', False),
                    "has_lists": chunk.get('has_lists', False),
                    "technical_terms_count": chunk.get('technical_terms', 0)
                },
                "search_metadata": {
                    "searchable_content": chunk.get('content', '')[:500],
                    "keywords": self._extract_chunk_keywords(chunk.get('content', '')),
                    "section_context": chunk.get('metadata', {}).get('section_number', ''),
                    "importance_score": self._calculate_chunk_importance(chunk)
                }
            }
            for chunk in chunks
        ]
    
    def _format_tables_for_frontend(self, tables: List[Dict]) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞"""
        return [
            {
                "id": table.get('id', f'table_{i+1}'),
                "number": table.get('number', str(i+1)),
                "title": table.get('title', f'–¢–∞–±–ª–∏—Ü–∞ {i+1}'),
                "headers": table.get('headers', []),
                "rows": table.get('rows', []),
                "metadata": {
                    "row_count": len(table.get('rows', [])),
                    "column_count": len(table.get('headers', [])),
                    "page_number": table.get('page_number', 0),
                    "is_structured": len(table.get('headers', [])) > 0
                },
                "display_options": {
                    "show_headers": len(table.get('headers', [])) > 0,
                    "searchable": True,
                    "exportable": True,
                    "max_display_rows": 100
                }
            }
            for i, table in enumerate(tables)
        ]
    
    def _extract_chunk_keywords(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —á–∞–Ω–∫–∞"""
        keywords = []
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Å—Å—ã–ª–∫–∏
        tech_refs = re.findall(r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü)\s+[\d.-]+', content, re.IGNORECASE)
        keywords.extend(tech_refs)
        
        # –ò–∑–º–µ—Ä–µ–Ω–∏—è
        measurements = re.findall(r'\b\d+(?:\.\d+)?\s*(?:–º–º|—Å–º|–º|–∫–º|–∫–≥|—Ç|–ú–ü–∞|¬∞C)\b', content, re.IGNORECASE)
        keywords.extend([m.strip() for m in measurements])
        
        return list(dict.fromkeys(keywords))[:8]
    
    def _calculate_chunk_importance(self, chunk: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ —á–∞–Ω–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        importance = 0.5
        
        quality = chunk.get('quality_score', 0)
        importance += quality * 0.3
        
        if chunk.get('has_tables', False):
            importance += 0.2
        
        tech_terms = chunk.get('technical_terms', 0)
        if tech_terms > 0:
            importance += min(tech_terms / 10.0, 0.2)
        
        return min(importance, 1.0)
    
    def _generate_document_id(self, file_path: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        if not file_path:
            return "doc_" + hashlib.md5(str(datetime.now()).encode()).hexdigest()[:8]
        return "doc_" + hashlib.md5(file_path.encode()).hexdigest()[:8]
    
    def _create_error_response(self, error_message: str, file_path: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ"""
        return {
            "document_info": {
                "id": "error",
                "title": "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞",
                "file_name": Path(file_path).name if file_path else "unknown",
                "status": "error",
                "error_message": error_message,
                "processing_time": datetime.now().isoformat()
            },
            "sections": [],
            "chunks": [],
            "tables": [],
            "lists": [],
            "statistics": {
                "content_stats": {},
                "processing_stats": {}
            },
            "processing_info": {
                "processor_version": "Error_v1.0",
                "processing_method": "error_fallback",
                "extraction_quality": 0.0,
                "features_used": {}
            }
        }

class WorkingEnhancedRAGTrainer:
    """
    üöÄ –†–∞–±–æ—á–∞—è –≤–µ—Ä—Å–∏—è Enhanced RAG Trainer —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
    
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
    """
    
    def __init__(self, use_intelligent_chunking: bool = True, **kwargs):
        
        self.use_intelligent_chunking = use_intelligent_chunking
        self.frontend_processor = WorkingFrontendRAGProcessor()
        
        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Enhanced trainer, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if ENHANCED_TRAINER_AVAILABLE:
            try:
                self.base_trainer = CompleteEnhancedBldrRAGTrainer(**kwargs)
                self.use_enhanced_trainer = True
                logger.info("‚úÖ Enhanced trainer initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Enhanced trainer failed: {e}")
                self.base_trainer = None
                self.use_enhanced_trainer = False
        else:
            self.base_trainer = None
            self.use_enhanced_trainer = False
        
        logger.info(f"üöÄ Working RAG Trainer initialized - Intelligent: {self.use_intelligent_chunking}, Enhanced: {self.use_enhanced_trainer}")
    
    def process_single_document(self, content: str, file_path: str = "") -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–≥–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥)"""
        
        logger.info(f"üìÑ Processing document: {Path(file_path).name if file_path else 'unnamed'}")
        
        result = self.frontend_processor.process_document_for_frontend(content, file_path)
        
        chunks_count = len(result.get('chunks', []))
        quality = result['processing_info'].get('extraction_quality', 0)
        
        logger.info(f"‚úÖ Document processed: {chunks_count} chunks, quality: {quality:.2f}")
        
        return result
    
    def get_chunks_for_rag(self, content: str, file_path: str = "") -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
        result = self.process_single_document(content, file_path)
        return result.get('chunks', [])
    
    def train(self, max_files: Optional[int] = None, base_dir: Optional[str] = None) -> Dict[str, Any]:
        """–û–±—É—á–µ–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã"""
        
        logger.info("üöÄ Starting RAG training with improved chunking")
        
        if self.base_trainer and self.use_enhanced_trainer:
            try:
                result = self.base_trainer.train(max_files)
                logger.info("‚úÖ Enhanced RAG training completed")
                return result
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Enhanced training failed: {e}, using simplified training")
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
        return self._simplified_training(max_files, base_dir)
    
    def _simplified_training(self, max_files: Optional[int] = None, base_dir: Optional[str] = None) -> Dict[str, Any]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"""
        
        base_path = base_dir or os.getenv("BASE_DIR", "I:/docs")
        
        if not os.path.exists(base_path):
            logger.warning(f"‚ö†Ô∏è Base directory not found: {base_path}")
            return {
                'training_summary': {
                    'error': f'Base directory not found: {base_path}',
                    'documents_processed': 0,
                    'total_chunks_created': 0
                }
            }
        
        # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤
        all_files = []
        for ext in ['*.pdf', '*.docx', '*.txt']:
            try:
                files = list(Path(base_path).rglob(ext))
                all_files.extend([str(f) for f in files])
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error searching for {ext}: {e}")
        
        if max_files:
            all_files = all_files[:max_files]
        
        logger.info(f"Found {len(all_files)} files for processing")
        
        processed_count = 0
        total_chunks = 0
        error_count = 0
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        files_to_process = all_files[:min(10, len(all_files))]
        
        for file_path in files_to_process:
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è —á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                if content:
                    result = self.process_single_document(content, file_path)
                    total_chunks += len(result.get('chunks', []))
                    processed_count += 1
                    
                    logger.info(f"‚úÖ Processed: {Path(file_path).name}")
                
            except Exception as e:
                error_count += 1
                logger.error(f"‚ùå Failed to process {file_path}: {e}")
        
        logger.info(f"üìä Training completed: {processed_count} docs, {total_chunks} chunks, {error_count} errors")
        
        return {
            'training_summary': {
                'documents_processed': processed_count,
                'total_chunks_created': total_chunks,
                'errors': error_count,
                'processing_method': 'simplified_with_improved_chunking',
                'files_found': len(all_files),
                'files_attempted': len(files_to_process)
            }
        }

# API-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def process_document_api_compatible(content: str, file_path: str = "") -> Dict[str, Any]:
    """
    üîß API-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    """
    processor = WorkingFrontendRAGProcessor()
    return processor.process_document_for_frontend(content, file_path)

def get_document_structure_api(content: str, file_path: str = "") -> Dict[str, Any]:
    """üìä API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏)"""
    result = process_document_api_compatible(content, file_path)
    
    return {
        'document_info': result['document_info'],
        'sections': result['sections'],
        'statistics': result['statistics']['content_stats']
    }

def get_document_chunks_api(content: str, file_path: str = "") -> List[Dict[str, Any]]:
    """üß© API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã)"""
    result = process_document_api_compatible(content, file_path)
    return result['chunks']

def create_working_rag_trainer(**kwargs) -> WorkingEnhancedRAGTrainer:
    """üöÄ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞–±–æ—á–µ–≥–æ RAG —Ç—Ä–µ–Ω–µ—Ä–∞"""
    return WorkingEnhancedRAGTrainer(**kwargs)

if __name__ == "__main__":
    print("üîß Working Frontend RAG Integration v3 - Ready!")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
        trainer = create_working_rag_trainer(use_intelligent_chunking=True)
        
        # –¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –°–ü 50.13330.2012
        test_content = """
–°–ü 50.13330.2012
–¢–ï–ü–õ–û–í–ê–Ø –ó–ê–©–ò–¢–ê –ó–î–ê–ù–ò–ô

–ê–∫—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–µ–¥–∞–∫—Ü–∏—è –°–ù–∏–ü 23-02-2003

–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏

1. –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø

1.1 –ù–∞—Å—Ç–æ—è—â–∏–π —Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∑–∞—â–∏—Ç—ã –∑–¥–∞–Ω–∏–π —Ä–∞–∑–ª–∏—á–Ω–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è.

1.2 –°–≤–æ–¥ –ø—Ä–∞–≤–∏–ª –Ω–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∑–∞—â–∏—Ç—ã:
- –∑–¥–∞–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å–æ —Å—Ä–æ–∫–æ–º —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ –º–µ–Ω–µ–µ 2 –ª–µ—Ç;
- –∑–¥–∞–Ω–∏–π, –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –∂–∏–≤–æ—Ç–Ω—ã—Ö;
- –∑–¥–∞–Ω–∏–π –∫—É–ª—å—Ç–æ–≤—ã—Ö.

2. –ù–û–†–ú–ê–¢–ò–í–ù–´–ï –°–°–´–õ–ö–ò

–í –Ω–∞—Å—Ç–æ—è—â–µ–º —Å–≤–æ–¥–µ –ø—Ä–∞–≤–∏–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:
- –ì–û–°–¢ 30494-2011 –ó–¥–∞–Ω–∏—è –∂–∏–ª—ã–µ –∏ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∏–∫—Ä–æ–∫–ª–∏–º–∞—Ç–∞ –≤ –ø–æ–º–µ—â–µ–Ω–∏—è—Ö
- –°–ü 23-101-2004 –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∑–∞—â–∏—Ç—ã –∑–¥–∞–Ω–∏–π

–¢–∞–±–ª–∏—Ü–∞ 1 - –ù–æ—Ä–º–∏—Ä—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è —Ç–µ–ø–ª–æ–ø–µ—Ä–µ–¥–∞—á–µ
|–¢–∏–ø –∑–¥–∞–Ω–∏—è|–°–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–ø–µ—Ä–µ–¥–∞—á–µ, –º¬≤¬∑¬∞–°/–í—Ç|
|–ñ–∏–ª—ã–µ –∑–¥–∞–Ω–∏—è|3,5|
|–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–¥–∞–Ω–∏—è|2,8|
|–ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–¥–∞–Ω–∏—è|2,0|

3. –¢–ï–†–ú–ò–ù–´ –ò –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø

3.1 —Ç–µ–ø–ª–æ–≤–∞—è –∑–∞—â–∏—Ç–∞ –∑–¥–∞–Ω–∏—è: –°–æ–≤–æ–∫—É–ø–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö, –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö, –∏–Ω–∂–µ–Ω–µ—Ä–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∏ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫.

3.2 –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–µ —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–ø–µ—Ä–µ–¥–∞—á–µ: –°–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–ø–µ—Ä–µ–¥–∞—á–µ —Å —É—á–µ—Ç–æ–º —Ç–µ–ø–ª–æ—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –Ω–µ–æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç–µ–π.

–¢–∞–±–ª–∏—Ü–∞ 2 - –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏
|–£—Å–ª–æ–≤–∏—è|–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞, ¬∞C|
|–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ|18-22|
|–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ|20-24|
|–î–æ–ø—É—Å—Ç–∏–º—ã–µ|16-28|

4. –û–ë–©–ò–ï –¢–†–ï–ë–û–í–ê–ù–ò–Ø

4.1 –¢–µ–ø–ª–æ–≤–∞—è –∑–∞—â–∏—Ç–∞ –∑–¥–∞–Ω–∏—è –¥–æ–ª–∂–Ω–∞ –æ—Ç–≤–µ—á–∞—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º —ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.

–û—Å–Ω–æ–≤–Ω—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:
- –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —Ç–µ–ø–ª–æ–ø–æ—Ç–µ—Ä—å —á–µ—Ä–µ–∑ –æ–≥—Ä–∞–∂–¥–∞—é—â–∏–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏;
- –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω–¥–µ–Ω—Å–∞—Ç–∞ –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—è—Ö;
- –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–∏–∫—Ä–æ–∫–ª–∏–º–∞—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –ì–û–°–¢ 30494.
"""
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
        logger.info("üß™ Starting integration test...")
        result = trainer.process_single_document(test_content, "test_sp_50.13330.2012.pdf")
        
        print(f"\n‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:")
        print(f"  üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {result['document_info']['title']}")
        print(f"  üìä –ù–æ–º–µ—Ä: {result['document_info']['number']}")
        print(f"  üìù –¢–∏–ø: {result['document_info']['type']}")
        print(f"  üè¢ –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è: {result['document_info']['organization']}")
        print(f"  üìã –†–∞–∑–¥–µ–ª–æ–≤: {len(result['sections'])}")
        print(f"  üß© –ß–∞–Ω–∫–æ–≤: {len(result['chunks'])}")
        print(f"  üìä –¢–∞–±–ª–∏—Ü: {len(result['tables'])}")
        print(f"  üìÑ –°–ø–∏—Å–∫–æ–≤: {len(result['lists'])}")
        print(f"  üìà –ö–∞—á–µ—Å—Ç–≤–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {result['processing_info']['extraction_quality']:.2f}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —á–∞–Ω–∫–∞—Ö
        chunks = result['chunks']
        if chunks:
            print(f"\nüß© –ü—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤:")
            for i, chunk in enumerate(chunks[:3]):
                print(f"  {i+1}. [{chunk['type']}] {chunk['content'][:100]}...")
                print(f"     –ö–∞—á–µ—Å—Ç–≤–æ: {chunk['metadata']['quality_score']:.2f}, –°–ª–æ–≤: {chunk['metadata']['word_count']}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        tables = result['tables']
        if tables:
            print(f"\nüìä –ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã:")
            for table in tables:
                print(f"  - {table['title']}")
                print(f"    –ó–∞–≥–æ–ª–æ–≤–∫–∏: {table['headers']}")
                print(f"    –°—Ç—Ä–æ–∫: {len(table['rows'])}")
        
        print(f"\nüîß API —Ñ—É–Ω–∫—Ü–∏–∏ –≥–æ—Ç–æ–≤—ã:")
        print(f"  - process_document_api_compatible() ‚úÖ")
        print(f"  - get_document_structure_api() ‚úÖ") 
        print(f"  - get_document_chunks_api() ‚úÖ")
        print(f"  - create_working_rag_trainer() ‚úÖ")
        
        print(f"\nüéâ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º!")
        print(f"üìù –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'working_frontend_rag_integration.py' –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–µ API
        structure = get_document_structure_api(test_content, "test.pdf")
        chunks_only = get_document_chunks_api(test_content, "test.pdf")
        
        print(f"\nüìà API —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:")
        print(f"  - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∞: {len(structure['sections'])} —Ä–∞–∑–¥–µ–ª–æ–≤")
        print(f"  - –ß–∞–Ω–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω—ã: {len(chunks_only)} —á–∞–Ω–∫–æ–≤")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        print("‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º")