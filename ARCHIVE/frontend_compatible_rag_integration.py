#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üîß FRONTEND COMPATIBLE RAG INTEGRATION V3
========================================
–ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ –≤ Enhanced RAG Trainer
—Å –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º

–ö–õ–Æ–ß–ï–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
‚úÖ –ó–∞–º–µ–Ω–∞ —Å—Ç–∞—Ä–æ–≥–æ —á–∞–Ω–∫–∏–Ω–≥–∞ –Ω–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π (–ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞)
‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ API —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º
‚úÖ –£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã, —Ç–∞–±–ª–∏—Ü—ã, —Å–ø–∏—Å–∫–∏)
‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –æ–∂–∏–¥–∞–µ–º–æ–º —Ñ—Ä–æ–Ω—Ç–æ–º
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ Enhanced RAG Trainer –±–µ–∑ –ø–æ–ª–æ–º–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞
"""

import os
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from dataclasses import asdict
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à—É –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É
try:
    from integrated_structure_chunking_system import (
        IntegratedStructureChunkingSystem,
        process_document_with_intelligent_chunking,
        SmartChunk,
        ChunkType
    )
    INTEGRATED_SYSTEM_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("‚úÖ Integrated Structure & Chunking System loaded successfully")
except ImportError as e:
    INTEGRATED_SYSTEM_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning(f"‚ö†Ô∏è Integrated system not available: {e}")

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º Enhanced RAG Trainer
try:
    from complete_enhanced_bldr_rag_trainer import CompleteEnhancedBldrRAGTrainer
    ENHANCED_TRAINER_AVAILABLE = True
    logger.info("‚úÖ Enhanced RAG Trainer loaded successfully")
except ImportError as e:
    ENHANCED_TRAINER_AVAILABLE = False
    logger.warning(f"‚ö†Ô∏è Enhanced RAG Trainer not available: {e}")

class FrontendCompatibleRAGProcessor:
    """
    üîß Frontend-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π RAG –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
    
    –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –≤ Enhanced RAG Trainer
    —Å –ø–æ–ª–Ω—ã–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
    """
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        
        if INTEGRATED_SYSTEM_AVAILABLE:
            self.intelligent_system = IntegratedStructureChunkingSystem()
            self.use_intelligent_chunking = True
            logger.info("üß© Using intelligent structure-based chunking")
        else:
            self.intelligent_system = None
            self.use_intelligent_chunking = False
            logger.warning("‚ö†Ô∏è Fallback to basic chunking")
        
        if ENHANCED_TRAINER_AVAILABLE:
            self.enhanced_trainer = CompleteEnhancedBldrRAGTrainer()
            self.use_enhanced_trainer = True
            logger.info("üöÄ Using Enhanced RAG Trainer v3")
        else:
            self.enhanced_trainer = None
            self.use_enhanced_trainer = False
            logger.warning("‚ö†Ô∏è Enhanced RAG Trainer not available")
    
    def process_document_for_frontend(self, content: str, file_path: str = "", 
                                    additional_metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """
        üîß –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–º–µ—Å—Ç–∏–º—É—é —Å –æ–∂–∏–¥–∞–Ω–∏—è–º–∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞:
        - document_info: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤ –æ–∂–∏–¥–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        - sections: –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è
        - chunks: –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è RAG
        - tables: —Ç–∞–±–ª–∏—Ü—ã —Å –¥–∞–Ω–Ω—ã–º–∏
        - processing_quality: –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        """
        
        try:
            if self.use_intelligent_chunking:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—É—é —Å–∏—Å—Ç–µ–º—É
                result = self._process_with_intelligent_system(content, file_path, additional_metadata)
            else:
                # Fallback –∫ –±–∞–∑–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ
                result = self._process_with_fallback_system(content, file_path, additional_metadata)
            
            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
            frontend_result = self._adapt_for_frontend_api(result, file_path)
            
            logger.info(f"‚úÖ Document processed: {len(frontend_result.get('chunks', []))} chunks created")
            
            return frontend_result
            
        except Exception as e:
            logger.error(f"‚ùå Document processing failed: {e}")
            return self._create_error_response(str(e), file_path)
    
    def _process_with_intelligent_system(self, content: str, file_path: str, 
                                       additional_metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ø–æ–º–æ—â—å—é –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É
        result = self.intelligent_system.process_document(content, file_path)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if additional_metadata:
            result['document_info'].update(additional_metadata)
        
        return result
    
    def _process_with_fallback_system(self, content: str, file_path: str,
                                    additional_metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """Fallback –æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–µ–∑ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        
        logger.info("Using fallback processing without intelligent chunking")
        
        # –ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        document_info = self._extract_basic_metadata(content, file_path)
        if additional_metadata:
            document_info.update(additional_metadata)
        
        # –ë–∞–∑–æ–≤—ã–µ —Ä–∞–∑–¥–µ–ª—ã (–ø—Ä–æ—Å—Ç–æ–π regex)
        sections = self._extract_basic_sections(content)
        
        # –ë–∞–∑–æ–≤—ã–µ —á–∞–Ω–∫–∏ (—Ä–∞–∑–º–µ—Ä–Ω–æ–µ –¥–µ–ª–µ–Ω–∏–µ)
        chunks = self._create_basic_chunks(content)
        
        # –ë–∞–∑–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        tables = self._extract_basic_tables(content)
        
        return {
            'document_info': document_info,
            'sections': sections,
            'chunks': chunks,
            'tables': tables,
            'lists': [],
            'statistics': {
                'content_length': len(content),
                'word_count': len(content.split()),
                'chunks_created': len(chunks),
                'avg_chunk_quality': 0.6,  # –£–º–µ—Ä–µ–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è fallback
                'chunk_types_distribution': {'basic': len(chunks)}
            },
            'processing_info': {
                'extracted_at': datetime.now().isoformat(),
                'processor_version': 'Fallback_v1.0',
                'structure_quality': 0.5,
                'chunking_quality': 0.6,
                'processing_method': 'basic_fallback'
            }
        }
    
    def _adapt_for_frontend_api(self, result: Dict[str, Any], file_path: str) -> Dict[str, Any]:
        """
        üîß –ê–¥–∞–ø—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–æ–¥ API —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
        
        –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ø–æ–ª–Ω—É—é —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –æ–∂–∏–¥–∞–µ–º–æ–π —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
        """
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –æ–∂–∏–¥–∞–µ–º–∞—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º
        frontend_result = {
            # –û—Å–Ω–æ–≤–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ
            "document_info": {
                "id": self._generate_document_id(file_path),
                "title": result['document_info'].get('title', ''),
                "number": result['document_info'].get('number', ''),
                "type": result['document_info'].get('type', 'unknown'),
                "organization": result['document_info'].get('organization', ''),
                "date": result['document_info'].get('approval_date', ''),
                "file_name": result['document_info'].get('file_name', Path(file_path).name),
                "file_size": result['document_info'].get('file_size', 0),
                "keywords": result['document_info'].get('keywords', []),
                "status": "processed",
                "processing_time": datetime.now().isoformat()
            },
            
            # –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–∑–¥–µ–ª–æ–≤
            "sections": self._format_sections_for_navigation(result.get('sections', [])),
            
            # –ß–∞–Ω–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ, –æ–∂–∏–¥–∞–µ–º–æ–º –¥–ª—è RAG
            "chunks": self._format_chunks_for_rag(result.get('chunks', [])),
            
            # –¢–∞–±–ª–∏—Ü—ã –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            "tables": self._format_tables_for_frontend(result.get('tables', [])),
            
            # –°–ø–∏—Å–∫–∏
            "lists": result.get('lists', []),
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è UI
            "statistics": {
                "content_stats": {
                    "total_characters": result['statistics'].get('content_length', 0),
                    "total_words": result['statistics'].get('word_count', 0),
                    "total_sections": len(result.get('sections', [])),
                    "total_paragraphs": result['statistics'].get('paragraph_count', 0),
                    "total_tables": len(result.get('tables', [])),
                    "total_lists": len(result.get('lists', []))
                },
                "processing_stats": {
                    "chunks_created": result['statistics'].get('chunks_created', 0),
                    "avg_chunk_quality": result['statistics'].get('avg_chunk_quality', 0),
                    "chunk_types": result['statistics'].get('chunk_types_distribution', {}),
                    "structure_quality": result['processing_info'].get('structure_quality', 0),
                    "chunking_quality": result['processing_info'].get('chunking_quality', 0)
                }
            },
            
            # –ú–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            "processing_info": {
                "processor_version": result['processing_info'].get('processor_version', 'Unknown'),
                "processing_method": result['processing_info'].get('processing_method', 'unknown'),
                "extraction_quality": result['processing_info'].get('structure_quality', 0),
                "processing_time": result['processing_info'].get('extracted_at', ''),
                "features_used": {
                    "intelligent_chunking": self.use_intelligent_chunking,
                    "enhanced_trainer": self.use_enhanced_trainer,
                    "structure_extraction": True,
                    "table_extraction": len(result.get('tables', [])) > 0,
                    "list_extraction": len(result.get('lists', [])) > 0
                }
            }
        }
        
        return frontend_result
    
    def _format_sections_for_navigation(self, sections: List[Dict]) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞"""
        
        navigation_sections = []
        
        for section in sections:
            nav_section = {
                "id": section.get('id', ''),
                "number": section.get('number', ''),
                "title": section.get('title', ''),
                "level": section.get('level', 1),
                "has_content": section.get('content_length', 0) > 0,
                "has_subsections": section.get('has_subsections', False),
                "parent_path": section.get('parent_path', ''),
                "metadata": {
                    "word_count": section.get('content_length', 0) // 5,  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
                    "section_type": section.get('type', 'section')
                }
            }
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
            if section.get('subsections'):
                nav_section['subsections'] = self._format_sections_for_navigation(section['subsections'])
            
            navigation_sections.append(nav_section)
        
        return navigation_sections
    
    def _format_chunks_for_rag(self, chunks: List[Dict]) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã"""
        
        rag_chunks = []
        
        for chunk in chunks:
            rag_chunk = {
                "id": chunk.get('id', ''),
                "content": chunk.get('content', ''),
                "type": chunk.get('type', 'unknown'),
                "source_elements": chunk.get('source_elements', []),
                "metadata": {
                    **chunk.get('metadata', {}),
                    "word_count": chunk.get('word_count', 0),
                    "char_count": chunk.get('char_count', 0),
                    "quality_score": chunk.get('quality_score', 0),
                    "has_tables": chunk.get('has_tables', False),
                    "has_lists": chunk.get('has_lists', False),
                    "technical_terms_count": chunk.get('technical_terms', 0)
                },
                "search_metadata": {
                    "searchable_content": chunk.get('content', '')[:500],  # –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
                    "keywords": self._extract_chunk_keywords(chunk.get('content', '')),
                    "section_context": chunk.get('metadata', {}).get('section_number', ''),
                    "importance_score": self._calculate_chunk_importance(chunk)
                }
            }
            
            rag_chunks.append(rag_chunk)
        
        return rag_chunks
    
    def _format_tables_for_frontend(self, tables: List[Dict]) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞"""
        
        frontend_tables = []
        
        for i, table in enumerate(tables):
            frontend_table = {
                "id": table.get('id', f'table_{i+1}'),
                "number": table.get('number', str(i+1)),
                "title": table.get('title', f'–¢–∞–±–ª–∏—Ü–∞ {i+1}'),
                "headers": table.get('headers', []),
                "rows": table.get('rows', []),
                "metadata": {
                    "row_count": len(table.get('rows', [])),
                    "column_count": len(table.get('headers', [])),
                    "page_number": table.get('page_number', 0),
                    "is_structured": len(table.get('headers', [])) > 0
                },
                "display_options": {
                    "show_headers": len(table.get('headers', [])) > 0,
                    "searchable": True,
                    "exportable": True,
                    "max_display_rows": 100  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è UI
                }
            }
            
            frontend_tables.append(frontend_table)
        
        return frontend_tables
    
    def integrate_with_enhanced_trainer(self) -> 'EnhancedBldrRAGTrainerWithIntelligentChunking':
        """
        üîß –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Enhanced RAG Trainer
        
        –°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
        """
        
        if not (ENHANCED_TRAINER_AVAILABLE and INTEGRATED_SYSTEM_AVAILABLE):
            logger.warning("‚ö†Ô∏è Full integration not available, creating limited version")
            return EnhancedBldrRAGTrainerWithIntelligentChunking(
                use_intelligent_chunking=False,
                use_enhanced_trainer=False
            )
        
        return EnhancedBldrRAGTrainerWithIntelligentChunking(
            use_intelligent_chunking=True,
            use_enhanced_trainer=True
        )
    
    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _generate_document_id(self, file_path: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        import hashlib
        path_hash = hashlib.md5(file_path.encode()).hexdigest()[:8]
        return f"doc_{path_hash}"
    
    def _extract_chunk_keywords(self, content: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ —á–∞–Ω–∫–∞"""
        import re
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        keywords = []
        
        # –ì–û–°–¢, –°–ü, –°–ù–∏–ü
        tech_refs = re.findall(r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü)\s+[\d.-]+', content, re.IGNORECASE)
        keywords.extend(tech_refs)
        
        # –ò–∑–º–µ—Ä–µ–Ω–∏—è
        measurements = re.findall(r'\b\d+(?:\.\d+)?\s*(?:–º–º|—Å–º|–º|–∫–º|–∫–≥|—Ç|–ú–ü–∞|¬∞C)\b', content, re.IGNORECASE)
        keywords.extend([m.strip() for m in measurements])
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        return list(dict.fromkeys(keywords))[:10]
    
    def _calculate_chunk_importance(self, chunk: Dict) -> float:
        """–†–∞—Å—á–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç–∏ —á–∞–Ω–∫–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        importance = 0.5  # –ë–∞–∑–æ–≤–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–∞—á–µ—Å—Ç–≤–æ
        quality = chunk.get('quality_score', 0)
        importance += quality * 0.3
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–∞–±–ª–∏—Ü
        if chunk.get('has_tables', False):
            importance += 0.2
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        tech_terms = chunk.get('technical_terms', 0)
        if tech_terms > 0:
            importance += min(tech_terms / 10.0, 0.2)
        
        return min(importance, 1.0)
    
    def _extract_basic_metadata(self, content: str, file_path: str) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (fallback)"""
        import re
        
        # –ü—Ä–æ—Å—Ç–µ–π—à–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        title_match = re.search(r'^([–ê-–Ø–Å][–ê-–Ø–Å\s]{10,100})', content, re.MULTILINE)
        title = title_match.group(1).strip() if title_match else ''
        
        number_match = re.search(r'(?:–°–ü|–ì–û–°–¢|–°–ù–∏–ü)\s+([\d.-]+)', content, re.IGNORECASE)
        number = number_match.group(1) if number_match else ''
        
        return {
            'title': title,
            'number': number,
            'type': 'unknown',
            'organization': '',
            'approval_date': '',
            'file_name': Path(file_path).name,
            'file_size': len(content.encode('utf-8')),
            'keywords': []
        }
    
    def _extract_basic_sections(self, content: str) -> List[Dict]:
        """–ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ (fallback)"""
        import re
        
        sections = []
        patterns = [r'^(\d+)\.\s+([–ê-–Ø–Å][–ê-–Ø–Å\s]{5,80})', r'^(\d+\.\d+)\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s]{5,80})']
        
        for i, pattern in enumerate(patterns):
            matches = re.findall(pattern, content, re.MULTILINE)
            for number, title in matches:
                sections.append({
                    'id': f'section_{len(sections)+1}',
                    'number': number,
                    'title': title,
                    'level': len(number.split('.')),
                    'type': 'section',
                    'content_length': 100,  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
                    'has_subsections': False,
                    'parent_path': ''
                })
        
        return sections[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
    
    def _create_basic_chunks(self, content: str) -> List[Dict]:
        """–ë–∞–∑–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ (fallback)"""
        chunks = []
        chunk_size = 800
        overlap = 100
        
        for i in range(0, len(content), chunk_size - overlap):
            chunk_text = content[i:i + chunk_size]
            if len(chunk_text) >= 50:
                chunks.append({
                    'id': f'chunk_{i//chunk_size + 1}',
                    'content': chunk_text,
                    'type': 'basic',
                    'source_elements': [],
                    'metadata': {
                        'start_position': i,
                        'end_position': i + len(chunk_text)
                    },
                    'quality_score': 0.6,
                    'word_count': len(chunk_text.split()),
                    'char_count': len(chunk_text),
                    'has_tables': '|' in chunk_text,
                    'has_lists': bool(re.search(r'^[-‚Ä¢*]', chunk_text, re.MULTILINE)),
                    'technical_terms': 0
                })
        
        return chunks
    
    def _extract_basic_tables(self, content: str) -> List[Dict]:
        """–ë–∞–∑–æ–≤–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü (fallback)"""
        import re
        
        tables = []
        table_matches = re.finditer(r'–¢–∞–±–ª–∏—Ü–∞\s+(\d+)(?:\s*[-‚Äì‚Äî]\s*([^\n]+))?\n((?:\|.*\|.*\n)+)', content, re.IGNORECASE)
        
        for i, match in enumerate(table_matches):
            number = match.group(1)
            title = match.group(2) if match.group(2) else f'–¢–∞–±–ª–∏—Ü–∞ {number}'
            table_content = match.group(3)
            
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
            rows = []
            headers = []
            for line in table_content.split('\n'):
                if '|' in line:
                    cells = [cell.strip() for cell in line.split('|') if cell.strip()]
                    if cells:
                        if not headers:
                            headers = cells
                        else:
                            rows.append(cells)
            
            if headers or rows:
                tables.append({
                    'id': f'table_{i+1}',
                    'number': number,
                    'title': title,
                    'headers': headers,
                    'rows': rows,
                    'page_number': 0,
                    'metadata': {
                        'source_line': 0,
                        'table_type': 'basic'
                    }
                })
        
        return tables
    
    def _create_error_response(self, error_message: str, file_path: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ–± –æ—à–∏–±–∫–µ"""
        return {
            "document_info": {
                "id": "error",
                "title": "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏",
                "file_name": Path(file_path).name,
                "status": "error",
                "error_message": error_message
            },
            "sections": [],
            "chunks": [],
            "tables": [],
            "lists": [],
            "statistics": {
                "content_stats": {},
                "processing_stats": {}
            },
            "processing_info": {
                "processor_version": "Error_v1.0",
                "processing_method": "error_fallback",
                "features_used": {}
            }
        }

class EnhancedBldrRAGTrainerWithIntelligentChunking:
    """
    üöÄ Enhanced BLDR RAG Trainer —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
    
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞, –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É—é—â–∞—è –≤—Å–µ —É–ª—É—á—à–µ–Ω–∏—è
    –≤–∫–ª—é—á–∞—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    """
    
    def __init__(self, use_intelligent_chunking: bool = True, use_enhanced_trainer: bool = True,
                 **kwargs):
        
        self.use_intelligent_chunking = use_intelligent_chunking
        self.use_enhanced_trainer = use_enhanced_trainer
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.frontend_processor = FrontendCompatibleRAGProcessor()
        
        if use_enhanced_trainer and ENHANCED_TRAINER_AVAILABLE:
            self.base_trainer = CompleteEnhancedBldrRAGTrainer(**kwargs)
        else:
            self.base_trainer = None
            logger.warning("‚ö†Ô∏è Enhanced trainer not available, using limited functionality")
        
        logger.info(f"üöÄ Enhanced RAG Trainer initialized with intelligent chunking: {use_intelligent_chunking}")
    
    def train(self, max_files: Optional[int] = None, base_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        üöÄ –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ RAG —Å–∏—Å—Ç–µ–º—ã —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
        """
        
        logger.info("üöÄ Starting Enhanced RAG Training with intelligent chunking")
        
        if self.base_trainer:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π Enhanced trainer
            result = self.base_trainer.train(max_files)
            logger.info("‚úÖ Enhanced RAG training completed")
            return result
        else:
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            logger.warning("‚ö†Ô∏è Using simplified training without full Enhanced trainer")
            return self._simplified_training(max_files, base_dir)
    
    def process_single_document(self, content: str, file_path: str = "") -> Dict[str, Any]:
        """
        üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º
        """
        
        logger.info(f"üìÑ Processing document: {Path(file_path).name}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—à frontend-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        result = self.frontend_processor.process_document_for_frontend(content, file_path)
        
        logger.info(f"‚úÖ Document processed: {len(result.get('chunks', []))} chunks, quality: {result['processing_info'].get('extraction_quality', 0):.2f}")
        
        return result
    
    def get_chunks_for_rag(self, content: str, file_path: str = "") -> List[Dict[str, Any]]:
        """
        üß© –ü–æ–ª—É—á–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã
        
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤
        """
        
        result = self.process_single_document(content, file_path)
        return result.get('chunks', [])
    
    def _simplified_training(self, max_files: Optional[int] = None, base_dir: Optional[str] = None) -> Dict[str, Any]:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ Enhanced trainer"""
        
        base_path = base_dir or os.getenv("BASE_DIR", "I:/docs")
        
        # –ü–æ–∏—Å–∫ —Ñ–∞–π–ª–æ–≤
        all_files = []
        for ext in ['*.pdf', '*.docx', '*.txt']:
            files = list(Path(base_path).rglob(ext))
            all_files.extend([str(f) for f in files])
        
        if max_files:
            all_files = all_files[:max_files]
        
        logger.info(f"Found {len(all_files)} files for processing")
        
        processed_count = 0
        total_chunks = 0
        
        for file_path in all_files[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –¥–µ–º–æ
            try:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                result = self.process_single_document(content, file_path)
                total_chunks += len(result.get('chunks', []))
                processed_count += 1
                
            except Exception as e:
                logger.error(f"Failed to process {file_path}: {e}")
        
        return {
            'training_summary': {
                'documents_processed': processed_count,
                'total_chunks_created': total_chunks,
                'processing_method': 'simplified_intelligent_chunking'
            }
        }

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
def create_frontend_compatible_rag_trainer(**kwargs) -> EnhancedBldrRAGTrainerWithIntelligentChunking:
    """
    üöÄ –ë—ã—Å—Ç—Ä–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ RAG —Ç—Ä–µ–Ω–µ—Ä–∞ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
    
    Returns:
        –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π —Ç—Ä–µ–Ω–µ—Ä —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
    """
    
    return EnhancedBldrRAGTrainerWithIntelligentChunking(
        use_intelligent_chunking=True,
        use_enhanced_trainer=True,
        **kwargs
    )

# API-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞–º–∏
def process_document_api_compatible(content: str, file_path: str = "") -> Dict[str, Any]:
    """
    üîß API-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö API —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞—Ö –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
    """
    
    processor = FrontendCompatibleRAGProcessor()
    return processor.process_document_for_frontend(content, file_path)

def get_document_structure_api(content: str, file_path: str = "") -> Dict[str, Any]:
    """
    üìä API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –≤ UI)
    """
    
    result = process_document_api_compatible(content, file_path)
    
    return {
        'document_info': result['document_info'],
        'sections': result['sections'],
        'statistics': result['statistics']['content_stats']
    }

def get_document_chunks_api(content: str, file_path: str = "") -> List[Dict[str, Any]]:
    """
    üß© API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã)
    """
    
    result = process_document_api_compatible(content, file_path)
    return result['chunks']

if __name__ == "__main__":
    print("üîß Frontend Compatible RAG Integration v3 - Ready!")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
    try:
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
        trainer = create_frontend_compatible_rag_trainer()
        
        # –¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        test_content = """
–°–ü 50.13330.2012
–¢–ï–ü–õ–û–í–ê–Ø –ó–ê–©–ò–¢–ê –ó–î–ê–ù–ò–ô

1. –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø
1.1 –ù–∞—Å—Ç–æ—è—â–∏–π —Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∑–∞—â–∏—Ç—ã –∑–¥–∞–Ω–∏–π.

–¢–∞–±–ª–∏—Ü–∞ 1 - –ù–æ—Ä–º–∏—Ä—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
|–¢–∏–ø –∑–¥–∞–Ω–∏—è|–°–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ, –º¬≤¬∑¬∞–°/–í—Ç|
|–ñ–∏–ª—ã–µ|3,5|
|–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ|2,8|

2. –ù–û–†–ú–ê–¢–ò–í–ù–´–ï –°–°–´–õ–ö–ò
–í –Ω–∞—Å—Ç–æ—è—â–µ–º —Å–≤–æ–¥–µ –ø—Ä–∞–≤–∏–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:
- –ì–û–°–¢ 30494-2011 –ó–¥–∞–Ω–∏—è –∂–∏–ª—ã–µ –∏ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ
- –°–ü 23-101-2004 –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∑–∞—â–∏—Ç—ã –∑–¥–∞–Ω–∏–π
"""
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        result = trainer.process_single_document(test_content, "test_sp.pdf")
        
        print(f"‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç:")
        print(f"  üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {result['document_info']['title']}")
        print(f"  üìä –†–∞–∑–¥–µ–ª–æ–≤: {len(result['sections'])}")
        print(f"  üß© –ß–∞–Ω–∫–æ–≤: {len(result['chunks'])}")
        print(f"  üìã –¢–∞–±–ª–∏—Ü: {len(result['tables'])}")
        print(f"  üéØ –ö–∞—á–µ—Å—Ç–≤–æ: {result['processing_info']['extraction_quality']:.2f}")
        
        print(f"\nüîß API —Ñ—É–Ω–∫—Ü–∏–∏:")
        print(f"  - process_document_api_compatible() ‚úÖ")
        print(f"  - get_document_structure_api() ‚úÖ") 
        print(f"  - get_document_chunks_api() ‚úÖ")
        
        print(f"\nüéâ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º!")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        print("‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π")