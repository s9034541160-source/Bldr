#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üîç ENHANCED STRUCTURE EXTRACTOR V3
==================================
–î–µ—Ç–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏

–£–õ–£–ß–®–ï–ù–ò–Ø –°–¢–†–£–ö–¢–£–†–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê:
‚úÖ –î–µ—Ç–∞–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∞–±–∑–∞—Ü–µ–≤ —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π
‚úÖ –¢–æ—á–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç–æ–≤ –∏ –ø–æ–¥–ø—É–Ω–∫—Ç–æ–≤ (1.1, 1.2.1, –∏ —Ç.–¥.)
‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ (–º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ, –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ)
‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –Ω–æ–º–µ—Ä–∞ –°–ü/–ì–û–°–¢/–°–ù–∏–ü
‚úÖ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –∏ –¥–∞—Ç—ã —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Ñ—Ä–æ–Ω—Ç–æ–º
‚úÖ JSON-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
"""

import re
import json
from typing import Dict, List, Any, Tuple, Optional
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path

@dataclass
class DocumentSection:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    number: str
    title: str
    level: int
    content: str
    subsections: List['DocumentSection'] = field(default_factory=list)
    paragraphs: List[str] = field(default_factory=list)
    tables: List[Dict] = field(default_factory=list)
    lists: List[Dict] = field(default_factory=list)
    page_numbers: List[int] = field(default_factory=list)

@dataclass
class DocumentTable:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ç–∞–±–ª–∏—Ü—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    number: str
    title: str
    headers: List[str]
    rows: List[List[str]]
    caption: str = ""
    page_number: int = 0
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class DocumentList:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    list_type: str  # 'numbered', 'bulleted', 'multilevel'
    items: List[Dict]
    level: int = 1
    parent_section: str = ""

class EnhancedStructureExtractor:
    """
    üîç –£–ª—É—á—à–µ–Ω–Ω—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –ø–æ–ª–Ω—ã–º–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏:
    - –í—Å–µ –∞–±–∑–∞—Ü—ã —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π
    - –ü—É–Ω–∫—Ç—ã –∏ –ø–æ–¥–ø—É–Ω–∫—Ç—ã (1.1, 1.2.1 –∏ —Ç.–¥.)
    - –¢–∞–±–ª–∏—Ü—ã —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
    - –°–ø–∏—Å–∫–∏ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤
    - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–∞–∑–≤–∞–Ω–∏–µ, –Ω–æ–º–µ—Ä, –¥–∞—Ç–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è)
    """
    
    def __init__(self):
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–∞–∑–¥–µ–ª–æ–≤
        self.section_patterns = {
            'numbered': [
                r'^(\d+(?:\.\d+)*)\s+([–ê-–Ø–Å][^.\n]{5,100})$',  # 1. –û–°–ù–û–í–ù–´–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø
                r'^(\d+(?:\.\d+)*)\s+([–ê-–Ø–∞-—è—ë\s]{10,80}[^.]?)$',  # 1.1 –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
                r'^(\d+(?:\.\d+)*\.?\s*)([–ê-–Ø–Å\s]{5,60})$',  # 1.1. –¢–†–ï–ë–û–í–ê–ù–ò–Ø
            ],
            'lettered': [
                r'^([–∞-—è]\)|\([–∞-—è]\))\s+([–ê-–Ø–∞-—è—ë\s]{5,80})$',  # –∞) —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
                r'^([–ê-–Ø]\)|\([–ê-–Ø]\))\s+([–ê-–Ø–∞-—è—ë\s]{5,80})$',  # –ê) –¢–†–ï–ë–û–í–ê–ù–ò–Ø
            ],
            'appendix': [
                r'^(–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\s+[–ê-–Ø])\s*\.?\s*([–ê-–Ø–∞-—è—ë\s]{5,100})$',
                r'^(–ü–†–ò–õ–û–ñ–ï–ù–ò–ï\s+[–ê-–Ø])\s*\.?\s*([–ê-–Ø–Å\s]{5,100})$',
            ]
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–∞–±–ª–∏—Ü
        self.table_patterns = {
            'table_start': r'(?:–¢–∞–±–ª–∏—Ü–∞|–¢–ê–ë–õ–ò–¶–ê)\s+(\d+(?:\.\d+)*)\s*[-‚Äì‚Äî]?\s*([^\n]{0,80})',
            'table_row': r'\|([^|]+)\|',
            'table_separator': r'\|[-\s:|]+\|',
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–ø–∏—Å–∫–æ–≤
        self.list_patterns = {
            'numbered': r'^(\d+(?:\.\d+)*)\.\s+(.+)$',
            'bulleted': r'^[-‚Ä¢*]\s+(.+)$',
            'lettered': r'^[–∞-—è]\)\s+(.+)$|^\([–∞-—è]\)\s+(.+)$',
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞
        self.metadata_patterns = {
            'document_number': [
                r'(?:–°–ü|–°–í–û–î\s+–ü–†–ê–í–ò–õ)\s+(\d+(?:\.\d+)*(?:-\d+)?)',
                r'(?:–ì–û–°–¢)\s+(\d+(?:\.\d+)*(?:-\d+)?)',
                r'(?:–°–ù–∏–ü)\s+([\d.-]+)',
                r'‚Ññ\s*([–ê-–Ø0-9.-]+)',
            ],
            'document_title': [
                r'^([–ê-–Ø–Å][^.\n]{20,120})$',  # –ü–µ—Ä–≤–∞—è –∑–∞–≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
                r'(?:–°–í–û–î\s+–ü–†–ê–í–ò–õ|–°–ü\s+\d+[^.]*)\s*\.?\s*([–ê-–Ø–Å][^\n]{20,120})',
                r'(?:–ì–û–°–¢\s+\d+[^.]*)\s*\.?\s*([–ê-–Ø–Å][^\n]{20,120})',
            ],
            'organization': [
                r'(?:–£–¢–í–ï–†–ñ–î–ï–ù|–£–¢–í–ï–†–ñ–î–ï–ù–û)\s+([–ê-–Ø–Å][–ê-–Ø–∞-—è—ë\s]{5,60})',
                r'(?:–ú–∏–Ω—Å—Ç—Ä–æ–π\s+–†–æ—Å—Å–∏–∏|–†–æ—Å—Å—Ç–∞–Ω–¥–∞—Ä—Ç|–ú–∏–Ω—Ä–µ–≥–∏–æ–Ω\s+–†–æ—Å—Å–∏–∏)',
                r'([–ê-–Ø–Å][–ê-–Ø–∞-—è—ë\s]{5,40}(?:–∏–Ω—Å—Ç–∏—Ç—É—Ç|—Ü–µ–Ω—Ç—Ä|–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è|–ø—Ä–µ–¥–ø—Ä–∏—è—Ç–∏–µ))',
            ],
            'approval_date': [
                r'(?:—É—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ|—É—Ç–≤–µ—Ä–∂–¥–µ–Ω)\s+.*?(\d{1,2}\.\d{1,2}\.\d{4})',
                r'(?:–æ—Ç|‚Ññ)\s*.*?(\d{1,2}\.\d{1,2}\.\d{4})',
                r'(\d{1,2}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{4})',
            ],
            'effective_date': [
                r'(?:–≤–≤–æ–¥–∏—Ç—Å—è\s+–≤\s+–¥–µ–π—Å—Ç–≤–∏–µ|–¥–µ–π—Å—Ç–≤—É–µ—Ç\s+—Å)\s+(\d{1,2}\.\d{1,2}\.\d{4})',
                r'(?:—Å\s+)(\d{1,2}\.\d{1,2}\.\d{4})',
            ]
        }

    def extract_full_structure(self, content: str, file_path: str = "") -> Dict[str, Any]:
        """
        üîç –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î: –ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        
        Returns:
            Dict with complete document structure including:
            - metadata: document info (title, number, dates, organization)
            - sections: hierarchical sections with all content
            - tables: all tables with data and metadata
            - lists: all lists with structure
            - paragraphs: all paragraphs with metadata
            - statistics: document statistics
        """
        
        # 1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞
        metadata = self._extract_document_metadata(content, file_path)
        
        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–∞–∑–¥–µ–ª–æ–≤
        sections = self._extract_hierarchical_sections(content)
        
        # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü
        tables = self._extract_all_tables(content)
        
        # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ø–∏—Å–∫–æ–≤
        lists = self._extract_all_lists(content)
        
        # 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∞–±–∑–∞—Ü–µ–≤
        paragraphs = self._extract_all_paragraphs(content, sections)
        
        # 6. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        statistics = self._calculate_document_statistics(content, sections, tables, lists)
        
        # 7. –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        full_structure = {
            'metadata': metadata,
            'sections': [self._section_to_dict(section) for section in sections],
            'tables': [self._table_to_dict(table) for table in tables],
            'lists': [self._list_to_dict(list_item) for list_item in lists],
            'paragraphs': paragraphs,
            'statistics': statistics,
            'extraction_info': {
                'extracted_at': datetime.now().isoformat(),
                'extractor_version': 'Enhanced_v3.0',
                'file_path': file_path,
                'content_length': len(content),
                'extraction_quality_score': self._calculate_extraction_quality(sections, tables, lists, metadata)
            }
        }
        
        return full_structure

    def _extract_document_metadata(self, content: str, file_path: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        metadata = {
            'document_number': '',
            'document_title': '',
            'document_type': 'unknown',
            'organization': '',
            'approval_date': '',
            'effective_date': '',
            'keywords': [],
            'file_info': {
                'file_name': Path(file_path).name if file_path else '',
                'file_path': file_path,
            }
        }
        
        # –ü–µ—Ä–≤—ã–µ 2000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        header_content = content[:2000]
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞
        for pattern in self.metadata_patterns['document_number']:
            match = re.search(pattern, header_content, re.IGNORECASE | re.MULTILINE)
            if match:
                metadata['document_number'] = match.group(1)
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                if '–°–ü' in pattern or '–°–í–û–î' in pattern:
                    metadata['document_type'] = 'sp'  # –°–≤–æ–¥ –ø—Ä–∞–≤–∏–ª
                elif '–ì–û–°–¢' in pattern:
                    metadata['document_type'] = 'gost'  # –ì–û–°–¢
                elif '–°–ù–∏–ü' in pattern:
                    metadata['document_type'] = 'snip'  # –°–ù–∏–ü
                break
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞
        for pattern in self.metadata_patterns['document_title']:
            matches = re.findall(pattern, header_content, re.MULTILINE)
            if matches:
                # –ë–µ—Ä–µ–º —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–æ–±—ã—á–Ω–æ —ç—Ç–æ –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ)
                metadata['document_title'] = max(matches, key=len).strip()
                break
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        for pattern in self.metadata_patterns['organization']:
            match = re.search(pattern, header_content, re.IGNORECASE | re.MULTILINE)
            if match:
                metadata['organization'] = match.group(1).strip() if match.groups() else match.group(0).strip()
                break
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç
        for pattern in self.metadata_patterns['approval_date']:
            match = re.search(pattern, header_content, re.IGNORECASE | re.MULTILINE)
            if match:
                metadata['approval_date'] = match.group(1)
                break
        
        for pattern in self.metadata_patterns['effective_date']:
            match = re.search(pattern, header_content, re.IGNORECASE | re.MULTILINE)
            if match:
                metadata['effective_date'] = match.group(1)
                break
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        keywords = self._extract_keywords(content, metadata['document_type'])
        metadata['keywords'] = keywords
        
        return metadata

    def _extract_hierarchical_sections(self, content: str) -> List[DocumentSection]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–∞–∑–¥–µ–ª–æ–≤"""
        sections = []
        lines = content.split('\n')
        current_section = None
        current_subsection = None
        current_content = []
        
        for line_num, line in enumerate(lines):
            line = line.strip()
            if not line:
                if current_content and current_content[-1]:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
                    current_content.append('')
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —Ä–∞–∑–¥–µ–ª–∞
            section_match = self._match_section_header(line)
            
            if section_match:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Ä–∞–∑–¥–µ–ª
                if current_section:
                    current_section.content = '\n'.join(current_content).strip()
                    if current_subsection:
                        current_section.subsections.append(current_subsection)
                        current_subsection = None
                    sections.append(current_section)
                
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª
                number, title, level = section_match
                current_section = DocumentSection(
                    number=number,
                    title=title,
                    level=level,
                    content="",
                    page_numbers=[self._estimate_page_number(line_num, len(lines))]
                )
                current_content = []
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–æ–º
                if level > 1 and sections:
                    # –≠—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª, –¥–æ–±–∞–≤–ª—è–µ–º –∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º—É —Ä–∞–∑–¥–µ–ª—É
                    parent_section = sections[-1]
                    if current_subsection and current_subsection.level < level:
                        current_subsection.content = '\n'.join(current_content).strip()
                        parent_section.subsections.append(current_subsection)
                    current_subsection = current_section
                    current_section = parent_section
                    current_content = []
            else:
                # –û–±—ã—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                current_content.append(line)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑–¥–µ–ª
        if current_section:
            current_section.content = '\n'.join(current_content).strip()
            if current_subsection:
                current_section.subsections.append(current_subsection)
            sections.append(current_section)
        
        return sections

    def _match_section_header(self, line: str) -> Optional[Tuple[str, str, int]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —Ä–∞–∑–¥–µ–ª–∞"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        for pattern in self.section_patterns['numbered']:
            match = re.match(pattern, line, re.IGNORECASE)
            if match:
                number = match.group(1)
                title = match.group(2).strip()
                level = len(number.split('.'))
                return (number, title, level)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±—É–∫–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
        for pattern in self.section_patterns['lettered']:
            match = re.match(pattern, line, re.IGNORECASE)
            if match:
                number = match.group(1)
                title = match.group(2).strip()
                return (number, title, 2)  # –ë—É–∫–≤–µ–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã –æ–±—ã—á–Ω–æ —É—Ä–æ–≤–Ω—è 2
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        for pattern in self.section_patterns['appendix']:
            match = re.match(pattern, line, re.IGNORECASE)
            if match:
                number = match.group(1)
                title = match.group(2).strip() if len(match.groups()) > 1 else ""
                return (number, title, 1)  # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è - —É—Ä–æ–≤–µ–Ω—å 1
        
        return None

    def _extract_all_tables(self, content: str) -> List[DocumentTable]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        tables = []
        lines = content.split('\n')
        current_table = None
        in_table = False
        table_rows = []
        
        for line_num, line in enumerate(lines):
            line = line.strip()
            
            # –ü–æ–∏—Å–∫ –Ω–∞—á–∞–ª–∞ —Ç–∞–±–ª–∏—Ü—ã
            table_start_match = re.search(self.table_patterns['table_start'], line, re.IGNORECASE)
            if table_start_match:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ç–∞–±–ª–∏—Ü—É
                if current_table and table_rows:
                    current_table.rows = table_rows
                    tables.append(current_table)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É
                table_number = table_start_match.group(1)
                table_title = table_start_match.group(2).strip() if len(table_start_match.groups()) > 1 else ""
                
                current_table = DocumentTable(
                    number=table_number,
                    title=table_title,
                    headers=[],
                    rows=[],
                    page_number=self._estimate_page_number(line_num, len(lines))
                )
                table_rows = []
                in_table = True
                continue
            
            if in_table:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å—Ç—Ä–æ–∫–æ–π —Ç–∞–±–ª–∏—Ü—ã
                if '|' in line:
                    # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)
                    if re.match(self.table_patterns['table_separator'], line):
                        continue
                    
                    # –û–±—ã—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
                    cells = [cell.strip() for cell in line.split('|') if cell.strip()]
                    if cells:
                        if not current_table.headers:
                            current_table.headers = cells
                        else:
                            table_rows.append(cells)
                else:
                    # –ö–æ–Ω–µ—Ü —Ç–∞–±–ª–∏—Ü—ã
                    if current_table and table_rows:
                        current_table.rows = table_rows
                        tables.append(current_table)
                    in_table = False
                    current_table = None
                    table_rows = []
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–∞–±–ª–∏—Ü—É
        if current_table and table_rows:
            current_table.rows = table_rows
            tables.append(current_table)
        
        return tables

    def _extract_all_lists(self, content: str) -> List[DocumentList]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ø–∏—Å–∫–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        lists = []
        lines = content.split('\n')
        current_list = None
        current_items = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            list_match = None
            list_type = None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã —Å–ø–∏—Å–∫–æ–≤
            for list_pattern_name, pattern in self.list_patterns.items():
                match = re.match(pattern, line, re.IGNORECASE)
                if match:
                    list_match = match
                    list_type = list_pattern_name
                    break
            
            if list_match:
                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–æ–≤—ã–π —Ç–∏–ø —Å–ø–∏—Å–∫–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π
                if current_list and current_list.list_type != list_type and current_items:
                    current_list.items = current_items
                    lists.append(current_list)
                    current_items = []
                
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π
                if not current_list or current_list.list_type != list_type:
                    current_list = DocumentList(
                        list_type=list_type,
                        items=[],
                        level=1
                    )
                
                # –î–æ–±–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å–ø–∏—Å–∫–∞
                item_text = list_match.group(1) if list_match.group(1) else list_match.group(2)
                if item_text:
                    current_items.append({
                        'text': item_text.strip(),
                        'number': list_match.group(0).split()[0] if list_type == 'numbered' else '',
                        'level': self._determine_list_level(line)
                    })
            else:
                # –ó–∞–≤–µ—Ä—à–∞–µ–º —Ç–µ–∫—É—â–∏–π —Å–ø–∏—Å–æ–∫
                if current_list and current_items:
                    current_list.items = current_items
                    lists.append(current_list)
                    current_list = None
                    current_items = []
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ø–∏—Å–æ–∫
        if current_list and current_items:
            current_list.items = current_items
            lists.append(current_list)
        
        return lists

    def _extract_all_paragraphs(self, content: str, sections: List[DocumentSection]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –∞–±–∑–∞—Ü–µ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
        paragraphs = []
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
        raw_paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        for i, paragraph in enumerate(raw_paragraphs):
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫ –∫–∞–∫–æ–º—É —Ä–∞–∑–¥–µ–ª—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∞–±–∑–∞—Ü
            parent_section = self._find_parent_section(paragraph, sections)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∞–±–∑–∞—Ü–∞
            paragraph_info = {
                'id': f'p_{i}',
                'text': paragraph,
                'section': parent_section.number if parent_section else '',
                'section_title': parent_section.title if parent_section else '',
                'word_count': len(paragraph.split()),
                'char_count': len(paragraph),
                'has_numbers': bool(re.search(r'\d+', paragraph)),
                'has_technical_terms': self._has_technical_terms(paragraph),
                'is_list_item': self._is_list_item(paragraph),
                'is_table_content': '|' in paragraph,
                'references': self._extract_references(paragraph),
                'position': i
            }
            
            paragraphs.append(paragraph_info)
        
        return paragraphs

    def _calculate_document_statistics(self, content: str, sections: List[DocumentSection], 
                                     tables: List[DocumentTable], lists: List[DocumentList]) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        lines = content.split('\n')
        paragraphs = [p for p in content.split('\n\n') if p.strip()]
        words = content.split()
        
        return {
            'content_length': len(content),
            'line_count': len(lines),
            'paragraph_count': len(paragraphs),
            'word_count': len(words),
            'section_count': len(sections),
            'subsection_count': sum(len(section.subsections) for section in sections),
            'table_count': len(tables),
            'list_count': len(lists),
            'list_item_count': sum(len(lst.items) for lst in lists),
            'avg_section_length': np.mean([len(section.content) for section in sections]) if sections else 0,
            'longest_section': max((len(section.content) for section in sections), default=0),
            'has_appendices': any('–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' in section.number.lower() for section in sections),
            'technical_density': self._calculate_technical_density(content),
        }

    def _calculate_extraction_quality(self, sections: List[DocumentSection], tables: List[DocumentTable], 
                                    lists: List[DocumentList], metadata: Dict[str, Any]) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        quality_score = 0.0
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–∞–∑–¥–µ–ª–æ–≤ (40%)
        if sections:
            section_score = min(len(sections) / 10.0, 1.0) * 0.4
            quality_score += section_score
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (30%)
        metadata_fields = ['document_number', 'document_title', 'organization']
        filled_fields = sum(1 for field in metadata_fields if metadata.get(field))
        metadata_score = (filled_fields / len(metadata_fields)) * 0.3
        quality_score += metadata_score
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü (15%)
        if tables:
            table_score = min(len(tables) / 5.0, 1.0) * 0.15
            quality_score += table_score
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤ (15%)
        if lists:
            list_score = min(len(lists) / 10.0, 1.0) * 0.15
            quality_score += list_score
        
        return min(quality_score, 1.0)

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _section_to_dict(self, section: DocumentSection) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å–µ–∫—Ü–∏–∏ –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            'number': section.number,
            'title': section.title,
            'level': section.level,
            'content': section.content,
            'subsections': [self._section_to_dict(sub) for sub in section.subsections],
            'paragraphs': section.paragraphs,
            'tables': section.tables,
            'lists': section.lists,
            'page_numbers': section.page_numbers
        }
    
    def _table_to_dict(self, table: DocumentTable) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            'number': table.number,
            'title': table.title,
            'headers': table.headers,
            'rows': table.rows,
            'caption': table.caption,
            'page_number': table.page_number,
            'metadata': table.metadata,
            'row_count': len(table.rows),
            'column_count': len(table.headers) if table.headers else 0
        }
    
    def _list_to_dict(self, lst: DocumentList) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –≤ —Å–ª–æ–≤–∞—Ä—å"""
        return {
            'type': lst.list_type,
            'items': lst.items,
            'level': lst.level,
            'parent_section': lst.parent_section,
            'item_count': len(lst.items)
        }
    
    def _estimate_page_number(self, line_num: int, total_lines: int) -> int:
        """–ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        lines_per_page = 50  # –ü—Ä–∏–º–µ—Ä–Ω–æ 50 —Å—Ç—Ä–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        return max(1, (line_num // lines_per_page) + 1)
    
    def _find_parent_section(self, paragraph: str, sections: List[DocumentSection]) -> Optional[DocumentSection]:
        """–ü–æ–∏—Å–∫ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ –¥–ª—è –∞–±–∑–∞—Ü–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ - –∏—â–µ–º —Ä–∞–∑–¥–µ–ª —Å –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
        best_match = None
        best_similarity = 0
        
        for section in sections:
            if paragraph in section.content:
                return section
            
            # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ä–∞ —Å—Ö–æ–¥—Å—Ç–≤–∞ –ø–æ –æ–±—â–∏–º —Å–ª–æ–≤–∞–º
            paragraph_words = set(paragraph.lower().split())
            section_words = set(section.content.lower().split())
            common_words = paragraph_words & section_words
            if common_words:
                similarity = len(common_words) / len(paragraph_words)
                if similarity > best_similarity:
                    best_similarity = similarity
                    best_match = section
        
        return best_match
    
    def _has_technical_terms(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        technical_terms = [
            r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü)\s+[\d.-]+',
            r'\b\d+\s*(?:–º–º|—Å–º|–º|–∫–º|–º–≥|–≥|–∫–≥|—Ç)\b',
            r'\b\d+\s*(?:¬∞C|–ú–ü–∞|–∫–ü–∞|–ù|–∫–ù)\b',
            r'\b(?:–ø—Ä–æ—á–Ω–æ—Å—Ç—å|–¥–µ—Ñ–æ—Ä–º–∞—Ü–∏—è|–Ω–∞–≥—Ä—É–∑–∫–∞|–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ|–º–æ–¥—É–ª—å)\b'
        ]
        return any(re.search(term, text, re.IGNORECASE) for term in technical_terms)
    
    def _is_list_item(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–æ–º —Å–ø–∏—Å–∫–∞"""
        return bool(re.match(r'^\s*(?:\d+\.|\w\)|[-‚Ä¢*])\s+', text))
    
    def _extract_references(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        references = []
        patterns = [
            r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü)\s+[\d.-]+(?:-\d+)?',
            r'\b(?:–ø\.|–ø—É–Ω–∫—Ç)\s+\d+(?:\.\d+)*',
            r'\b(?:—Ç–∞–±–ª\.|—Ç–∞–±–ª–∏—Ü–∞)\s+\d+(?:\.\d+)*'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            references.extend(matches)
        
        return references
    
    def _determine_list_level(self, line: str) -> int:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ —Å–ø–∏—Å–∫–∞"""
        # –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–±–µ–ª–æ–≤ –∏–ª–∏ —Ç–∞–±–æ–≤ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
        leading_spaces = len(line) - len(line.lstrip())
        return max(1, leading_spaces // 4 + 1)  # 4 –ø—Ä–æ–±–µ–ª–∞ = 1 —É—Ä–æ–≤–µ–Ω—å
    
    def _extract_keywords(self, content: str, doc_type: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        keywords = []
        
        # –¢–∏–ø–æ—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        type_keywords = {
            'sp': ['—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è', '–Ω–æ—Ä–º—ã'],
            'gost': ['—Å—Ç–∞–Ω–¥–∞—Ä—Ç', '–∫–∞—á–µ—Å—Ç–≤–æ', '–∏—Å–ø—ã—Ç–∞–Ω–∏—è', '–º–µ—Ç–æ–¥—ã'],
            'snip': ['–Ω–æ—Ä–º—ã', '–ø—Ä–∞–≤–∏–ª–∞', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ']
        }
        
        # –û–±—â–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        general_patterns = [
            r'\b(?:–ø—Ä–æ—á–Ω–æ—Å—Ç—å|–¥–µ—Ñ–æ—Ä–º–∞—Ü–∏—è|–Ω–∞–≥—Ä—É–∑–∫–∞|–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ)\b',
            r'\b(?:–º–∞—Ç–µ—Ä–∏–∞–ª|–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è|—ç–ª–µ–º–µ–Ω—Ç|—É–∑–µ–ª)\b',
            r'\b(?:–∫–æ–Ω—Ç—Ä–æ–ª—å|–ø—Ä–æ–≤–µ—Ä–∫–∞|–∏–∑–º–µ—Ä–µ–Ω–∏–µ|—Ä–∞—Å—á–µ—Ç)\b'
        ]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∏–ø–æ—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        if doc_type in type_keywords:
            for keyword in type_keywords[doc_type]:
                if keyword in content.lower():
                    keywords.append(keyword)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        for pattern in general_patterns:
            matches = re.findall(pattern, content.lower(), re.IGNORECASE)
            keywords.extend(matches)
        
        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        return list(dict.fromkeys(keywords))[:15]
    
    def _calculate_technical_density(self, content: str) -> float:
        """–†–∞—Å—á–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        technical_patterns = [
            r'\b\d+(?:\.\d+)?\s*(?:–º–º|—Å–º|–º|–∫–º|–≥|–∫–≥|—Ç|–ú–ü–∞|–∫–ü–∞|¬∞C)\b',
            r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü)\s+[\d.-]+',
            r'\b\d+(?:\.\d+)*\s*%\b'
        ]
        
        technical_count = 0
        for pattern in technical_patterns:
            technical_count += len(re.findall(pattern, content, re.IGNORECASE))
        
        word_count = len(content.split())
        return technical_count / word_count if word_count > 0 else 0.0


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def extract_document_structure(content: str, file_path: str = "") -> Dict[str, Any]:
    """
    üîç –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
    
    Args:
        content: –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    
    Returns:
        –ü–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    extractor = EnhancedStructureExtractor()
    return extractor.extract_full_structure(content, file_path)


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ñ—Ä–æ–Ω—Ç–æ–º
def get_frontend_compatible_structure(content: str, file_path: str = "") -> Dict[str, Any]:
    """
    üîß –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ, —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å —Ñ—Ä–æ–Ω—Ç–æ–º
    
    –ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ–¥ –æ–∂–∏–¥–∞–µ–º—ã–π —Ñ—Ä–æ–Ω—Ç–æ–º —Ñ–æ—Ä–º–∞—Ç
    """
    structure = extract_document_structure(content, file_path)
    
    # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø–æ–¥ –æ–∂–∏–¥–∞–µ–º—ã–π —Ñ—Ä–æ–Ω—Ç–æ–º —Ñ–æ—Ä–º–∞—Ç
    frontend_structure = {
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (–∫–∞–∫ –æ–∂–∏–¥–∞–µ—Ç —Ñ—Ä–æ–Ω—Ç)
        'document_info': {
            'title': structure['metadata']['document_title'],
            'number': structure['metadata']['document_number'],
            'type': structure['metadata']['document_type'],
            'organization': structure['metadata']['organization'],
            'date': structure['metadata']['approval_date'],
            'file_name': structure['metadata']['file_info']['file_name']
        },
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–∞–∑–¥–µ–ª–æ–≤ (–ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
        'sections': [],
        
        # –¢–∞–±–ª–∏—Ü—ã –≤ –æ–∂–∏–¥–∞–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        'tables': [
            {
                'id': f'table_{i}',
                'number': table['number'],
                'title': table['title'],
                'data': {
                    'headers': table['headers'],
                    'rows': table['rows']
                },
                'page': table['page_number']
            }
            for i, table in enumerate(structure['tables'])
        ],
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        'statistics': structure['statistics'],
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        'extraction_quality': structure['extraction_info']['extraction_quality_score'],
        
        # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        'compatibility_version': 'v3.0_frontend_compatible'
    }
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã –≤ –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫
    def flatten_sections(sections, parent_path=""):
        flat_sections = []
        for section in sections:
            section_path = f"{parent_path}.{section['number']}" if parent_path else section['number']
            
            flat_section = {
                'id': section_path,
                'number': section['number'],
                'title': section['title'],
                'level': section['level'],
                'content': section['content'],
                'parent': parent_path,
                'has_subsections': len(section['subsections']) > 0,
                'word_count': len(section['content'].split()) if section['content'] else 0
            }
            flat_sections.append(flat_section)
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
            if section['subsections']:
                flat_sections.extend(flatten_sections(section['subsections'], section_path))
        
        return flat_sections
    
    frontend_structure['sections'] = flatten_sections(structure['sections'])
    
    return frontend_structure


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    print("üîç Enhanced Structure Extractor v3 - Ready!")
    
    # –¢–µ—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
    test_content = """
–°–ü 50.13330.2012

–¢–ï–ü–õ–û–í–ê–Ø –ó–ê–©–ò–¢–ê –ó–î–ê–ù–ò–ô

–ê–∫—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–µ–¥–∞–∫—Ü–∏—è –°–ù–∏–ü 23-02-2003

–£–¢–í–ï–†–ñ–î–ï–ù–û
–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ–º —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –†–§
31 –º–∞—è 2012 –≥. ‚Ññ 269

1. –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø

1.1 –ù–∞—Å—Ç–æ—è—â–∏–π —Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∑–∞—â–∏—Ç—ã –∑–¥–∞–Ω–∏–π.

1.2 –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ —Å–≤–æ–¥–∞ –ø—Ä–∞–≤–∏–ª –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã –¥–ª—è –≤—Å–µ—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π.

2. –ù–û–†–ú–ê–¢–ò–í–ù–´–ï –°–°–´–õ–ö–ò

–í –Ω–∞—Å—Ç–æ—è—â–µ–º —Å–≤–æ–¥–µ –ø—Ä–∞–≤–∏–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:
- –ì–û–°–¢ 30494-2011 –ó–¥–∞–Ω–∏—è –∂–∏–ª—ã–µ –∏ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∏–∫—Ä–æ–∫–ª–∏–º–∞—Ç–∞ –≤ –ø–æ–º–µ—â–µ–Ω–∏—è—Ö
- –°–ü 23-101-2004 –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∑–∞—â–∏—Ç—ã –∑–¥–∞–Ω–∏–π

–¢–∞–±–ª–∏—Ü–∞ 1 - –ù–æ—Ä–º–∏—Ä—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è

|–¢–∏–ø –∑–¥–∞–Ω–∏—è|–°–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–ø–µ—Ä–µ–¥–∞—á–µ, –º¬≤¬∑¬∞–°/–í—Ç|
|–ñ–∏–ª—ã–µ|3,5|
|–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ|2,8|
"""
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    structure = extract_document_structure(test_content, "test_sp.pdf")
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ:")
    print(f"  üìã –ù–∞–∑–≤–∞–Ω–∏–µ: {structure['metadata']['document_title']}")
    print(f"  üî¢ –ù–æ–º–µ—Ä: {structure['metadata']['document_number']}")
    print(f"  üìä –†–∞–∑–¥–µ–ª–æ–≤: {len(structure['sections'])}")
    print(f"  üìã –¢–∞–±–ª–∏—Ü: {len(structure['tables'])}")
    print(f"  üéØ –ö–∞—á–µ—Å—Ç–≤–æ: {structure['extraction_info']['extraction_quality_score']:.2f}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Ñ—Ä–æ–Ω—Ç–æ–º
    frontend_structure = get_frontend_compatible_structure(test_content, "test_sp.pdf")
    print(f"  üîß –§—Ä–æ–Ω—Ç —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: {frontend_structure['compatibility_version']}")