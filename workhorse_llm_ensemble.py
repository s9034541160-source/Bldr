"""
üöÄ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å LLM –¥–ª—è RTX 4060
Qwen3-8B (—Ä–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞) + RuT5 (—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é)
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª—è RAG-—Ç—Ä–µ–Ω–µ—Ä–∞
"""

import logging
import torch
import json
import time
import os
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    BitsAndBytesConfig
)

logger = logging.getLogger(__name__)

@dataclass
class WorkhorseLLMConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Ä–∞–±–æ—á–µ–≥–æ –∞–Ω—Å–∞–º–±–ª—è LLM"""
    # –†–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞: Qwen3-8B –¥–ª—è –≤—Å–µ—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö –∑–∞–¥–∞—á
    workhorse_model: str = "Qwen/Qwen3-8B"
    workhorse_max_length: int = 32768
    
    # –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: RuT5 –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    extraction_model: str = "ai-forever/ruT5-base"  # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô RuT5
    extraction_max_length: int = 2048
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ VRAM –¥–ª—è RTX 4060 (8GB)
    use_4bit_quantization: bool = True
    use_8bit_quantization: bool = False
    max_memory_gb: float = 7.0  # –û—Å—Ç–∞–≤–ª—è–µ–º 1GB –¥–ª—è —Å–∏—Å—Ç–µ–º—ã
    
    # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    device: str = "auto"
    cache_dir: str = "./models_cache"
    temperature: float = 0.1
    
    @classmethod
    def from_env(cls):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Hugging Face
        os.environ.setdefault('HF_HOME', 'I:/huggingface_cache')
        os.environ.setdefault('TRANSFORMERS_CACHE', 'I:/huggingface_cache')
        os.environ.setdefault('HF_DATASETS_CACHE', 'I:/huggingface_cache')
        
        return cls(
            workhorse_model=os.getenv('LLM_WORKHORSE_MODEL', 'Qwen/Qwen3-8B'),
            workhorse_max_length=int(os.getenv('LLM_WORKHORSE_MAX_LENGTH', '32768')),
            extraction_model=os.getenv('LLM_EXTRACTION_MODEL', 'ai-forever/ruT5-base'),
            extraction_max_length=int(os.getenv('LLM_EXTRACTION_MAX_LENGTH', '2048')),
            use_4bit_quantization=os.getenv('LLM_4BIT_QUANTIZATION', 'true').lower() == 'true',
            use_8bit_quantization=os.getenv('LLM_8BIT_QUANTIZATION', 'false').lower() == 'true',
            max_memory_gb=float(os.getenv('LLM_MAX_MEMORY_GB', '7.0')),
            device=os.getenv('LLM_DEVICE', 'auto'),
            cache_dir=os.getenv('LLM_CACHE_DIR', 'I:/models_cache'),
            temperature=float(os.getenv('LLM_TEMPERATURE', '0.1'))
        )

class WorkhorseLLMEnsemble:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å LLM –¥–ª—è RTX 4060
    Qwen3-8B (—Ä–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞) + RuT5 (—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é)
    """
    
    def __init__(self, config: WorkhorseLLMConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥–µ–ª–∏
        self.workhorse_model = None
        self.workhorse_tokenizer = None
        self.extraction_model = None
        self.extraction_tokenizer = None
        
        self._initialize_models()
    
    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—á–µ–≥–æ –∞–Ω—Å–∞–º–±–ª—è"""
        try:
            self.logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—á–µ–≥–æ –∞–Ω—Å–∞–º–±–ª—è LLM...")
            
            # 1. –†–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞: Qwen3-8B (–≤—Å–µ–≥–¥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)
            self._initialize_workhorse_model()
            
            # 2. –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: RuT5 (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é)
            self._initialize_extraction_model()
            
            self.logger.info("‚úÖ –†–∞–±–æ—á–∏–π –∞–Ω—Å–∞–º–±–ª—å LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω—Å–∞–º–±–ª—è: {e}")
            raise
    
    def _get_quantization_config(self) -> Optional[BitsAndBytesConfig]:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 4060"""
        if not torch.cuda.is_available():
            return None
        
        if self.config.use_4bit_quantization:
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
            )
        elif self.config.use_8bit_quantization:
            return BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
            )
        
        return None
    
    def _initialize_workhorse_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—á–µ–π –ª–æ—à–∞–¥–∫–∏: Qwen3-8B"""
        try:
            self.logger.info(f"üêé –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–±–æ—á–µ–π –ª–æ—à–∞–¥–∫–∏: {self.config.workhorse_model}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.workhorse_tokenizer = AutoTokenizer.from_pretrained(
                self.config.workhorse_model,
                cache_dir=self.config.cache_dir,
                trust_remote_code=True
            )
            
            if self.workhorse_tokenizer.pad_token is None:
                self.workhorse_tokenizer.pad_token = self.workhorse_tokenizer.eos_token
            
            # üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø VRAM –¥–ª—è RTX 4060 (8GB)
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π device_map="auto" –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã
            device_map_config = "auto" if torch.cuda.is_available() else "cpu"
            
            # üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è Qwen3-8B
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
                # ‚¨ÖÔ∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–õ–ê–ì –¢–ï–ü–ï–†–¨ –í–ù–£–¢–†–ò –ö–û–ù–§–ò–ì–ê
                llm_int8_enable_fp32_cpu_offload=True,
                bnb_4bit_compute_dtype=torch.bfloat16  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è Qwen
            )
            
            # üö® –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç VRAM –¥–æ 97.5%
            MAX_QWEN_VRAM = 7800  # 7800 MiB (97.5% –æ—Ç 8192 MiB)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –ª–∏–º–∏—Ç–æ–º VRAM
            self.workhorse_model = AutoModelForCausalLM.from_pretrained(
                self.config.workhorse_model,
                cache_dir=self.config.cache_dir,
                quantization_config=bnb_config,  # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ö–í–ê–ù–¢–ò–ó–ê–¶–ò–Ø
                device_map="auto",  # ‚¨ÖÔ∏è –í–û–ó–í–†–ê–©–ê–ï–ú device_map –¥–ª—è max_memory
                max_memory={0: f'{MAX_QWEN_VRAM}MiB', 'cpu': '50GiB'},  # ‚¨ÖÔ∏è –Ø–î–ï–†–ù–ê–Ø –ü–†–ê–í–ö–ê OOM
                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True,
                low_cpu_mem_usage=True,  # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU RAM
                offload_folder="./offload_cache"  # –ü–∞–ø–∫–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è
            )
            
            self.logger.info("‚úÖ –†–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞ (Qwen3-8B) –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            self.logger.info(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.workhorse_model.device}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–±–æ—á–µ–π –ª–æ—à–∞–¥–∫–∏: {e}")
            self.workhorse_model = None
    
    def _initialize_extraction_model(self):
        """üîç –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞: RuT5 (–ù–ï RuGPT!)"""
        try:
            self.logger.info(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞: {self.config.extraction_model}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä RuT5 —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–º slow tokenizer
            self.extraction_tokenizer = AutoTokenizer.from_pretrained(
                self.config.extraction_model,
                cache_dir=self.config.cache_dir,
                use_fast=False,  # ‚¨ÖÔ∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–ê–í–ö–ê –î–õ–Ø WINDOWS/RuT5
                trust_remote_code=True
            )
            
            if self.extraction_tokenizer.pad_token is None:
                self.extraction_tokenizer.pad_token = self.extraction_tokenizer.eos_token
            
            # ‚úÖ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û RuT5 –Ω–∞ CPU —Å float32 (float16 –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ CPU!)
            self.extraction_model = AutoModelForSeq2SeqLM.from_pretrained(
                self.config.extraction_model,
                cache_dir=self.config.cache_dir,
                trust_remote_code=True,
                torch_dtype=torch.float32  # ‚úÖ float32 –¥–ª—è CPU
            ).to("cpu")  # ‚úÖ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –ù–ê CPU
            
            # ‚úÖ –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê: –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ RuT5 –Ω–∞ CPU
            self.extraction_model = self.extraction_model.to("cpu")
            self.logger.info("‚úÖ –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç (RuT5) –∑–∞–≥—Ä—É–∂–µ–Ω")
            self.logger.info(f"üîß RuT5 device: {next(self.extraction_model.parameters()).device}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ RuT5. –†–∞–±–æ—Ç–∞ –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞: {e}")
            self.extraction_model = None
            self.extraction_tokenizer = None
    
    def stage_4_classify_document(self, content: str) -> Dict:
        """
        Stage 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (Qwen3-8B)
        –ë—ã—Å—Ç—Ä–∞—è –∏ —Ç–æ—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        """
        try:
            if self.workhorse_model is None:
                return {'classification_available': False, 'error': 'Workhorse model not loaded'}
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Qwen3-8B
            prompt = self._prepare_classification_prompt(content)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å Qwen3-8B
            classification_result = self._classify_with_workhorse(prompt)
            
            return {
                'classification_available': True,
                'document_type': classification_result.get('document_type', 'unknown'),
                'confidence': classification_result.get('confidence', 0.0),
                'subtype': classification_result.get('subtype', ''),
                'model': 'Qwen3-8B',
                'context_length': len(prompt)
            }
            
        except Exception as e:
            self.logger.error(f"Stage 4 classification failed: {e}")
            return {'classification_available': False, 'error': str(e)}
    
    def stage_8_extract_metadata(self, content: str, structural_data: Dict) -> Dict:
        """
        Stage 8: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (RuT5)
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å QA –ø–æ–¥—Ö–æ–¥–æ–º
        """
        try:
            if self.extraction_model is None:
                return {'extraction_available': False, 'error': 'Extraction model not loaded'}
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å RuT5
            metadata = self._extract_metadata_with_specialist(content, structural_data)
            
            return {
                'extraction_available': True,
                'metadata': metadata,
                'model': 'RuT5',
                'fields_extracted': len(metadata)
            }
            
        except Exception as e:
            self.logger.error(f"Stage 8 extraction failed: {e}")
            return {'extraction_available': False, 'error': str(e)}
    
    def stage_5_5_deep_analysis(self, content: str, vlm_metadata: Dict) -> Dict:
        """
        Stage 5.5: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (Qwen3-8B)
        –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ä–∞–±–æ—á–µ–π –ª–æ—à–∞–¥–∫–æ–π
        """
        try:
            if self.workhorse_model is None:
                return {'analysis_available': False, 'error': 'Workhorse model not loaded'}
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å Qwen3-8B
            analysis_result = self._analyze_with_workhorse(content, vlm_metadata)
            
            return {
                'analysis_available': True,
                'analysis': analysis_result,
                'model': 'Qwen3-8B',
                'enhanced_sections': len(analysis_result.get('sections', [])),
                'extracted_entities': len(analysis_result.get('entities', []))
            }
            
        except Exception as e:
            self.logger.error(f"Stage 5.5 analysis failed: {e}")
            return {'analysis_available': False, 'error': str(e)}
    
    def stage_10_complex_logic(self, task_description: str, context: str) -> Dict:
        """
        Stage 10: –°–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ (Qwen3-8B)
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Cypher –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ —Å–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞
        """
        try:
            if self.workhorse_model is None:
                return {'complex_logic_available': False, 'error': 'Workhorse model not loaded'}
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É —Å Qwen3-8B
            result = self._execute_complex_logic_with_workhorse(task_description, context)
            
            return {
                'complex_logic_available': True,
                'result': result,
                'model': 'Qwen3-8B',
                'task_type': 'complex_logic'
            }
            
        except Exception as e:
            self.logger.error(f"Stage 10 complex logic failed: {e}")
            return {'complex_logic_available': False, 'error': str(e)}
    
    def _prepare_classification_prompt(self, content: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (Qwen3-8B)"""
        max_length = self.config.workhorse_max_length - 500
        truncated_content = content[:max_length] if len(content) > max_length else content
        
        prompt = f"""<|im_start|>system
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é.
<|im_end|>
<|im_start|>user
–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞:

{truncated_content}

–¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
- –°–ù–∏–ü (–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –ø—Ä–∞–≤–∏–ª–∞)
- –°–ü (–°–≤–æ–¥—ã –ø—Ä–∞–≤–∏–ª)
- –ì–û–°–¢ (–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- –ü–ü–† (–ü—Ä–æ–µ–∫—Ç –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç)
- –°–º–µ—Ç–∞ (–õ–æ–∫–∞–ª—å–Ω–∞—è —Å–º–µ—Ç–∞)
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä–µ–≥–ª–∞–º–µ–Ω—Ç
- –ü—Ä–∏–∫–∞–∑/–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
- –ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- –î—Ä—É–≥–æ–µ

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "document_type": "–æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø",
    "subtype": "–ø–æ–¥—Ç–∏–ø –∏–ª–∏ –Ω–æ–º–µ—Ä",
    "confidence": 0.95,
    "keywords": ["–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"]
}}
<|im_end|>
<|im_start|>assistant"""
        
        return prompt
    
    def _get_model_device(self, model):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
        try:
            return next(model.parameters()).device
        except StopIteration:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—Ä–µ–¥–∫–æ), –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º CPU
            return torch.device('cpu')
    
    def _move_inputs_to_model_device(self, inputs, model, model_name="Model"):
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏"""
        device = self._get_model_device(model)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        self.logger.debug(f"üîß {model_name} device: {device}, inputs device: {inputs['input_ids'].device}")
        return inputs

    def _classify_with_workhorse(self, prompt: str) -> Dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å Qwen3-8B"""
        try:
            inputs = self.workhorse_tokenizer(
                prompt, 
                return_tensors="pt", 
                max_length=self.config.workhorse_max_length, 
                truncation=True
            )
            
            # ‚úÖ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            inputs = self._move_inputs_to_model_device(inputs, self.workhorse_model, "Qwen3-8B")
            
            with torch.no_grad():
                outputs = self.workhorse_model.generate(
                    **inputs,
                    max_length=min(len(inputs['input_ids'][0]) + 200, self.config.workhorse_max_length),
                    temperature=self.config.temperature,
                    do_sample=True,
                    pad_token_id=self.workhorse_tokenizer.eos_token_id
                )
            
            response = self.workhorse_tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = generated_text[json_start:json_end]
                return json.loads(json_text)
            else:
                return {"document_type": "unknown", "confidence": 0.0}
                
        except Exception as e:
            self.logger.error(f"Classification error: {e}")
            return {"document_type": "unknown", "confidence": 0.0}
    
    def _extract_metadata_with_specialist(self, content: str, structural_data: Dict) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å RuT5"""
        metadata = {}
        
        # –°–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        qa_questions = [
            ("–î–∞—Ç–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞", "date_approval"),
            ("–ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞", "document_number"),
            ("–ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏", "organization"),
            ("–î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è –≤ –¥–µ–π—Å—Ç–≤–∏–µ", "date_effective"),
            ("–û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è", "scope"),
            ("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞", "keywords")
        ]
        
        for question, field in qa_questions:
            try:
                qa_prompt = f"""–ù–∞–π–¥–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}

–î–û–ö–£–ú–ï–ù–¢:
{content[:2000]}

–û—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞):"""
                
                inputs = self.extraction_tokenizer(qa_prompt, return_tensors="pt", max_length=self.config.extraction_max_length, truncation=True)
                
                # ‚úÖ –£–ù–ò–í–ï–†–°–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ RuT5
                inputs = self._move_inputs_to_model_device(inputs, self.extraction_model, "RuT5")
                
                with torch.no_grad():
                    outputs = self.extraction_model.generate(
                        **inputs,
                        max_length=min(len(inputs['input_ids'][0]) + 100, self.config.extraction_max_length),
                        temperature=0.1,
                        do_sample=True,
                        pad_token_id=self.extraction_tokenizer.eos_token_id
                    )
                
                response = self.extraction_tokenizer.decode(outputs[0], skip_special_tokens=True)
                answer = response[len(qa_prompt):].strip()
                
                if answer and len(answer) < 200:
                    metadata[field] = answer
                    
            except Exception as e:
                self.logger.warning(f"Failed to extract {field}: {e}")
                continue
        
        return metadata
    
    def _analyze_with_workhorse(self, content: str, vlm_metadata: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å Qwen3-8B"""
        try:
            prompt = f"""<|im_start|>system
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
<|im_end|>
<|im_start|>user
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç:

{content[:3000]}

VLM –ê–ù–ê–õ–ò–ó:
- –¢–∞–±–ª–∏—Ü—ã: {len(vlm_metadata.get('tables', []))}
- –°–µ–∫—Ü–∏–∏: {len(vlm_metadata.get('sections', []))}

–ò–∑–≤–ª–µ–∫–∏:
1. –í—Å–µ —Å–µ–∫—Ü–∏–∏ –∏ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
2. –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–¥–∞—Ç—ã, –Ω–æ–º–µ—Ä–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)
3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "sections": [
        {{"title": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "content": "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ", "level": 1}}
    ],
    "entities": [
        {{"text": "—Å—É—â–Ω–æ—Å—Ç—å", "type": "—Ç–∏–ø", "confidence": 0.9}}
    ],
    "relations": [
        {{"source": "–∏—Å—Ç–æ—á–Ω–∏–∫", "target": "—Ü–µ–ª—å", "type": "—Å–≤—è–∑—å"}}
    ]
}}
<|im_end|>
<|im_start|>assistant"""
            
            inputs = self.workhorse_tokenizer(prompt, return_tensors="pt", max_length=self.config.workhorse_max_length, truncation=True)
            
            if not hasattr(self.workhorse_model, 'hf_quantizer'):
                inputs = {k: v.to(self.workhorse_model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.workhorse_model.generate(
                    **inputs,
                    max_length=min(len(inputs['input_ids'][0]) + 500, self.config.workhorse_max_length),
                    temperature=self.config.temperature,
                    do_sample=True,
                    pad_token_id=self.workhorse_tokenizer.eos_token_id
                )
            
            response = self.workhorse_tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = generated_text[json_start:json_end]
                return json.loads(json_text)
            else:
                return {"sections": [], "entities": [], "relations": []}
                
        except Exception as e:
            self.logger.error(f"Analysis error: {e}")
            return {"sections": [], "entities": [], "relations": []}
    
    def _execute_complex_logic_with_workhorse(self, task_description: str, context: str) -> Dict:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–∏ —Å Qwen3-8B"""
        try:
            prompt = f"""<|im_start|>system
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–µ –¥–ª—è —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –í—ã–ø–æ–ª–Ω–∏ —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.
<|im_end|>
<|im_start|>user
–ó–∞–¥–∞—á–∞: {task_description}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í—ã–ø–æ–ª–Ω–∏ –∑–∞–¥–∞—á—É –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
<|im_end|>
<|im_start|>assistant"""
            
            inputs = self.workhorse_tokenizer(prompt, return_tensors="pt", max_length=self.config.workhorse_max_length, truncation=True)
            
            if not hasattr(self.workhorse_model, 'hf_quantizer'):
                inputs = {k: v.to(self.workhorse_model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.workhorse_model.generate(
                    **inputs,
                    max_length=min(len(inputs['input_ids'][0]) + 1000, self.config.workhorse_max_length),
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=self.workhorse_tokenizer.eos_token_id
                )
            
            response = self.workhorse_tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            return {
                'task_result': generated_text,
                'task_type': 'complex_logic',
                'model_used': 'Qwen3-8B'
            }
            
        except Exception as e:
            self.logger.error(f"Complex logic error: {e}")
            return {'error': str(e)}
    
    def get_ensemble_info(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–Ω—Å–∞–º–±–ª–µ"""
        return {
            'workhorse_model': self.config.workhorse_model,
            'extraction_model': self.config.extraction_model,
            'workhorse_loaded': self.workhorse_model is not None,
            'extraction_loaded': self.extraction_model is not None,
            'device': str(self.workhorse_model.device) if self.workhorse_model is not None else 'unknown',
            'extraction_device': str(self.extraction_model.device) if self.extraction_model is not None else 'unknown',
            'vram_optimized': True,
            'quantization': '4bit' if self.config.use_4bit_quantization else '8bit' if self.config.use_8bit_quantization else 'none',
            'strategy': 'Qwen3-8B (workhorse) + RuT5 (specialist)'
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è RTX 4060
    config = WorkhorseLLMConfig(
        workhorse_model="Qwen/Qwen3-8B",
        extraction_model="ai-forever/ruT5-base",
        use_4bit_quantization=True,
        max_memory_gb=7.0
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    ensemble = WorkhorseLLMEnsemble(config)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    test_content = "–°–ù–∏–ü 2.01.07-85* –ù–∞–≥—Ä—É–∑–∫–∏ –∏ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è. –£—Ç–≤–µ—Ä–∂–¥–µ–Ω 29.12.2020."
    
    # Stage 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    classification = ensemble.stage_4_classify_document(test_content)
    print(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {classification}")
    
    # Stage 8: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata = ensemble.stage_8_extract_metadata(test_content, {})
    print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata}")
    
    # Stage 5.5: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
    analysis = ensemble.stage_5_5_deep_analysis(test_content, {})
    print(f"–ê–Ω–∞–ª–∏–∑: {analysis}")
    
    # Stage 10: –°–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞
    complex_logic = ensemble.stage_10_complex_logic(
        "–°–æ–∑–¥–∞–π Cypher –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –Ω–∞–≥—Ä—É–∑–∫–∞–º–∏",
        test_content
    )
    print(f"–°–ª–æ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞: {complex_logic}")
    
    print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω—Å–∞–º–±–ª–µ: {ensemble.get_ensemble_info()}")
