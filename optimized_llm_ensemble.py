"""
üöÄ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å LLM –¥–ª—è RTX 4060 (8GB VRAM)
Qwen3-8B (—Ä–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞) + RuT5 (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ) + Qwen3-Coder-30B (—Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏)
"""

import logging
import torch
import json
import time
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
from transformers import (
    AutoModelForCausalLM, 
    AutoTokenizer, 
    AutoModelForSeq2SeqLM,
    BitsAndBytesConfig
)

logger = logging.getLogger(__name__)

@dataclass
class OptimizedLLMConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è LLM"""
    # –†–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞: Qwen3-8B –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞
    workhorse_model: str = "Qwen/Qwen2.5-7B-Instruct"  # Qwen3-8B
    workhorse_max_length: int = 4096
    
    # –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: RuT5 –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    extraction_model: str = "ai-forever/rugpt3large_based_on_gpt2"  # Fallback –∫ RuGPT
    extraction_max_length: int = 2048
    
    # –¢—è–∂–µ–ª–æ–≤–µ—Å: Qwen3-Coder-30B –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
    heavy_model: str = "Qwen/Qwen2.5-Coder-32B-Instruct"  # Qwen3-Coder-30B
    heavy_model_max_length: int = 8192
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ VRAM –¥–ª—è RTX 4060 (8GB)
    use_4bit_quantization: bool = True
    use_8bit_quantization: bool = False
    max_memory_gb: float = 7.0  # –û—Å—Ç–∞–≤–ª—è–µ–º 1GB –¥–ª—è —Å–∏—Å—Ç–µ–º—ã
    
    # –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    device: str = "auto"
    cache_dir: str = "./models_cache"
    temperature: float = 0.1

class OptimizedLLMEnsemble:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å LLM –¥–ª—è RTX 4060
    Qwen3-8B (—Ä–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞) + RuT5 (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ) + Qwen3-Coder-30B (—Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏)
    """
    
    def __init__(self, config: OptimizedLLMConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
        self.workhorse_model = None
        self.workhorse_tokenizer = None
        self.extraction_model = None
        self.extraction_tokenizer = None
        self.heavy_model = None
        self.heavy_tokenizer = None
        
        self._initialize_models()
    
    def _initialize_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è"""
        try:
            self.logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è LLM...")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 4060
            quantization_config = self._get_quantization_config()
            
            # 1. –†–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞: Qwen3-8B (–≤—Å–µ–≥–¥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)
            self._initialize_workhorse_model(quantization_config)
            
            # 2. –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç: RuT5 (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ø–æ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—é)
            self._initialize_extraction_model(quantization_config)
            
            # 3. –¢—è–∂–µ–ª–æ–≤–µ—Å: Qwen3-Coder-30B (–∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
            # self._initialize_heavy_model(quantization_config)  # –û—Ç–ª–æ–∂–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
            
            self.logger.info("‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω—Å–∞–º–±–ª—è: {e}")
            raise
    
    def _get_quantization_config(self) -> Optional[BitsAndBytesConfig]:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –¥–ª—è RTX 4060"""
        if not torch.cuda.is_available():
            return None
        
        if self.config.use_4bit_quantization:
            return BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
            )
        elif self.config.use_8bit_quantization:
            return BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
            )
        
        return None
    
    def _initialize_workhorse_model(self, quantization_config):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—á–µ–π –ª–æ—à–∞–¥–∫–∏: Qwen3-8B"""
        try:
            self.logger.info(f"üêé –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞–±–æ—á–µ–π –ª–æ—à–∞–¥–∫–∏: {self.config.workhorse_model}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.workhorse_tokenizer = AutoTokenizer.from_pretrained(
                self.config.workhorse_model,
                cache_dir=self.config.cache_dir,
                trust_remote_code=True
            )
            
            if self.workhorse_tokenizer.pad_token is None:
                self.workhorse_tokenizer.pad_token = self.workhorse_tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π VRAM
            self.workhorse_model = AutoModelForCausalLM.from_pretrained(
                self.config.workhorse_model,
                cache_dir=self.config.cache_dir,
                quantization_config=quantization_config,
                device_map=self.config.device,
                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True,
                max_memory={0: f"{self.config.max_memory_gb}GB"} if torch.cuda.is_available() else None
            )
            
            self.logger.info("‚úÖ –†–∞–±–æ—á–∞—è –ª–æ—à–∞–¥–∫–∞ (Qwen3-8B) –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            self.logger.info(f"üìä –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.workhorse_model.device}")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–±–æ—á–µ–π –ª–æ—à–∞–¥–∫–∏: {e}")
            self.workhorse_model = None
    
    def _initialize_extraction_model(self, quantization_config):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞: RuT5"""
        try:
            self.logger.info(f"üîç –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞: {self.config.extraction_model}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.extraction_tokenizer = AutoTokenizer.from_pretrained(
                self.config.extraction_model,
                cache_dir=self.config.cache_dir,
                trust_remote_code=True
            )
            
            if self.extraction_tokenizer.pad_token is None:
                self.extraction_tokenizer.pad_token = self.extraction_tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.extraction_model = AutoModelForCausalLM.from_pretrained(
                self.config.extraction_model,
                cache_dir=self.config.cache_dir,
                quantization_config=quantization_config,
                device_map=self.config.device,
                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
                trust_remote_code=True,
                max_memory={0: f"{self.config.max_memory_gb}GB"} if torch.cuda.is_available() else None
            )
            
            self.logger.info("‚úÖ –°–ø–µ—Ü–∏–∞–ª–∏—Å—Ç (RuT5) –∑–∞–≥—Ä—É–∂–µ–Ω")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–∞: {e}")
            self.extraction_model = None
    
    def _initialize_heavy_model(self, quantization_config):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—è–∂–µ–ª–æ–≤–µ—Å–∞: Qwen3-Coder-30B (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)"""
        try:
            self.logger.info(f"üèãÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—è–∂–µ–ª–æ–≤–µ—Å–∞: {self.config.heavy_model}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            self.heavy_tokenizer = AutoTokenizer.from_pretrained(
                self.config.heavy_model,
                cache_dir=self.config.cache_dir,
                trust_remote_code=True
            )
            
            if self.heavy_tokenizer.pad_token is None:
                self.heavy_tokenizer.pad_token = self.heavy_tokenizer.eos_token
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π VRAM
            self.heavy_model = AutoModelForCausalLM.from_pretrained(
                self.config.heavy_model,
                cache_dir=self.config.cache_dir,
                quantization_config=quantization_config,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                trust_remote_code=True,
                max_memory={0: f"{self.config.max_memory_gb}GB"}
            )
            
            self.logger.info("‚úÖ –¢—è–∂–µ–ª–æ–≤–µ—Å (Qwen3-Coder-30B) –∑–∞–≥—Ä—É–∂–µ–Ω")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ç—è–∂–µ–ª–æ–≤–µ—Å–∞: {e}")
            self.heavy_model = None
    
    def stage_4_classify_document(self, content: str) -> Dict:
        """
        Stage 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (Qwen3-8B)
        –ë—ã—Å—Ç—Ä–∞—è –∏ —Ç–æ—á–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        """
        try:
            if self.workhorse_model is None:
                return {'classification_available': False, 'error': 'Workhorse model not loaded'}
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Qwen3-8B
            prompt = self._prepare_classification_prompt(content)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º —Å Qwen3-8B
            classification_result = self._classify_with_workhorse(prompt)
            
            return {
                'classification_available': True,
                'document_type': classification_result.get('document_type', 'unknown'),
                'confidence': classification_result.get('confidence', 0.0),
                'subtype': classification_result.get('subtype', ''),
                'model': 'Qwen3-8B',
                'context_length': len(prompt)
            }
            
        except Exception as e:
            self.logger.error(f"Stage 4 classification failed: {e}")
            return {'classification_available': False, 'error': str(e)}
    
    def stage_8_extract_metadata(self, content: str, structural_data: Dict) -> Dict:
        """
        Stage 8: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (RuT5)
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å QA –ø–æ–¥—Ö–æ–¥–æ–º
        """
        try:
            if self.extraction_model is None:
                return {'extraction_available': False, 'error': 'Extraction model not loaded'}
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å RuT5
            metadata = self._extract_metadata_with_specialist(content, structural_data)
            
            return {
                'extraction_available': True,
                'metadata': metadata,
                'model': 'RuT5',
                'fields_extracted': len(metadata)
            }
            
        except Exception as e:
            self.logger.error(f"Stage 8 extraction failed: {e}")
            return {'extraction_available': False, 'error': str(e)}
    
    def stage_5_5_deep_analysis(self, content: str, vlm_metadata: Dict) -> Dict:
        """
        Stage 5.5: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ (Qwen3-8B)
        –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ —Å —Ä–∞–±–æ—á–µ–π –ª–æ—à–∞–¥–∫–æ–π
        """
        try:
            if self.workhorse_model is None:
                return {'analysis_available': False, 'error': 'Workhorse model not loaded'}
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å Qwen3-8B
            analysis_result = self._analyze_with_workhorse(content, vlm_metadata)
            
            return {
                'analysis_available': True,
                'analysis': analysis_result,
                'model': 'Qwen3-8B',
                'enhanced_sections': len(analysis_result.get('sections', [])),
                'extracted_entities': len(analysis_result.get('entities', []))
            }
            
        except Exception as e:
            self.logger.error(f"Stage 5.5 analysis failed: {e}")
            return {'analysis_available': False, 'error': str(e)}
    
    def complex_task_with_heavy_model(self, task_description: str, context: str) -> Dict:
        """
        –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏ —Å Qwen3-Coder-30B
        –¢–æ–ª—å–∫–æ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã—Ö –∑–∞–¥–∞—á
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—è–∂–µ–ª–æ–≤–µ—Å —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            if self.heavy_model is None:
                self.logger.info("üèãÔ∏è –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—è–∂–µ–ª–æ–≤–µ—Å –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏...")
                quantization_config = self._get_quantization_config()
                self._initialize_heavy_model(quantization_config)
            
            if self.heavy_model is None:
                return {'heavy_task_available': False, 'error': 'Heavy model not loaded'}
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É
            result = self._execute_heavy_task(task_description, context)
            
            return {
                'heavy_task_available': True,
                'result': result,
                'model': 'Qwen3-Coder-30B',
                'task_type': 'complex_analysis'
            }
            
        except Exception as e:
            self.logger.error(f"Heavy model task failed: {e}")
            return {'heavy_task_available': False, 'error': str(e)}
    
    def _prepare_classification_prompt(self, content: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (Qwen3-8B)"""
        max_length = self.config.workhorse_max_length - 500
        truncated_content = content[:max_length] if len(content) > max_length else content
        
        prompt = f"""<|im_start|>system
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é.
<|im_end|>
<|im_start|>user
–û–ø—Ä–µ–¥–µ–ª–∏ —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞:

{truncated_content}

–¢–∏–ø—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
- –°–ù–∏–ü (–°—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ—Ä–º—ã –∏ –ø—Ä–∞–≤–∏–ª–∞)
- –°–ü (–°–≤–æ–¥—ã –ø—Ä–∞–≤–∏–ª)
- –ì–û–°–¢ (–ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç)
- –ü–ü–† (–ü—Ä–æ–µ–∫—Ç –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç)
- –°–º–µ—Ç–∞ (–õ–æ–∫–∞–ª—å–Ω–∞—è —Å–º–µ—Ç–∞)
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Ä–µ–≥–ª–∞–º–µ–Ω—Ç
- –ü—Ä–∏–∫–∞–∑/–ü–æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
- –ü—Ä–æ–µ–∫—Ç–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
- –î—Ä—É–≥–æ–µ

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "document_type": "–æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∏–ø",
    "subtype": "–ø–æ–¥—Ç–∏–ø –∏–ª–∏ –Ω–æ–º–µ—Ä",
    "confidence": 0.95,
    "keywords": ["–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"]
}}
<|im_end|>
<|im_start|>assistant"""
        
        return prompt
    
    def _classify_with_workhorse(self, prompt: str) -> Dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å Qwen3-8B"""
        try:
            inputs = self.workhorse_tokenizer(
                prompt, 
                return_tensors="pt", 
                max_length=self.config.workhorse_max_length, 
                truncation=True
            )
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if not hasattr(self.workhorse_model, 'hf_quantizer'):
                inputs = {k: v.to(self.workhorse_model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.workhorse_model.generate(
                    **inputs,
                    max_length=min(len(inputs['input_ids'][0]) + 200, self.config.workhorse_max_length),
                    temperature=self.config.temperature,
                    do_sample=True,
                    pad_token_id=self.workhorse_tokenizer.eos_token_id
                )
            
            response = self.workhorse_tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = generated_text[json_start:json_end]
                return json.loads(json_text)
            else:
                return {"document_type": "unknown", "confidence": 0.0}
                
        except Exception as e:
            self.logger.error(f"Classification error: {e}")
            return {"document_type": "unknown", "confidence": 0.0}
    
    def _extract_metadata_with_specialist(self, content: str, structural_data: Dict) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å RuT5"""
        metadata = {}
        
        # –°–ø–∏—Å–æ–∫ –ø–æ–ª–µ–π –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        qa_questions = [
            ("–î–∞—Ç–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞", "date_approval"),
            ("–ù–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞", "document_number"),
            ("–ù–∞–∑–≤–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏", "organization"),
            ("–î–∞—Ç–∞ –≤–≤–µ–¥–µ–Ω–∏—è –≤ –¥–µ–π—Å—Ç–≤–∏–µ", "date_effective"),
            ("–û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è", "scope"),
            ("–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞", "keywords")
        ]
        
        for question, field in qa_questions:
            try:
                qa_prompt = f"""–ù–∞–π–¥–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}

–î–û–ö–£–ú–ï–ù–¢:
{content[:2000]}

–û—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞):"""
                
                inputs = self.extraction_tokenizer(qa_prompt, return_tensors="pt", max_length=self.config.extraction_max_length, truncation=True)
                
                if not hasattr(self.extraction_model, 'hf_quantizer'):
                    inputs = {k: v.to(self.extraction_model.device) for k, v in inputs.items()}
                
                with torch.no_grad():
                    outputs = self.extraction_model.generate(
                        **inputs,
                        max_length=min(len(inputs['input_ids'][0]) + 100, self.config.extraction_max_length),
                        temperature=0.1,
                        do_sample=True,
                        pad_token_id=self.extraction_tokenizer.eos_token_id
                    )
                
                response = self.extraction_tokenizer.decode(outputs[0], skip_special_tokens=True)
                answer = response[len(qa_prompt):].strip()
                
                if answer and len(answer) < 200:
                    metadata[field] = answer
                    
            except Exception as e:
                self.logger.warning(f"Failed to extract {field}: {e}")
                continue
        
        return metadata
    
    def _analyze_with_workhorse(self, content: str, vlm_metadata: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —Å Qwen3-8B"""
        try:
            prompt = f"""<|im_start|>system
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞.
<|im_end|>
<|im_start|>user
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç:

{content[:3000]}

VLM –ê–ù–ê–õ–ò–ó:
- –¢–∞–±–ª–∏—Ü—ã: {len(vlm_metadata.get('tables', []))}
- –°–µ–∫—Ü–∏–∏: {len(vlm_metadata.get('sections', []))}

–ò–∑–≤–ª–µ–∫–∏:
1. –í—Å–µ —Å–µ–∫—Ü–∏–∏ –∏ –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
2. –ò–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ (–¥–∞—Ç—ã, –Ω–æ–º–µ—Ä–∞, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏)
3. –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ —Å–≤—è–∑–∏ –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏

–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
{{
    "sections": [
        {{"title": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "content": "—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ", "level": 1}}
    ],
    "entities": [
        {{"text": "—Å—É—â–Ω–æ—Å—Ç—å", "type": "—Ç–∏–ø", "confidence": 0.9}}
    ],
    "relations": [
        {{"source": "–∏—Å—Ç–æ—á–Ω–∏–∫", "target": "—Ü–µ–ª—å", "type": "—Å–≤—è–∑—å"}}
    ]
}}
<|im_end|>
<|im_start|>assistant"""
            
            inputs = self.workhorse_tokenizer(prompt, return_tensors="pt", max_length=self.config.workhorse_max_length, truncation=True)
            
            if not hasattr(self.workhorse_model, 'hf_quantizer'):
                inputs = {k: v.to(self.workhorse_model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.workhorse_model.generate(
                    **inputs,
                    max_length=min(len(inputs['input_ids'][0]) + 500, self.config.workhorse_max_length),
                    temperature=self.config.temperature,
                    do_sample=True,
                    pad_token_id=self.workhorse_tokenizer.eos_token_id
                )
            
            response = self.workhorse_tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            # –ü–∞—Ä—Å–∏–º JSON
            json_start = generated_text.find('{')
            json_end = generated_text.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_text = generated_text[json_start:json_end]
                return json.loads(json_text)
            else:
                return {"sections": [], "entities": [], "relations": []}
                
        except Exception as e:
            self.logger.error(f"Analysis error: {e}")
            return {"sections": [], "entities": [], "relations": []}
    
    def _execute_heavy_task(self, task_description: str, context: str) -> Dict:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏ —Å Qwen3-Coder-30B"""
        try:
            prompt = f"""<|im_start|>system
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Å–ª–æ–∂–Ω–æ–º—É –∞–Ω–∞–ª–∏–∑—É —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –í—ã–ø–æ–ª–Ω–∏ —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é.
<|im_end|>
<|im_start|>user
–ó–∞–¥–∞—á–∞: {task_description}

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í—ã–ø–æ–ª–Ω–∏ –∑–∞–¥–∞—á—É –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
<|im_end|>
<|im_start|>assistant"""
            
            inputs = self.heavy_tokenizer(prompt, return_tensors="pt", max_length=self.config.heavy_model_max_length, truncation=True)
            
            if not hasattr(self.heavy_model, 'hf_quantizer'):
                inputs = {k: v.to(self.heavy_model.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.heavy_model.generate(
                    **inputs,
                    max_length=min(len(inputs['input_ids'][0]) + 1000, self.config.heavy_model_max_length),
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=self.heavy_tokenizer.eos_token_id
                )
            
            response = self.heavy_tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_text = response[len(prompt):].strip()
            
            return {
                'task_result': generated_text,
                'task_type': 'complex_analysis',
                'model_used': 'Qwen3-Coder-30B'
            }
            
        except Exception as e:
            self.logger.error(f"Heavy task error: {e}")
            return {'error': str(e)}
    
    def get_ensemble_info(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–Ω—Å–∞–º–±–ª–µ"""
        return {
            'workhorse_model': self.config.workhorse_model,
            'extraction_model': self.config.extraction_model,
            'heavy_model': self.config.heavy_model,
            'workhorse_loaded': self.workhorse_model is not None,
            'extraction_loaded': self.extraction_model is not None,
            'heavy_loaded': self.heavy_model is not None,
            'device': str(self.workhorse_model.device) if self.workhorse_model is not None else 'unknown',
            'vram_optimized': True,
            'quantization': '4bit' if self.config.use_4bit_quantization else '8bit' if self.config.use_8bit_quantization else 'none'
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è RTX 4060
    config = OptimizedLLMConfig(
        workhorse_model="Qwen/Qwen2.5-7B-Instruct",
        extraction_model="ai-forever/rugpt3large_based_on_gpt2",
        heavy_model="Qwen/Qwen2.5-Coder-32B-Instruct",
        use_4bit_quantization=True,
        max_memory_gb=7.0
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    ensemble = OptimizedLLMEnsemble(config)
    
    # –¢–µ—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    test_content = "–°–ù–∏–ü 2.01.07-85* –ù–∞–≥—Ä—É–∑–∫–∏ –∏ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è. –£—Ç–≤–µ—Ä–∂–¥–µ–Ω 29.12.2020."
    
    # Stage 4: –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
    classification = ensemble.stage_4_classify_document(test_content)
    print(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {classification}")
    
    # Stage 8: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    metadata = ensemble.stage_8_extract_metadata(test_content, {})
    print(f"–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata}")
    
    # Stage 5.5: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑
    analysis = ensemble.stage_5_5_deep_analysis(test_content, {})
    print(f"–ê–Ω–∞–ª–∏–∑: {analysis}")
    
    print(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–Ω—Å–∞–º–±–ª–µ: {ensemble.get_ensemble_info()}")
