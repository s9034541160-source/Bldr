#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è PDF-—Ñ–∞–π–ª–æ–≤ —Å —Å–∞–π—Ç–∞ –ú–∏–Ω—Å—Ç—Ä–æ—è –†–æ—Å—Å–∏–∏
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
"""

import requests
from bs4 import BeautifulSoup
import os
import time
import logging
from urllib.parse import urljoin, urlparse
from datetime import datetime
import json
from pathlib import Path
import argparse
from typing import List, Dict, Optional
import sys

class InteractiveLogger:
    """–ö–ª–∞—Å—Å –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Å–∞–º–æ–∑–∞–º–µ–Ω—è—é—â–∏–º–∏—Å—è —Å—Ç—Ä–æ–∫–∞–º–∏"""
    
    def __init__(self):
        self.current_line = ""
        self.last_update = 0
        
    def update(self, message: str, force: bool = False):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–æ–∫—É –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        current_time = time.time()
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–ª–∏ –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ
        if force or current_time - self.last_update > 0.5:
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–æ–∫—É
            if self.current_line:
                sys.stdout.write('\r' + ' ' * len(self.current_line) + '\r')
            
            # –í—ã–≤–æ–¥–∏–º –Ω–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
            sys.stdout.write(f'\r{message}')
            sys.stdout.flush()
            
            self.current_line = message
            self.last_update = current_time
    
    def finish(self, message: str = ""):
        """–ó–∞–≤–µ—Ä—à–∞–µ—Ç —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–æ–∫—É –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏—Ç –Ω–∞ –Ω–æ–≤—É—é"""
        if self.current_line:
            sys.stdout.write('\r' + ' ' * len(self.current_line) + '\r')
        if message:
            print(message)
        self.current_line = ""

class MinstroyParser:
    def __init__(self, config_file: str = "minstroy_config.json"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        self.config = self._load_config(config_file)
        self.session = requests.Session()
        self._setup_session()
        self._setup_logging()
        self.interactive_logger = InteractiveLogger()
        
    def _load_config(self, config_file: str) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞ –∏–ª–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"""
        default_config = {
            "urls": [
                "https://minstroyrf.gov.ru/docs/?date_from=&t%5B%5D=76&d%5B%5D=&q=&active%5B%5D=65"
            ],
            "download_dir": "minstroy_pdfs",
            "headers": {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1"
            },
            "timeout": 30,
            "retry_attempts": 3,
            "retry_delay": 2,
            "chunk_size": 8192,
            "max_file_size": 100 * 1024 * 1024,  # 100MB
            "skip_existing": True,
            "log_level": "INFO",
            "max_pages_to_scan": 2000,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            "max_empty_pages": 10,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥—Ä—è–¥
            "page_delay": 2  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        }
        
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}. –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
        
        return default_config
    
    def _setup_session(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ HTTP-—Å–µ—Å—Å–∏–∏"""
        self.session.headers.update(self.config["headers"])
        self.session.timeout = self.config["timeout"]
        
    def _setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_level = getattr(logging, self.config["log_level"].upper(), logging.INFO)
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('minstroy_parser.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def _create_download_dir(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–æ–∫"""
        download_dir = Path(self.config["download_dir"])
        download_dir.mkdir(parents=True, exist_ok=True)
        self.logger.info(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–æ–∫: {download_dir.absolute()}")
        return download_dir
    
    def _is_valid_pdf_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Å—Å—ã–ª–∫–æ–π –Ω–∞ PDF"""
        parsed_url = urlparse(url)
        return (
            url.lower().endswith('.pdf') or 
            'pdf' in parsed_url.path.lower() or
            'application/pdf' in url.lower()
        )
    
    def _get_filename_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –∏–∑ URL"""
        parsed_url = urlparse(url)
        filename = os.path.basename(parsed_url.path)
        
        # –û—á–∏—Å—Ç–∫–∞ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if '?' in filename:
            filename = filename.split('?')[0]
        
        # –ï—Å–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ –ø—É—Å—Ç–æ–µ –∏–ª–∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        if not filename or not filename.endswith('.pdf'):
            filename = f"document_{int(time.time())}.pdf"
        
        # –ó–∞–º–µ–Ω–∞ –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        filename = "".join(c for c in filename if c.isalnum() or c in "._-")
        
        return filename
    
    def _download_pdf(self, url: str, filepath: Path) -> bool:
        """–°–∫–∞—á–∏–≤–∞–Ω–∏–µ PDF-—Ñ–∞–π–ª–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
        for attempt in range(self.config["retry_attempts"]):
            try:
                self.logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{self.config['retry_attempts']}: {url}")
                
                response = self.session.get(url, stream=True, timeout=self.config["timeout"])
                response.raise_for_status()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
                content_length = response.headers.get('content-length')
                if content_length and int(content_length) > self.config["max_file_size"]:
                    self.logger.warning(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {content_length} –±–∞–π—Ç")
                    return False
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                content_type = response.headers.get('content-type', '').lower()
                if 'pdf' not in content_type and not url.lower().endswith('.pdf'):
                    self.logger.warning(f"–ù–µ PDF —Ñ–∞–π–ª: {content_type}")
                    return False
                
                # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
                total_size = 0
                with open(filepath, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=self.config["chunk_size"]):
                        if chunk:
                            f.write(chunk)
                            total_size += len(chunk)
                            
                            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –≤–æ –≤—Ä–µ–º—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                            if total_size > self.config["max_file_size"]:
                                self.logger.warning("–§–∞–π–ª –ø—Ä–µ–≤—ã—à–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä, –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏")
                                f.close()
                                filepath.unlink(missing_ok=True)
                                return False
                
                self.logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω: {filepath.name} ({total_size} –±–∞–π—Ç)")
                return True
                
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}")
                if attempt < self.config["retry_attempts"] - 1:
                    time.sleep(self.config["retry_delay"])
                else:
                    self.logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å –ø–æ—Å–ª–µ {self.config['retry_attempts']} –ø–æ–ø—ã—Ç–æ–∫: {url}")
                    return False
            except Exception as e:
                self.logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {url}: {e}")
                return False
        
        return False
    
    def _save_download_log(self, downloaded_files: List[Dict], download_dir: Path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        log_file = download_dir / "download_log.json"
        log_data = {
            "download_date": datetime.now().isoformat(),
            "total_files": len(downloaded_files),
            "files": downloaded_files
        }
        
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(log_data, f, ensure_ascii=False, indent=2)
            self.logger.info(f"–õ–æ–≥ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {log_file}")
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–∞: {e}")
    
    
    def _extract_pdf_links_from_page(self, url: str) -> tuple[List[str], bool]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ PDF-—Å—Å—ã–ª–æ–∫ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ—Ç—É"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            links = soup.find_all('a', href=True)
            
            pdf_links = []
            for link in links:
                href = link.get('href', '')
                if self._is_valid_pdf_url(href):
                    full_url = urljoin(url, href)
                    pdf_links.append(full_url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –∫–æ–Ω—Ç–µ–Ω—Ç (–Ω–µ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞)
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            content_indicators = soup.find_all(['div', 'span', 'p'], string=lambda text: text and any(
                keyword in text.lower() for keyword in ['–¥–æ–∫—É–º–µ–Ω—Ç', '—Ñ–∞–π–ª', 'pdf', '—Å–∫–∞—á–∞—Ç—å', '–∑–∞–≥—Ä—É–∑–∏—Ç—å']
            ))
            
            # –ï—Å–ª–∏ –Ω–µ—Ç PDF-—Å—Å—ã–ª–æ–∫ –∏ –Ω–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π
            is_empty = len(pdf_links) == 0 and len(content_indicators) == 0
            
            return pdf_links, is_empty
            
        except requests.exceptions.RequestException as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return [], True
    
    def _process_single_page(self, page_url: str, download_dir: Path, page_num: int) -> Dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ–º PDF-—Ñ–∞–π–ª–æ–≤"""
        # –ò–∑–≤–ª–µ–∫–∞–µ–º PDF-—Å—Å—ã–ª–∫–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        pdf_links, is_empty = self._extract_pdf_links_from_page(page_url)
        
        if is_empty:
            return {
                "page_url": page_url,
                "page_num": page_num,
                "pdf_links_found": 0,
                "downloaded": 0,
                "skipped": 0,
                "failed": 0,
                "is_empty": True
            }
        
        # –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–∞–π–ª—ã —Å—Ä–∞–∑—É
        downloaded_files = []
        skipped_files = []
        failed_files = []
        
        for i, pdf_url in enumerate(pdf_links, 1):
            filename = self._get_filename_from_url(pdf_url)
            filepath = download_dir / filename
            
            # –ü—Ä–æ–ø—É—Å–∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤
            if self.config["skip_existing"] and filepath.exists():
                skipped_files.append({"url": pdf_url, "filename": filename, "reason": "already_exists"})
                continue
            
            # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
            if self._download_pdf(pdf_url, filepath):
                file_info = {
                    "url": pdf_url,
                    "filename": filename,
                    "filepath": str(filepath),
                    "size": filepath.stat().st_size if filepath.exists() else 0,
                    "download_time": datetime.now().isoformat(),
                    "page_num": page_num
                }
                downloaded_files.append(file_info)
            else:
                failed_files.append({"url": pdf_url, "filename": filename, "reason": "download_failed"})
        
        return {
            "page_url": page_url,
            "page_num": page_num,
            "pdf_links_found": len(pdf_links),
            "downloaded": len(downloaded_files),
            "skipped": len(skipped_files),
            "failed": len(failed_files),
            "downloaded_files": downloaded_files,
            "skipped_files": skipped_files,
            "failed_files": failed_files,
            "is_empty": False
        }

    def _find_pagination_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """–ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥—Ä—É–≥–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        pagination_links = []
        
        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –≤ HTML
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if any(param in href for param in ['PAGEN_1=', 'page=', 'p=']):
                full_url = urljoin(base_url, href)
                if full_url not in pagination_links:
                    pagination_links.append(full_url)
        
        # –ü–æ–∏—Å–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –≤ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
        max_page = 1
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if 'PAGEN_1=' in href:
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ URL
                    page_part = href.split('PAGEN_1=')[1].split('&')[0]
                    page_num = int(page_part)
                    max_page = max(max_page, page_num)
                except (ValueError, IndexError):
                    continue
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–∞–≥–∏–Ω–∞—Ü–∏—é –≤ HTML, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        if max_page == 1:
            max_page = self.config.get("max_pages_to_scan", 2000)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫—É –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–∞ –ø–∞–≥–∏–Ω–∞—Ü–∏—è –¥–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {max_page}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º URL –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        for page_num in range(2, max_page + 1):
            # –û—Å–Ω–æ–≤–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç —Å PAGEN_1
            page_url = f"{base_url}?date_from=&t%5B0%5D=60&d%5B0%5D=&q=&active%5B0%5D=65&PAGEN_1={page_num}"
            pagination_links.append(page_url)
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã URL
            alt_url1 = f"{base_url}?date_from=&t%5B%5D=60&d%5B%5D=&q=&active%5B%5D=65&PAGEN_1={page_num}"
            if alt_url1 not in pagination_links:
                pagination_links.append(alt_url1)
            
            alt_url2 = f"{base_url}?page={page_num}"
            if alt_url2 not in pagination_links:
                pagination_links.append(alt_url2)
        
        return pagination_links

    def parse_and_download(self) -> Dict:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü"""
        start_time = time.time()
        download_dir = self._create_download_dir()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        urls_to_process = self.config.get("urls", [self.config.get("url", "")])
        if not urls_to_process or not urls_to_process[0]:
            return {"success": False, "error": "–ù–µ —É–∫–∞–∑–∞–Ω—ã URL –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"}
        
        print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ {len(urls_to_process)} —Ä–∞–∑–¥–µ–ª–æ–≤ —Å–∞–π—Ç–∞ –ú–∏–Ω—Å—Ç—Ä–æ—è")
        print("=" * 60)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_stats = {
            "all_downloaded_files": [],
            "all_skipped_files": [],
            "all_failed_files": [],
            "total_pages": 0,
            "total_urls_processed": 0
        }
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π URL
        for url_index, base_url in enumerate(urls_to_process, 1):
            print(f"\nüìÇ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–¥–µ–ª–∞ {url_index}/{len(urls_to_process)}")
            print(f"üîó URL: {base_url}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
            try:
                self.interactive_logger.update(f"‚è≥ –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ä–∞–∑–¥–µ–ª–∞ {url_index}...")
                response = self.session.get(base_url)
                response.raise_for_status()
                self.interactive_logger.finish(f"‚úÖ –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (HTTP {response.status_code})")
            except requests.exceptions.RequestException as e:
                self.interactive_logger.finish(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
                continue
            
            # –ü–∞—Ä—Å–∏–Ω–≥ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
            soup = BeautifulSoup(response.content, 'html.parser')
            pagination_links = self._find_pagination_links(soup, base_url)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            all_pages = [base_url] + pagination_links
            
            self.interactive_logger.finish(f"üìÑ –ù–∞–π–¥–µ–Ω–æ {len(all_pages)} —Å—Ç—Ä–∞–Ω–∏—Ü –≤ —Ä–∞–∑–¥–µ–ª–µ {url_index}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞
            section_stats = {
                "downloaded_files": [],
                "skipped_files": [],
                "failed_files": [],
                "processed_pages": 0,
                "empty_pages_count": 0
            }
            
            max_empty_pages = self.config.get("max_empty_pages", 10)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
            for page_num, page_url in enumerate(all_pages, 1):
                # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                progress_msg = (f"üìä –†–∞–∑–¥–µ–ª {url_index}/{len(urls_to_process)} | "
                              f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num}/{len(all_pages)} | "
                              f"–°–∫–∞—á–∞–Ω–æ: {len(section_stats['downloaded_files'])} | "
                              f"–ü—Ä–æ–ø—É—â–µ–Ω–æ: {len(section_stats['skipped_files'])} | "
                              f"–û—à–∏–±–æ–∫: {len(section_stats['failed_files'])}")
                
                self.interactive_logger.update(progress_msg)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
                page_result = self._process_single_page(page_url, download_dir, page_num)
                
                section_stats["processed_pages"] += 1
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–∑–¥–µ–ª–∞
                if not page_result["is_empty"]:
                    section_stats["empty_pages_count"] = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø—Ä–∏ –Ω–∞–π–¥–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ
                    section_stats["downloaded_files"].extend(page_result["downloaded_files"])
                    section_stats["skipped_files"].extend(page_result["skipped_files"])
                    section_stats["failed_files"].extend(page_result["failed_files"])
                else:
                    section_stats["empty_pages_count"] += 1
                    
                    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –ø—Ä–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥—Ä—è–¥
                    if section_stats["empty_pages_count"] >= max_empty_pages:
                        self.interactive_logger.finish(f"‚èπÔ∏è  –ù–∞–π–¥–µ–Ω–æ {max_empty_pages} –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ–¥—Ä—è–¥. –û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–∑–¥–µ–ª–∞ {url_index}.")
                        break
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
                time.sleep(self.config.get("page_delay", 2))
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            total_stats["all_downloaded_files"].extend(section_stats["downloaded_files"])
            total_stats["all_skipped_files"].extend(section_stats["skipped_files"])
            total_stats["all_failed_files"].extend(section_stats["failed_files"])
            total_stats["total_pages"] += section_stats["processed_pages"]
            total_stats["total_urls_processed"] += 1
            
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ä–∞–∑–¥–µ–ª—É
            self.interactive_logger.finish(f"‚úÖ –†–∞–∑–¥–µ–ª {url_index} –∑–∞–≤–µ—Ä—à–µ–Ω: "
                                         f"—Å—Ç—Ä–∞–Ω–∏—Ü {section_stats['processed_pages']}, "
                                         f"—Å–∫–∞—á–∞–Ω–æ {len(section_stats['downloaded_files'])}, "
                                         f"–ø—Ä–æ–ø—É—â–µ–Ω–æ {len(section_stats['skipped_files'])}, "
                                         f"–æ—à–∏–±–æ–∫ {len(section_stats['failed_files'])}")
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.interactive_logger.finish()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–∞
        self._save_download_log(total_stats["all_downloaded_files"], download_dir)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        end_time = time.time()
        duration = end_time - start_time
        
        stats = {
            "success": True,
            "total_pages": total_stats["total_pages"],
            "total_links": len(total_stats["all_downloaded_files"]) + len(total_stats["all_skipped_files"]) + len(total_stats["all_failed_files"]),
            "downloaded": len(total_stats["all_downloaded_files"]),
            "skipped": len(total_stats["all_skipped_files"]),
            "failed": len(total_stats["all_failed_files"]),
            "duration_seconds": round(duration, 2),
            "download_dir": str(download_dir),
            "downloaded_files": total_stats["all_downloaded_files"],
            "skipped_files": total_stats["all_skipped_files"],
            "failed_files": total_stats["all_failed_files"],
            "urls_processed": total_stats["total_urls_processed"]
        }
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\n" + "="*60)
        print("üéâ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ü–û–õ–ù–û–ì–û –ü–ê–†–°–ò–ù–ì–ê –ú–ò–ù–°–¢–†–û–Ø")
        print("="*60)
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ä–∞–∑–¥–µ–ª–æ–≤: {stats['urls_processed']}")
        print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {stats['total_pages']}")
        print(f"üîó –í—Å–µ–≥–æ PDF-—Å—Å—ã–ª–æ–∫ –Ω–∞–π–¥–µ–Ω–æ: {stats['total_links']}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {stats['downloaded']}")
        print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç): {stats['skipped']}")
        print(f"‚ùå –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏: {stats['failed']}")
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {stats['duration_seconds']} —Å–µ–∫")
        print(f"üìÅ –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏: {stats['download_dir']}")
        print("="*60)
        
        if total_stats["all_downloaded_files"]:
            print(f"\nüì• –°–∫–∞—á–∞–Ω–æ {len(total_stats['all_downloaded_files'])} —Ñ–∞–π–ª–æ–≤")
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∫—Ä–∞—Ç–∫–æ—Å—Ç–∏
            for file_info in total_stats["all_downloaded_files"][:10]:
                size_mb = file_info['size'] / (1024 * 1024)
                print(f"  üìÑ {file_info['filename']} ({size_mb:.2f} MB) - —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {file_info.get('page_num', '?')}")
            if len(total_stats["all_downloaded_files"]) > 10:
                print(f"  ... –∏ –µ—â–µ {len(total_stats['all_downloaded_files']) - 10} —Ñ–∞–π–ª–æ–≤")
        
        if total_stats["all_failed_files"]:
            print(f"\n‚ùå –û—à–∏–±–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏ ({len(total_stats['all_failed_files'])} —Ñ–∞–π–ª–æ–≤):")
            for file_info in total_stats["all_failed_files"][:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
                print(f"  üîó {file_info['filename']}: {file_info['reason']}")
            if len(total_stats["all_failed_files"]) > 5:
                print(f"  ... –∏ –µ—â–µ {len(total_stats['all_failed_files']) - 5} –æ—à–∏–±–æ–∫")
        
        return stats

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description="–ü–∞—Ä—Å–µ—Ä PDF-—Ñ–∞–π–ª–æ–≤ —Å —Å–∞–π—Ç–∞ –ú–∏–Ω—Å—Ç—Ä–æ—è –†–æ—Å—Å–∏–∏")
    parser.add_argument("--config", "-c", default="minstroy_config.json", 
                       help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
    parser.add_argument("--url", "-u", help="URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é)")
    parser.add_argument("--output", "-o", help="–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤")
    parser.add_argument("--verbose", "-v", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")
    
    args = parser.parse_args()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞
    minstroy_parser = MinstroyParser(args.config)
    
    # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
    if args.url:
        minstroy_parser.config["urls"] = [args.url]
    if args.output:
        minstroy_parser.config["download_dir"] = args.output
    if args.verbose:
        minstroy_parser.config["log_level"] = "DEBUG"
        minstroy_parser._setup_logging()
    
    # –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
    try:
        results = minstroy_parser.parse_and_download()
        if results["success"]:
            print(f"\nüéâ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            return 0
        else:
            print(f"\nüí• –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {results.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            return 1
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return 1
    except Exception as e:
        print(f"\nüí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return 1

if __name__ == "__main__":
    exit(main())
