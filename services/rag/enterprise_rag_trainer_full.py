"""
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Enterprise RAG Trainer —Å batch upload, int8 quantization –∏ Redis cache
"""
import os
import sys
import time
import json
import hashlib
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
import asyncio

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import numpy as np
import pandas as pd
from tqdm import tqdm
import redis
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, QuantizationConfig, ScalarQuantization
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModel

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
from .redis_cache import RedisCache
from .quantizer import Int8Quantizer

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DocumentChunk:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —á–∞–Ω–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None
    quantized_embedding: Optional[np.ndarray] = None

class OptimizedEnterpriseRAGTrainer:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Enterprise RAG Trainer"""
    
    def __init__(self, 
                 qdrant_url: str = "http://localhost:6333",
                 redis_url: str = "redis://localhost:6379",
                 batch_size: int = 64,
                 parallel_workers: int = 4,
                 use_quantization: bool = True,
                 cache_ttl: int = 3600):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ RAG trainer
        
        Args:
            qdrant_url: URL Qdrant —Å–µ—Ä–≤–µ—Ä–∞
            redis_url: URL Redis —Å–µ—Ä–≤–µ—Ä–∞
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            parallel_workers: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤
            use_quantization: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å int8 quantization
            cache_ttl: TTL –¥–ª—è –∫–µ—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        self.qdrant_url = qdrant_url
        self.redis_url = redis_url
        self.batch_size = batch_size
        self.parallel_workers = parallel_workers
        self.use_quantization = use_quantization
        self.cache_ttl = cache_ttl
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.qdrant_client = QdrantClient(url=qdrant_url)
        self.redis_cache = RedisCache(redis_url, cache_ttl)
        self.quantizer = Int8Quantizer() if use_quantization else None
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_model = None
        self.tokenizer = None
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_documents": 0,
            "total_chunks": 0,
            "upload_time": 0,
            "memory_peak": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }
    
    def load_embedding_model(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}")
            self.embedding_model = SentenceTransformer(model_name)
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def create_embeddings_batch(self, texts: List[str]) -> np.ndarray:
        """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        try:
            if not self.embedding_model:
                raise ValueError("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            embeddings = self.embedding_model.encode(texts, convert_to_numpy=True)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º quantization –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            if self.use_quantization and self.quantizer:
                embeddings = self.quantizer.quantize(embeddings)
            
            return embeddings
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
            raise
    
    def create_collection(self, collection_name: str, vector_size: int = 384):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
        try:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–∫—Ç–æ—Ä–æ–≤
            vector_params = VectorParams(
                size=vector_size,
                distance=Distance.COSINE
            )
            
            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è quantization –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            quantization_config = None
            if self.use_quantization:
                quantization_config = QuantizationConfig(
                    scalar=ScalarQuantization(
                        type="int8",
                        quantile=0.99,
                        always_ram=True
                    )
                )
            
            # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
            self.qdrant_client.create_collection(
                collection_name=collection_name,
                vectors_config=vector_params,
                quantization_config=quantization_config
            )
            
            logger.info(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {collection_name} —Å–æ–∑–¥–∞–Ω–∞ —Å quantization={self.use_quantization}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {e}")
            raise
    
    def upload_chunks_batch(self, 
                           collection_name: str, 
                           chunks: List[DocumentChunk],
                           show_progress: bool = True) -> Dict[str, Any]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –±–∞—Ç—á–∞–º–∏"""
        try:
            start_time = time.time()
            total_chunks = len(chunks)
            
            logger.info(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É {total_chunks} —á–∞–Ω–∫–æ–≤ –±–∞—Ç—á–∞–º–∏ –ø–æ {self.batch_size}")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏
            batches = [chunks[i:i + self.batch_size] for i in range(0, total_chunks, self.batch_size)]
            
            # –°–æ–∑–¥–∞–µ–º progress bar
            progress_bar = tqdm(
                total=total_chunks,
                desc="–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant",
                unit="chunks",
                ncols=100
            ) if show_progress else None
            
            uploaded_count = 0
            memory_peak = 0
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞—Ç—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            with ThreadPoolExecutor(max_workers=self.parallel_workers) as executor:
                futures = []
                
                for batch_idx, batch in enumerate(batches):
                    future = executor.submit(self._upload_single_batch, collection_name, batch, batch_idx)
                    futures.append(future)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                for future in as_completed(futures):
                    try:
                        batch_result = future.result()
                        uploaded_count += batch_result['uploaded']
                        memory_peak = max(memory_peak, batch_result['memory_peak'])
                        
                        if progress_bar:
                            progress_bar.update(batch_result['uploaded'])
                            progress_bar.set_postfix({
                                'Speed': f"{batch_result['speed']:.1f} docs/sec",
                                'Memory': f"{memory_peak:.1f} MB"
                            })
                            
                    except Exception as e:
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞: {e}")
            
            if progress_bar:
                progress_bar.close()
            
            upload_time = time.time() - start_time
            speed = total_chunks / upload_time if upload_time > 0 else 0
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats.update({
                "total_chunks": total_chunks,
                "upload_time": upload_time,
                "memory_peak": memory_peak,
                "upload_speed": speed
            })
            
            logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {uploaded_count} —á–∞–Ω–∫–æ–≤ –∑–∞ {upload_time:.2f}—Å ({speed:.1f} docs/sec)")
            
            return {
                "uploaded": uploaded_count,
                "time": upload_time,
                "speed": speed,
                "memory_peak": memory_peak
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–∞–Ω–∫–æ–≤: {e}")
            raise
    
    def _upload_single_batch(self, 
                           collection_name: str, 
                           batch: List[DocumentChunk], 
                           batch_idx: int) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞"""
        try:
            batch_start = time.time()
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –±–∞—Ç—á–∞
            texts = [chunk.content for chunk in batch]
            embeddings = self.create_embeddings_batch(texts)
            
            # –°–æ–∑–¥–∞–µ–º —Ç–æ—á–∫–∏ –¥–ª—è Qdrant
            points = []
            for i, (chunk, embedding) in enumerate(zip(batch, embeddings)):
                point = PointStruct(
                    id=batch_idx * self.batch_size + i,
                    vector=embedding.tolist(),
                    payload={
                        "content": chunk.content,
                        "metadata": chunk.metadata
                    }
                )
                points.append(point)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤ Qdrant
            self.qdrant_client.upsert(
                collection_name=collection_name,
                points=points
            )
            
            batch_time = time.time() - batch_start
            speed = len(batch) / batch_time if batch_time > 0 else 0
            
            # –û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
            memory_usage = self._estimate_memory_usage(embeddings)
            
            return {
                "uploaded": len(batch),
                "speed": speed,
                "memory_peak": memory_usage,
                "batch_time": batch_time
            }
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞ {batch_idx}: {e}")
            raise
    
    def _estimate_memory_usage(self, embeddings: np.ndarray) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –≤ MB"""
        try:
            # –†–∞–∑–º–µ—Ä –º–∞—Å—Å–∏–≤–∞ –≤ –±–∞–π—Ç–∞—Ö
            array_size = embeddings.nbytes
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ MB
            memory_mb = array_size / (1024 * 1024)
            return memory_mb
        except Exception:
            return 0.0
    
    def search_with_cache(self, 
                         collection_name: str, 
                         query: str, 
                         limit: int = 3) -> List[Dict[str, Any]]:
        """–ü–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Redis –∫–µ—à–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∫–µ—à–∞
            cache_key = hashlib.md5(query.encode()).hexdigest()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
            cached_result = self.redis_cache.get(cache_key)
            if cached_result:
                self.stats["cache_hits"] += 1
                logger.info(f"‚úÖ Cache hit –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
                return cached_result
            
            # –ö–µ—à –ø—Ä–æ–º–∞—Ö - –≤—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            self.stats["cache_misses"] += 1
            logger.info(f"üîç Cache miss –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}...")
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.create_embeddings_batch([query])[0]
            
            # –ü–æ–∏—Å–∫ –≤ Qdrant
            search_result = self.qdrant_client.search(
                collection_name=collection_name,
                query_vector=query_embedding.tolist(),
                limit=limit
            )
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            results = []
            for hit in search_result:
                results.append({
                    "content": hit.payload.get("content", ""),
                    "metadata": hit.payload.get("metadata", {}),
                    "score": hit.score
                })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
            self.redis_cache.set(cache_key, results)
            
            return results
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å –∫–µ—à–µ–º: {e}")
            return []
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–µ—à–∞"""
        try:
            total_requests = self.stats["cache_hits"] + self.stats["cache_misses"]
            hit_rate = (self.stats["cache_hits"] / total_requests * 100) if total_requests > 0 else 0
            
            return {
                "cache_hits": self.stats["cache_hits"],
                "cache_misses": self.stats["cache_misses"],
                "hit_rate": hit_rate,
                "total_requests": total_requests
            }
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–µ—à–∞: {e}")
            return {}
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        return {
            "total_documents": self.stats["total_documents"],
            "total_chunks": self.stats["total_chunks"],
            "upload_time": self.stats["upload_time"],
            "upload_speed": self.stats.get("upload_speed", 0),
            "memory_peak": self.stats["memory_peak"],
            "cache_stats": self.get_cache_stats()
        }
    
    def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            if hasattr(self, 'redis_cache'):
                self.redis_cache.close()
            logger.info("‚úÖ –†–µ—Å—É—Ä—Å—ã –æ—á–∏—â–µ–Ω—ã")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
def demonstrate_optimization():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ RAG —Å–∏—Å—Ç–µ–º—ã"""
    print("=== –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–ò RAG ===")
    
    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π trainer
    trainer = OptimizedEnterpriseRAGTrainer(
        batch_size=64,
        parallel_workers=4,
        use_quantization=True
    )
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        trainer.load_embedding_model()
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        trainer.create_collection("optimized_test", vector_size=384)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ —á–∞–Ω–∫–∏
        test_chunks = []
        for i in range(1000):
            chunk = DocumentChunk(
                content=f"–¢–µ—Å—Ç–æ–≤—ã–π —á–∞–Ω–∫ –Ω–æ–º–µ—Ä {i} —Å —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
                metadata={"source": f"test_doc_{i}", "chunk_id": i}
            )
            test_chunks.append(chunk)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏
        result = trainer.upload_chunks_batch("optimized_test", test_chunks)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        print(f"  ‚Ä¢ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {result['uploaded']} —á–∞–Ω–∫–æ–≤")
        print(f"  ‚Ä¢ –í—Ä–µ–º—è: {result['time']:.2f}—Å")
        print(f"  ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å: {result['speed']:.1f} docs/sec")
        print(f"  ‚Ä¢ –ü–∞–º—è—Ç—å: {result['memory_peak']:.1f} MB")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ —Å –∫–µ—à–µ–º
        query = "—Ç–µ—Å—Ç–æ–≤—ã–π —á–∞–Ω–∫"
        results = trainer.search_with_cache("optimized_test", query, limit=3)
        
        print(f"‚úÖ –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = trainer.get_performance_stats()
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"  ‚Ä¢ Cache hit rate: {stats['cache_stats']['hit_rate']:.1f}%")
        print(f"  ‚Ä¢ –û–±—â–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å: {stats['upload_speed']:.1f} docs/sec")
        
    finally:
        trainer.cleanup()


if __name__ == "__main__":
    demonstrate_optimization()
