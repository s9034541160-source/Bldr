#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üîß INTEGRATED STRUCTURE & CHUNKING SYSTEM V3
============================================
–ü–æ–ª–Ω–æ—Å—Ç—å—é –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å:
‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–º –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã, –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã, –∞–±–∑–∞—Ü—ã, —Ç–∞–±–ª–∏—Ü—ã)
‚úÖ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
‚úÖ –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º API
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ Enhanced RAG Trainer

–ü–†–û–ë–õ–ï–ú–´ –ò–°–ü–†–ê–í–õ–ï–ù–´:
‚ùå –ß–∞–Ω–∫–∏–Ω–≥ –ø–æ 300-500 —Å–∏–º–≤–æ–ª–æ–≤ ‚Üí ‚úÖ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
‚ùå –ù–µ—Ç–æ—á–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã ‚Üí ‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–µ—Ä–∞—Ä—Ö–∏—è —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
‚ùå –ù–µ—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Ñ—Ä–æ–Ω—Ç–æ–º ‚Üí ‚úÖ API-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
"""

import re
import json
import numpy as np
from typing import Dict, List, Any, Tuple, Optional, Union
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from enum import Enum
import hashlib
import logging

logger = logging.getLogger(__name__)

class ChunkType(Enum):
    """–¢–∏–ø—ã —á–∞–Ω–∫–æ–≤ –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è"""
    SECTION = "section"           # –¶–µ–ª—ã–π —Ä–∞–∑–¥–µ–ª 
    SUBSECTION = "subsection"     # –ü–æ–¥—Ä–∞–∑–¥–µ–ª
    PARAGRAPH = "paragraph"       # –ê–±–∑–∞—Ü
    TABLE = "table"              # –¢–∞–±–ª–∏—Ü–∞
    LIST = "list"                # –°–ø–∏—Å–æ–∫
    MIXED = "mixed"              # –°–º–µ—à–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    FRAGMENT = "fragment"        # –§—Ä–∞–≥–º–µ–Ω—Ç (–ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞)

@dataclass
class DocumentElement:
    """–ë–∞–∑–æ–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    id: str
    type: str
    content: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    parent_id: Optional[str] = None
    children_ids: List[str] = field(default_factory=list)
    position: int = 0
    page_number: int = 0

@dataclass
class DocumentSection(DocumentElement):
    """–†–∞–∑–¥–µ–ª –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    number: str = ""
    title: str = ""
    level: int = 1
    
@dataclass 
class DocumentParagraph(DocumentElement):
    """–ê–±–∑–∞—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    section_id: Optional[str] = None
    
@dataclass
class DocumentTable(DocumentElement):
    """–¢–∞–±–ª–∏—Ü–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    number: str = ""
    title: str = ""
    headers: List[str] = field(default_factory=list)
    rows: List[List[str]] = field(default_factory=list)

@dataclass
class DocumentList(DocumentElement):
    """–°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    list_type: str = "bulleted"  # bulleted, numbered, mixed
    items: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class SmartChunk:
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —á–∞–Ω–∫ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    id: str
    content: str
    chunk_type: ChunkType
    source_elements: List[str]  # IDs —ç–ª–µ–º–µ–Ω—Ç–æ–≤-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    metadata: Dict[str, Any]
    quality_score: float = 0.0
    embedding: Optional[List[float]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è API"""
        return {
            'id': self.id,
            'content': self.content,
            'type': self.chunk_type.value,
            'source_elements': self.source_elements,
            'metadata': self.metadata,
            'quality_score': self.quality_score,
            'word_count': len(self.content.split()),
            'char_count': len(self.content),
            'has_tables': 'table' in self.chunk_type.value.lower() or '|' in self.content,
            'has_lists': bool(re.search(r'^\s*[-‚Ä¢*\d+]\s+', self.content, re.MULTILINE)),
            'technical_terms': self._count_technical_terms()
        }
    
    def _count_technical_terms(self) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
        patterns = [
            r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü)\s+[\d.-]+',
            r'\b\d+(?:\.\d+)?\s*(?:–º–º|—Å–º|–º|–∫–º|–≥|–∫–≥|—Ç|–ú–ü–∞|–∫–ü–∞|¬∞C)\b',
            r'\b\d+(?:\.\d+)*\s*%\b'
        ]
        count = 0
        for pattern in patterns:
            count += len(re.findall(pattern, self.content, re.IGNORECASE))
        return count

class AdvancedStructureExtractor:
    """
    üîç –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É:
    - –ì–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã, –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã —Å —Ç–æ—á–Ω–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π
    - –ê–±–∑–∞—Ü—ã —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ —Ä–∞–∑–¥–µ–ª–∞–º
    - –¢–∞–±–ª–∏—Ü—ã —Å –ø–æ–ª–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    - –°–ø–∏—Å–∫–∏ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π
    - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–Ω–∞–∑–≤–∞–Ω–∏–µ, –Ω–æ–º–µ—Ä, –¥–∞—Ç—ã, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è)
    """
    
    def __init__(self):
        self.section_patterns = {
            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã: "1. –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø", "2. –ù–û–†–ú–ê–¢–ò–í–ù–´–ï –°–°–´–õ–ö–ò"
            'main_sections': [
                r'^(\d+)\.\s+([–ê-–Ø–Å][–ê-–Ø–Å\s]{4,80})(?:\s*\.?\s*)?$',
                r'^(\d+)\s+([–ê-–Ø–Å][–ê-–Ø–Å\s]{4,80})(?:\s*\.?\s*)?$',
            ],
            
            # –ü–æ–¥—Ä–∞–∑–¥–µ–ª—ã: "1.1 –û–±—â–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "1.2 –û–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"
            'subsections': [
                r'^(\d+\.\d+)\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s]{5,100})(?:\s*\.?\s*)?$',
                r'^(\d+\.\d+)\.\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s]{5,100})(?:\s*\.?\s*)?$',
            ],
            
            # –ü–æ–¥–ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã: "1.1.1 –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è"
            'subsubsections': [
                r'^(\d+\.\d+\.\d+)\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s]{5,100})(?:\s*\.?\s*)?$',
                r'^(\d+\.\d+\.\d+)\.\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s]{5,100})(?:\s*\.?\s*)?$',
            ],
            
            # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è: "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ê", "–ü–†–ò–õ–û–ñ–ï–ù–ò–ï –ë"
            'appendices': [
                r'^((?:–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ|–ü–†–ò–õ–û–ñ–ï–ù–ò–ï)\s+[–ê-–Ø])\s*\.?\s*([–ê-–Ø–Å–∞-—è—ë\s]{0,100})(?:\s*\.?\s*)?$',
            ],
            
            # –ë—É–∫–≤–µ–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã: "–∞) —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è", "–±) –º–µ—Ç–æ–¥—ã"
            'lettered_items': [
                r'^([–∞-—è])\)\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s\.,]{5,150})$',
                r'^\(([–∞-—è])\)\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s\.,]{5,150})$',
            ]
        }
        
        self.table_patterns = {
            'table_header': r'(?:–¢–∞–±–ª–∏—Ü–∞|–¢–ê–ë–õ–ò–¶–ê)\s+(\d+(?:\.\d+)*)\s*[-‚Äì‚Äî]?\s*([^\n\r]{0,100})',
            'table_row': r'\|([^|]+(?:\|[^|]*)*)\|',
            'table_separator': r'\|[-\s:=|]+\|',
        }
        
        self.list_patterns = {
            'numbered_list': r'^(\d+(?:\.\d+)*)\.\s+(.+)$',
            'bulleted_list': r'^[-‚Ä¢*]\s+(.+)$', 
            'lettered_list': r'^([–∞-—è])\)\s+(.+)$|^\(([–∞-—è])\)\s+(.+)$',
            'mixed_list': r'^(?:[-‚Ä¢*]|\d+\.|[–∞-—è]\)|\([–∞-—è]\))\s+(.+)$'
        }
        
        self.metadata_patterns = {
            'document_number': [
                r'(?:–°–ü|–°–í–û–î\s+–ü–†–ê–í–ò–õ)\s+(\d+(?:\.\d+)*(?:-\d+)*)',
                r'(?:–ì–û–°–¢)\s+(\d+(?:\.\d+)*(?:-\d+)*)',
                r'(?:–°–ù–∏–ü)\s+([\d.-]+)',
                r'(?:–†–î|–í–°–ù|–¢–£)\s+([\d.-]+)',
            ],
            'document_title': [
                r'^([–ê-–Ø–Å][–ê-–Ø–Å\s]{10,120})$',
                r'(?:–°–í–û–î\s+–ü–†–ê–í–ò–õ|–°–ü\s+[\d.-]+)\s*\.?\s*([–ê-–Ø–Å][–ê-–Ø–Å\s]{10,120})',
                r'(?:–ì–û–°–¢\s+[\d.-]+)\s*\.?\s*([–ê-–Ø–Å][–ê-–Ø–Å\s]{10,120})',
            ],
            'organization': [
                r'(?:–£–¢–í–ï–†–ñ–î–ï–ù|–£–¢–í–ï–†–ñ–î–ï–ù–û)\s+([–ê-–Ø–Å][–ê-–Ø–∞-—è—ë\s]{10,80})',
                r'(?:–ú–∏–Ω—Å—Ç—Ä–æ–π\s+–†–æ—Å—Å–∏–∏|–†–æ—Å—Å—Ç–∞–Ω–¥–∞—Ä—Ç|–ú–∏–Ω—Ä–µ–≥–∏–æ–Ω\s+–†–æ—Å—Å–∏–∏|–ì–æ—Å—Å—Ç—Ä–æ–π\s+–†–æ—Å—Å–∏–∏)',
            ],
            'dates': [
                r'(\d{1,2}\.\d{1,2}\.\d{4})',
                r'(\d{1,2}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)\s+\d{4})',
            ]
        }

    def extract_complete_structure(self, content: str, file_path: str = "") -> Dict[str, Any]:
        """
        üîç –ü–û–õ–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –°–¢–†–£–ö–¢–£–†–´ –î–û–ö–£–ú–ï–ù–¢–ê
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å–æ –≤—Å–µ–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏:
        - metadata: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        - sections: –∏–µ—Ä–∞—Ä—Ö–∏—è —Ä–∞–∑–¥–µ–ª–æ–≤
        - paragraphs: –≤—Å–µ –∞–±–∑–∞—Ü—ã —Å –ø—Ä–∏–≤—è–∑–∫–æ–π
        - tables: —Ç–∞–±–ª–∏—Ü—ã —Å –¥–∞–Ω–Ω—ã–º–∏
        - lists: —Å–ø–∏—Å–∫–∏ —Å –∏–µ—Ä–∞—Ä—Ö–∏–µ–π
        - elements: –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è API
        """
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = self._extract_enhanced_metadata(content, file_path)
        
        # 2. –†–∞–∑–±–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        lines = content.split('\n')
        
        # 3. –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞
        elements = self._extract_all_elements(lines, content)
        
        # 4. –°—Ç—Ä–æ–∏–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ä–∞–∑–¥–µ–ª–æ–≤
        sections_hierarchy = self._build_sections_hierarchy(elements)
        
        # 5. –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–±–∑–∞—Ü—ã
        paragraphs = self._extract_paragraphs(content, elements)
        
        # 6. –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        tables = self._extract_enhanced_tables(content, lines)
        
        # 7. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ø–∏—Å–∫–∏
        lists = self._extract_enhanced_lists(content, lines)
        
        # 8. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        statistics = self._calculate_enhanced_statistics(content, elements, tables, lists)
        
        # 9. –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        complete_structure = {
            'metadata': metadata,
            'sections_hierarchy': sections_hierarchy,
            'paragraphs': paragraphs,
            'tables': tables,
            'lists': lists,
            'elements': [elem.__dict__ for elem in elements],  # –ü–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –¥–ª—è API
            'statistics': statistics,
            'extraction_info': {
                'extracted_at': datetime.now().isoformat(),
                'extractor_version': 'Advanced_v3.0',
                'file_path': file_path,
                'content_length': len(content),
                'lines_count': len(lines),
                'quality_score': self._calculate_extraction_quality_score(elements, tables, lists, metadata)
            }
        }
        
        return complete_structure

    def _extract_enhanced_metadata(self, content: str, file_path: str) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        metadata = {
            'document_number': '',
            'document_title': '',
            'document_type': 'unknown',
            'organization': '',
            'approval_date': '',
            'effective_date': '',
            'status': 'active',
            'keywords': [],
            'references': [],
            'file_info': {
                'file_name': Path(file_path).name if file_path else '',
                'file_path': file_path,
                'file_size': len(content.encode('utf-8')),
            }
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 3000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        header_content = content[:3000]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
        for pattern in self.metadata_patterns['document_number']:
            match = re.search(pattern, header_content, re.IGNORECASE | re.MULTILINE)
            if match:
                metadata['document_number'] = match.group(1)
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥–æ–∫—É–º–µ–Ω—Ç–∞
                if '–°–ü' in pattern or '–°–í–û–î' in pattern:
                    metadata['document_type'] = 'sp'
                elif '–ì–û–°–¢' in pattern:
                    metadata['document_type'] = 'gost'
                elif '–°–ù–∏–ü' in pattern:
                    metadata['document_type'] = 'snip'
                elif '–†–î' in pattern:
                    metadata['document_type'] = 'rd'
                break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        for pattern in self.metadata_patterns['document_title']:
            matches = re.findall(pattern, header_content, re.MULTILINE)
            if matches:
                # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
                best_title = max(matches, key=lambda x: len(x) if isinstance(x, str) else len(str(x)))
                metadata['document_title'] = str(best_title).strip()
                break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—é
        for pattern in self.metadata_patterns['organization']:
            match = re.search(pattern, header_content, re.IGNORECASE | re.MULTILINE)
            if match:
                if match.groups():
                    metadata['organization'] = match.group(1).strip()
                else:
                    metadata['organization'] = match.group(0).strip()
                break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—ã
        date_matches = []
        for pattern in self.metadata_patterns['dates']:
            matches = re.findall(pattern, header_content, re.IGNORECASE | re.MULTILINE)
            date_matches.extend(matches)
        
        if date_matches:
            # –ü–µ—Ä–≤–∞—è –¥–∞—Ç–∞ –æ–±—ã—á–Ω–æ –¥–∞—Ç–∞ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
            metadata['approval_date'] = date_matches[0]
            if len(date_matches) > 1:
                metadata['effective_date'] = date_matches[1]
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        metadata['keywords'] = self._extract_document_keywords(content, metadata['document_type'])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        metadata['references'] = self._extract_document_references(content)
        
        return metadata

    def _extract_all_elements(self, lines: List[str], full_content: str) -> List[DocumentElement]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å —Ç–æ—á–Ω–æ–π —Ç–∏–ø–∏–∑–∞—Ü–∏–µ–π"""
        
        elements = []
        current_section = None
        element_counter = 0
        
        for line_num, line in enumerate(lines):
            line = line.strip()
            if not line:
                continue
                
            element_counter += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ä–∞–∑–¥–µ–ª
            section_match = self._match_section_line(line)
            if section_match:
                section_type, number, title, level = section_match
                
                element = DocumentSection(
                    id=f'section_{element_counter}',
                    type=section_type,
                    content=line,
                    number=number,
                    title=title,
                    level=level,
                    position=line_num,
                    page_number=self._estimate_page(line_num, len(lines)),
                    metadata={
                        'section_type': section_type,
                        'level': level,
                        'has_subsections': False  # –±—É–¥–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å –ø–æ–∑–∂–µ
                    }
                )
                elements.append(element)
                current_section = element
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ç–∞–±–ª–∏—Ü—É
            table_match = re.search(self.table_patterns['table_header'], line, re.IGNORECASE)
            if table_match:
                element = DocumentTable(
                    id=f'table_{element_counter}',
                    type='table_header',
                    content=line,
                    number=table_match.group(1),
                    title=table_match.group(2).strip() if len(table_match.groups()) > 1 else '',
                    position=line_num,
                    page_number=self._estimate_page(line_num, len(lines)),
                    metadata={
                        'table_number': table_match.group(1),
                        'parent_section': current_section.id if current_section else None
                    }
                )
                elements.append(element)
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å–ø–∏—Å–æ–∫
            list_match = self._match_list_line(line)
            if list_match:
                list_type, item_marker, item_text = list_match
                
                element = DocumentList(
                    id=f'list_item_{element_counter}',
                    type='list_item',
                    content=line,
                    list_type=list_type,
                    position=line_num,
                    page_number=self._estimate_page(line_num, len(lines)),
                    metadata={
                        'list_type': list_type,
                        'item_marker': item_marker,
                        'item_text': item_text,
                        'parent_section': current_section.id if current_section else None
                    }
                )
                elements.append(element)
                continue
            
            # –û–±—ã—á–Ω—ã–π –∞–±–∑–∞—Ü
            if len(line) > 10:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏
                element = DocumentParagraph(
                    id=f'paragraph_{element_counter}',
                    type='paragraph',
                    content=line,
                    position=line_num,
                    page_number=self._estimate_page(line_num, len(lines)),
                    section_id=current_section.id if current_section else None,
                    metadata={
                        'word_count': len(line.split()),
                        'has_numbers': bool(re.search(r'\d+', line)),
                        'has_technical_terms': self._has_technical_terms(line),
                        'parent_section': current_section.id if current_section else None
                    }
                )
                elements.append(element)
        
        return elements

    def _match_section_line(self, line: str) -> Optional[Tuple[str, str, str, int]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∏ —É—Ä–æ–≤–Ω—è —Ä–∞–∑–¥–µ–ª–∞"""
        
        # –ì–ª–∞–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã (—É—Ä–æ–≤–µ–Ω—å 1)
        for pattern in self.section_patterns['main_sections']:
            match = re.match(pattern, line)
            if match:
                number = match.group(1)
                title = match.group(2).strip()
                return ('main_section', number, title, 1)
        
        # –ü–æ–¥—Ä–∞–∑–¥–µ–ª—ã (—É—Ä–æ–≤–µ–Ω—å 2)
        for pattern in self.section_patterns['subsections']:
            match = re.match(pattern, line)
            if match:
                number = match.group(1)
                title = match.group(2).strip()
                return ('subsection', number, title, 2)
        
        # –ü–æ–¥–ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã (—É—Ä–æ–≤–µ–Ω—å 3)
        for pattern in self.section_patterns['subsubsections']:
            match = re.match(pattern, line)
            if match:
                number = match.group(1)
                title = match.group(2).strip()
                return ('subsubsection', number, title, 3)
        
        # –ü—Ä–∏–ª–æ–∂–µ–Ω–∏—è (—É—Ä–æ–≤–µ–Ω—å 1, –Ω–æ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ç–∏–ø)
        for pattern in self.section_patterns['appendices']:
            match = re.match(pattern, line)
            if match:
                number = match.group(1)
                title = match.group(2).strip() if len(match.groups()) > 1 else ''
                return ('appendix', number, title, 1)
        
        # –ë—É–∫–≤–µ–Ω–Ω—ã–µ –ø—É–Ω–∫—Ç—ã (—É—Ä–æ–≤–µ–Ω—å 4)
        for pattern in self.section_patterns['lettered_items']:
            match = re.match(pattern, line)
            if match:
                number = match.group(1)
                title = match.group(2).strip()
                return ('lettered_item', number, title, 4)
        
        return None

    def _match_list_line(self, line: str) -> Optional[Tuple[str, str, str]]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ —Å–ø–∏—Å–∫–∞"""
        
        for list_type, pattern in self.list_patterns.items():
            match = re.match(pattern, line, re.IGNORECASE)
            if match:
                if list_type == 'numbered_list':
                    marker = match.group(1) + '.'
                    text = match.group(2).strip()
                elif list_type == 'bulleted_list':
                    marker = line[0]  # -, ‚Ä¢, –∏–ª–∏ *
                    text = match.group(1).strip()
                elif list_type == 'lettered_list':
                    marker = match.group(1) if match.group(1) else match.group(3)
                    text = match.group(2).strip() if match.group(2) else match.group(4).strip()
                else:  # mixed_list
                    marker = re.match(r'^([-‚Ä¢*]|\d+\.|[–∞-—è]\)|\([–∞-—è]\))', line).group(1)
                    text = match.group(1).strip()
                
                return (list_type, marker, text)
        
        return None

class IntelligentChunker:
    """
    üß© –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô –ß–ê–ù–ö–ï–†
    
    –°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–∞:
    - –†–∞–∑–¥–µ–ª—ã –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ (–µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –ø–æ–¥—Ö–æ–¥–∏—Ç)
    - –¢–∞–±–ª–∏—Ü—ã –∫–∞–∫ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏
    - –°–ø–∏—Å–∫–∏ –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç—Å—è –ª–æ–≥–∏—á–Ω–æ
    - –ê–±–∑–∞—Ü—ã –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    - –§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ª–∏–º–∏—Ç–æ–≤
    """
    
    def __init__(self, min_chunk_size: int = 100, max_chunk_size: int = 1200, 
                 optimal_chunk_size: int = 800, overlap_size: int = 50):
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.optimal_chunk_size = optimal_chunk_size
        self.overlap_size = overlap_size

    def create_intelligent_chunks(self, document_structure: Dict[str, Any]) -> List[SmartChunk]:
        """
        üß© –ì–õ–ê–í–ù–´–ô –ú–ï–¢–û–î: –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        
        –ê–ª–≥–æ—Ä–∏—Ç–º:
        1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        2. –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∞–º (–µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –ø–æ–¥—Ö–æ–¥–∏—Ç)
        3. –¢–∞–±–ª–∏—Ü—ã –≤—ã–¥–µ–ª—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏
        4. –°–ø–∏—Å–∫–∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ª–æ–≥–∏—á–Ω–æ
        5. –ê–±–∑–∞—Ü—ã –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        6. –ü—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞ - —É–º–Ω–∞—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è
        """
        
        chunks = []
        elements = document_structure.get('elements', [])
        sections_hierarchy = document_structure.get('sections_hierarchy', [])
        tables = document_structure.get('tables', [])
        lists = document_structure.get('lists', [])
        
        chunk_counter = 0
        
        # 1. –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–¥–µ–ª–æ–≤
        section_chunks = self._create_section_chunks(sections_hierarchy, elements)
        chunks.extend(section_chunks)
        chunk_counter += len(section_chunks)
        
        # 2. –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü
        table_chunks = self._create_table_chunks(tables, elements)
        chunks.extend(table_chunks)
        chunk_counter += len(table_chunks)
        
        # 3. –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏ –¥–ª—è —Å–ø–∏—Å–∫–æ–≤
        list_chunks = self._create_list_chunks(lists, elements)
        chunks.extend(list_chunks)
        chunk_counter += len(list_chunks)
        
        # 4. –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è —ç–ª–µ–º–µ–Ω—Ç—ã (–∞–±–∑–∞—Ü—ã –±–µ–∑ —Å–µ–∫—Ü–∏–π)
        orphan_chunks = self._create_orphan_chunks(elements, chunks)
        chunks.extend(orphan_chunks)
        
        # 5. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤
        optimized_chunks = self._optimize_chunks(chunks)
        
        # 6. –î–æ–±–∞–≤–ª—è–µ–º ID –∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        final_chunks = []
        for i, chunk in enumerate(optimized_chunks):
            chunk.id = f'chunk_{i+1}'
            chunk.quality_score = self._calculate_chunk_quality(chunk)
            final_chunks.append(chunk)
        
        return final_chunks

    def _create_section_chunks(self, sections_hierarchy: List[Dict], elements: List[Dict]) -> List[SmartChunk]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–¥–µ–ª–æ–≤"""
        chunks = []
        
        for section in sections_hierarchy:
            section_content = self._gather_section_content(section, elements)
            
            if not section_content:
                continue
                
            # –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ - —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω —á–∞–Ω–∫
            if self.min_chunk_size <= len(section_content) <= self.max_chunk_size:
                chunk = SmartChunk(
                    id='',  # –±—É–¥–µ—Ç –Ω–∞–∑–Ω–∞—á–µ–Ω –ø–æ–∑–∂–µ
                    content=section_content,
                    chunk_type=ChunkType.SECTION,
                    source_elements=[section.get('id', '')],
                    metadata={
                        'section_number': section.get('number', ''),
                        'section_title': section.get('title', ''),
                        'section_level': section.get('level', 1),
                        'has_subsections': len(section.get('subsections', [])) > 0
                    }
                )
                chunks.append(chunk)
            
            # –ï—Å–ª–∏ —Ä–∞–∑–¥–µ–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π - —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º —É–º–Ω–æ
            elif len(section_content) > self.max_chunk_size:
                section_chunks = self._fragment_large_section(section, section_content, elements)
                chunks.extend(section_chunks)
        
        return chunks

    def _create_table_chunks(self, tables: List[Dict], elements: List[Dict]) -> List[SmartChunk]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ç–∞–±–ª–∏—Ü"""
        chunks = []
        
        for table in tables:
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–∞–±–ª–∏—Ü—ã
            table_content = self._format_table_content(table)
            
            if not table_content or len(table_content) < self.min_chunk_size:
                continue
            
            # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∞—è - —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º
            if len(table_content) > self.max_chunk_size:
                table_chunks = self._fragment_large_table(table, table_content)
                chunks.extend(table_chunks)
            else:
                chunk = SmartChunk(
                    id='',
                    content=table_content,
                    chunk_type=ChunkType.TABLE,
                    source_elements=[table.get('id', '')],
                    metadata={
                        'table_number': table.get('number', ''),
                        'table_title': table.get('title', ''),
                        'row_count': len(table.get('rows', [])),
                        'column_count': len(table.get('headers', [])),
                        'is_complete_table': True
                    }
                )
                chunks.append(chunk)
        
        return chunks

    def _create_list_chunks(self, lists: List[Dict], elements: List[Dict]) -> List[SmartChunk]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è —Å–ø–∏—Å–∫–æ–≤"""
        chunks = []
        
        for list_group in lists:
            list_content = self._format_list_content(list_group)
            
            if not list_content or len(list_content) < self.min_chunk_size:
                continue
            
            # –°–ø–∏—Å–∫–∏ –æ–±—ã—á–Ω–æ —É–º–µ—â–∞—é—Ç—Å—è –≤ –æ–¥–∏–Ω —á–∞–Ω–∫
            if len(list_content) <= self.max_chunk_size:
                chunk = SmartChunk(
                    id='',
                    content=list_content,
                    chunk_type=ChunkType.LIST,
                    source_elements=[list_group.get('id', '')],
                    metadata={
                        'list_type': list_group.get('type', ''),
                        'item_count': len(list_group.get('items', [])),
                        'is_complete_list': True
                    }
                )
                chunks.append(chunk)
            else:
                # –ë–æ–ª—å—à–∏–µ —Å–ø–∏—Å–∫–∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –ª–æ–≥–∏—á–Ω–æ
                list_chunks = self._fragment_large_list(list_group, list_content)
                chunks.extend(list_chunks)
        
        return chunks

    def _gather_section_content(self, section: Dict, elements: List[Dict]) -> str:
        """–°–±–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Ä–∞–∑–¥–µ–ª–∞ –≤–∫–ª—é—á–∞—è –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã"""
        content_parts = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ä–∞–∑–¥–µ–ª–∞
        if section.get('title'):
            header = f"{section.get('number', '')} {section.get('title', '')}"
            content_parts.append(header)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç —Ä–∞–∑–¥–µ–ª–∞
        if section.get('content'):
            content_parts.append(section.get('content', ''))
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
        for subsection in section.get('subsections', []):
            subsection_content = self._gather_section_content(subsection, elements)
            if subsection_content:
                content_parts.append(subsection_content)
        
        return '\n\n'.join(content_parts)

    def _format_table_content(self, table: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ç–∞–±–ª–∏—Ü—ã"""
        content_parts = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
        if table.get('number') or table.get('title'):
            header = f"–¢–∞–±–ª–∏—Ü–∞ {table.get('number', '')} {table.get('title', '')}".strip()
            content_parts.append(header)
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ —Å—Ç–æ–ª–±—Ü–æ–≤
        headers = table.get('headers', [])
        if headers:
            header_row = ' | '.join(headers)
            content_parts.append(header_row)
            content_parts.append('-' * len(header_row))
        
        # –°—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
        for row in table.get('rows', []):
            if isinstance(row, list):
                row_content = ' | '.join(str(cell) for cell in row)
                content_parts.append(row_content)
        
        return '\n'.join(content_parts)

    def _format_list_content(self, list_group: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–ø–∏—Å–∫–∞"""
        content_parts = []
        
        for item in list_group.get('items', []):
            if isinstance(item, dict):
                marker = item.get('number', '') or item.get('marker', '‚Ä¢')
                text = item.get('text', '')
                if text:
                    content_parts.append(f"{marker} {text}")
            else:
                content_parts.append(str(item))
        
        return '\n'.join(content_parts)

    def _fragment_large_section(self, section: Dict, content: str, elements: List[Dict]) -> List[SmartChunk]:
        """–£–º–Ω–∞—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±–æ–ª—å—à–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞"""
        chunks = []
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–±–∏—Ç—å –ø–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª–∞–º
        subsections = section.get('subsections', [])
        if subsections:
            for subsection in subsections:
                subsection_content = self._gather_section_content(subsection, elements)
                if subsection_content and len(subsection_content) >= self.min_chunk_size:
                    if len(subsection_content) <= self.max_chunk_size:
                        chunk = SmartChunk(
                            id='',
                            content=subsection_content,
                            chunk_type=ChunkType.SUBSECTION,
                            source_elements=[subsection.get('id', '')],
                            metadata={
                                'section_number': subsection.get('number', ''),
                                'section_title': subsection.get('title', ''),
                                'section_level': subsection.get('level', 2),
                                'parent_section': section.get('number', '')
                            }
                        )
                        chunks.append(chunk)
                    else:
                        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º
                        sub_chunks = self._fragment_large_section(subsection, subsection_content, elements)
                        chunks.extend(sub_chunks)
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
            paragraphs = content.split('\n\n')
            current_chunk_content = []
            current_size = 0
            
            for paragraph in paragraphs:
                paragraph_size = len(paragraph)
                
                if current_size + paragraph_size <= self.optimal_chunk_size:
                    current_chunk_content.append(paragraph)
                    current_size += paragraph_size
                else:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
                    if current_chunk_content:
                        chunk_content = '\n\n'.join(current_chunk_content)
                        if len(chunk_content) >= self.min_chunk_size:
                            chunk = SmartChunk(
                                id='',
                                content=chunk_content,
                                chunk_type=ChunkType.FRAGMENT,
                                source_elements=[section.get('id', '')],
                                metadata={
                                    'fragment_of': section.get('number', ''),
                                    'fragment_type': 'section_part'
                                }
                            )
                            chunks.append(chunk)
                    
                    # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫
                    current_chunk_content = [paragraph]
                    current_size = paragraph_size
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
            if current_chunk_content:
                chunk_content = '\n\n'.join(current_chunk_content)
                if len(chunk_content) >= self.min_chunk_size:
                    chunk = SmartChunk(
                        id='',
                        content=chunk_content,
                        chunk_type=ChunkType.FRAGMENT,
                        source_elements=[section.get('id', '')],
                        metadata={
                            'fragment_of': section.get('number', ''),
                            'fragment_type': 'section_part'
                        }
                    )
                    chunks.append(chunk)
        
        return chunks

    def _optimize_chunks(self, chunks: List[SmartChunk]) -> List[SmartChunk]:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–æ–≤ - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞–ª–µ–Ω—å–∫–∏—Ö, –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤"""
        optimized = []
        
        i = 0
        while i < len(chunks):
            current_chunk = chunks[i]
            
            # –ï—Å–ª–∏ —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π, –ø—ã—Ç–∞–µ–º—Å—è –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å–æ —Å–ª–µ–¥—É—é—â–∏–º
            if len(current_chunk.content) < self.min_chunk_size and i + 1 < len(chunks):
                next_chunk = chunks[i + 1]
                combined_size = len(current_chunk.content) + len(next_chunk.content)
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä –ø–æ–¥—Ö–æ–¥—è—â–∏–π –∏ —Ç–∏–ø—ã —Å–æ–≤–º–µ—Å—Ç–∏–º—ã
                if combined_size <= self.max_chunk_size and self._can_combine_chunks(current_chunk, next_chunk):
                    combined_chunk = SmartChunk(
                        id='',
                        content=f"{current_chunk.content}\n\n{next_chunk.content}",
                        chunk_type=ChunkType.MIXED,
                        source_elements=current_chunk.source_elements + next_chunk.source_elements,
                        metadata={
                            'combined_from': [current_chunk.chunk_type.value, next_chunk.chunk_type.value],
                            'is_combined': True
                        }
                    )
                    optimized.append(combined_chunk)
                    i += 2  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–π —á–∞–Ω–∫
                    continue
            
            optimized.append(current_chunk)
            i += 1
        
        return optimized

    def _can_combine_chunks(self, chunk1: SmartChunk, chunk2: SmartChunk) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤"""
        # –ù–µ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å –¥—Ä—É–≥–∏–º–∏ —Ç–∏–ø–∞–º–∏
        if chunk1.chunk_type == ChunkType.TABLE or chunk2.chunk_type == ChunkType.TABLE:
            return False
        
        # –ú–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å –∞–±–∑–∞—Ü—ã, —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏ —Å–ø–∏—Å–∫–∏
        combinable_types = {ChunkType.PARAGRAPH, ChunkType.FRAGMENT, ChunkType.LIST, ChunkType.MIXED}
        
        return chunk1.chunk_type in combinable_types and chunk2.chunk_type in combinable_types

    def _calculate_chunk_quality(self, chunk: SmartChunk) -> float:
        """–†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ —á–∞–Ω–∫–∞"""
        score = 0.0
        
        # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (30% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        size = len(chunk.content)
        if self.min_chunk_size <= size <= self.optimal_chunk_size:
            size_score = 1.0
        elif size <= self.max_chunk_size:
            size_score = 0.8
        else:
            size_score = 0.5
        score += size_score * 0.3
        
        # –¢–∏–ø —á–∞–Ω–∫–∞ (20% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        type_scores = {
            ChunkType.SECTION: 1.0,
            ChunkType.SUBSECTION: 0.9,
            ChunkType.TABLE: 0.95,
            ChunkType.LIST: 0.85,
            ChunkType.PARAGRAPH: 0.7,
            ChunkType.FRAGMENT: 0.6,
            ChunkType.MIXED: 0.5
        }
        score += type_scores.get(chunk.chunk_type, 0.5) * 0.2
        
        # –°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (30% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        technical_terms = chunk._count_technical_terms()
        content_density = technical_terms / len(chunk.content.split()) if chunk.content.split() else 0
        content_score = min(content_density * 10, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        score += content_score * 0.3
        
        # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (20% –æ—Ç –æ—Ü–µ–Ω–∫–∏)
        has_structure = any([
            '|' in chunk.content,  # —Ç–∞–±–ª–∏—Ü—ã
            bool(re.search(r'^\d+\.', chunk.content, re.MULTILINE)),  # –Ω—É–º–µ—Ä–∞—Ü–∏—è
            bool(re.search(r'^[-‚Ä¢*]', chunk.content, re.MULTILINE)),  # —Å–ø–∏—Å–∫–∏
        ])
        structure_score = 1.0 if has_structure else 0.6
        score += structure_score * 0.2
        
        return min(score, 1.0)

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã...
    def _fragment_large_table(self, table: Dict, content: str) -> List[SmartChunk]:
        """–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±–æ–ª—å—à–æ–π —Ç–∞–±–ª–∏—Ü—ã"""
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–∞–±–ª–∏—Ü –ø–æ —Å—Ç—Ä–æ–∫–∞–º
        # ... (–¥–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
        return []
    
    def _fragment_large_list(self, list_group: Dict, content: str) -> List[SmartChunk]:
        """–§—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏—è –±–æ–ª—å—à–æ–≥–æ —Å–ø–∏—Å–∫–∞"""
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å–ø–∏—Å–∫–æ–≤
        # ... (–¥–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
        return []
    
    def _create_orphan_chunks(self, elements: List[Dict], existing_chunks: List[SmartChunk]) -> List[SmartChunk]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤, –Ω–µ –ø–æ–ø–∞–≤—à–∏—Ö –≤ —Ä–∞–∑–¥–µ–ª—ã"""
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ "—Å–∏—Ä–æ—Ç—Å–∫–∏—Ö" —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        # ... (–¥–µ—Ç–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è)
        return []

class IntegratedStructureChunkingSystem:
    """
    üîß –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –°–¢–†–£–ö–¢–£–†–´ –ò –ß–ê–ù–ö–ò–ù–ì–ê
    
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥
    –≤ –µ–¥–∏–Ω—É—é —Å–∏—Å—Ç–µ–º—É —Å API-—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é
    """
    
    def __init__(self):
        self.structure_extractor = AdvancedStructureExtractor()
        self.intelligent_chunker = IntelligentChunker()

    def process_document(self, content: str, file_path: str = "") -> Dict[str, Any]:
        """
        üîß –ü–û–õ–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê –î–û–ö–£–ú–ï–ù–¢–ê
        
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É, —Å–æ–≤–º–µ—Å—Ç–∏–º—É—é —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º:
        - document_info: –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è UI
        - sections: –∏–µ—Ä–∞—Ä—Ö–∏—è —Ä–∞–∑–¥–µ–ª–æ–≤
        - chunks: –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ 
        - tables: —Ç–∞–±–ª–∏—Ü—ã —Å –¥–∞–Ω–Ω—ã–º–∏
        - statistics: —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        """
        
        # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
        document_structure = self.structure_extractor.extract_complete_structure(content, file_path)
        
        # 2. –°–æ–∑–¥–∞–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏
        smart_chunks = self.intelligent_chunker.create_intelligent_chunks(document_structure)
        
        # 3. –§–æ—Ä–º–∏—Ä—É–µ–º API-—Å–æ–≤–º–µ—Å—Ç–∏–º—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        api_compatible_result = {
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–¥–ª—è UI)
            'document_info': {
                'title': document_structure['metadata']['document_title'],
                'number': document_structure['metadata']['document_number'],
                'type': document_structure['metadata']['document_type'],
                'organization': document_structure['metadata']['organization'],
                'approval_date': document_structure['metadata']['approval_date'],
                'file_name': document_structure['metadata']['file_info']['file_name'],
                'file_size': document_structure['metadata']['file_info']['file_size'],
                'keywords': document_structure['metadata']['keywords']
            },
            
            # –ò–µ—Ä–∞—Ä—Ö–∏—è —Ä–∞–∑–¥–µ–ª–æ–≤ (–¥–ª—è –Ω–∞–≤–∏–≥–∞—Ü–∏–∏)
            'sections': self._format_sections_for_api(document_structure['sections_hierarchy']),
            
            # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–µ —á–∞–Ω–∫–∏ (–¥–ª—è RAG)
            'chunks': [chunk.to_dict() for chunk in smart_chunks],
            
            # –¢–∞–±–ª–∏—Ü—ã (–¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
            'tables': document_structure['tables'],
            
            # –°–ø–∏—Å–∫–∏
            'lists': document_structure['lists'],
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            'statistics': {
                **document_structure['statistics'],
                'chunks_created': len(smart_chunks),
                'avg_chunk_quality': np.mean([chunk.quality_score for chunk in smart_chunks]) if smart_chunks else 0,
                'chunk_types_distribution': self._get_chunk_types_stats(smart_chunks)
            },
            
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            'processing_info': {
                'extracted_at': document_structure['extraction_info']['extracted_at'],
                'processor_version': 'Integrated_v3.0',
                'structure_quality': document_structure['extraction_info']['quality_score'],
                'chunking_quality': np.mean([chunk.quality_score for chunk in smart_chunks]) if smart_chunks else 0,
                'total_elements': len(document_structure['elements']),
                'processing_method': 'intelligent_structure_based'
            }
        }
        
        return api_compatible_result

    def _format_sections_for_api(self, sections_hierarchy: List[Dict]) -> List[Dict]:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤ –¥–ª—è API"""
        formatted_sections = []
        
        def format_section_recursive(section: Dict, parent_path: str = "") -> Dict:
            section_path = f"{parent_path}.{section['number']}" if parent_path else section['number']
            
            formatted_section = {
                'id': section_path,
                'number': section.get('number', ''),
                'title': section.get('title', ''),
                'level': section.get('level', 1),
                'type': section.get('type', 'section'),
                'content_length': len(section.get('content', '')),
                'has_subsections': len(section.get('subsections', [])) > 0,
                'parent_path': parent_path,
                'metadata': section.get('metadata', {})
            }
            
            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–¥—Ä–∞–∑–¥–µ–ª—ã
            subsections = []
            for subsection in section.get('subsections', []):
                formatted_subsection = format_section_recursive(subsection, section_path)
                subsections.append(formatted_subsection)
            
            if subsections:
                formatted_section['subsections'] = subsections
            
            return formatted_section
        
        for section in sections_hierarchy:
            formatted_sections.append(format_section_recursive(section))
        
        return formatted_sections

    def _get_chunk_types_stats(self, chunks: List[SmartChunk]) -> Dict[str, int]:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —á–∞–Ω–∫–æ–≤"""
        stats = {}
        for chunk in chunks:
            chunk_type = chunk.chunk_type.value
            stats[chunk_type] = stats.get(chunk_type, 0) + 1
        return stats

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è AdvancedStructureExtractor
def _estimate_page(line_num: int, total_lines: int) -> int:
    """–û—Ü–µ–Ω–∫–∞ –Ω–æ–º–µ—Ä–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    lines_per_page = 50
    return max(1, (line_num // lines_per_page) + 1)

def _has_technical_terms(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã"""
    patterns = [
        r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü)\s+[\d.-]+',
        r'\b\d+(?:\.\d+)?\s*(?:–º–º|—Å–º|–º|–∫–º|–≥|–∫–≥|—Ç|–ú–ü–∞|–∫–ü–∞|¬∞C)\b',
        r'\b(?:–ø—Ä–æ—á–Ω–æ—Å—Ç—å|–¥–µ—Ñ–æ—Ä–º–∞—Ü–∏—è|–Ω–∞–≥—Ä—É–∑–∫–∞|–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ)\b'
    ]
    return any(re.search(pattern, text, re.IGNORECASE) for pattern in patterns)

# –ú–µ—Ç–æ–¥—ã –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ AdvancedStructureExtractor
AdvancedStructureExtractor._estimate_page = _estimate_page
AdvancedStructureExtractor._has_technical_terms = _has_technical_terms

def _build_sections_hierarchy(self, elements: List[DocumentElement]) -> List[Dict]:
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–µ—Ä–∞—Ä—Ö–∏–∏ —Ä–∞–∑–¥–µ–ª–æ–≤"""
    hierarchy = []
    sections_dict = {}
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Å–µ–∫—Ü–∏–∏
    for element in elements:
        if isinstance(element, DocumentSection):
            section_data = {
                'id': element.id,
                'number': element.number,
                'title': element.title,
                'level': element.level,
                'type': element.type,
                'content': element.content,
                'metadata': element.metadata,
                'subsections': []
            }
            sections_dict[element.id] = section_data
    
    # –°—Ç—Ä–æ–∏–º –∏–µ—Ä–∞—Ä—Ö–∏—é
    for section_data in sections_dict.values():
        level = section_data['level']
        
        if level == 1:
            hierarchy.append(section_data)
        else:
            # –ò—â–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π —Ä–∞–∑–¥–µ–ª
            parent_found = False
            for parent in sections_dict.values():
                if parent['level'] == level - 1 and not parent_found:
                    parent['subsections'].append(section_data)
                    parent_found = True
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–æ–¥–∏—Ç–µ–ª—è, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ—Ä–µ–Ω—å
            if not parent_found:
                hierarchy.append(section_data)
    
    return hierarchy

def _extract_paragraphs(self, content: str, elements: List[DocumentElement]) -> List[Dict]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–±–∑–∞—Ü–µ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏"""
    paragraphs = []
    
    for element in elements:
        if isinstance(element, DocumentParagraph):
            paragraph_data = {
                'id': element.id,
                'content': element.content,
                'section_id': element.section_id,
                'position': element.position,
                'page_number': element.page_number,
                'metadata': element.metadata,
                'word_count': len(element.content.split()),
                'char_count': len(element.content)
            }
            paragraphs.append(paragraph_data)
    
    return paragraphs

def _extract_enhanced_tables(self, content: str, lines: List[str]) -> List[Dict]:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü"""
    tables = []
    current_table = None
    in_table = False
    table_rows = []
    
    for line_num, line in enumerate(lines):
        line = line.strip()
        
        # –ü–æ–∏—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
        table_match = re.search(self.table_patterns['table_header'], line, re.IGNORECASE)
        if table_match:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ç–∞–±–ª–∏—Ü—É
            if current_table and table_rows:
                current_table['rows'] = table_rows
                tables.append(current_table)
            
            # –ù–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞
            current_table = {
                'id': f'table_{len(tables)+1}',
                'number': table_match.group(1),
                'title': table_match.group(2).strip() if len(table_match.groups()) > 1 else '',
                'headers': [],
                'rows': [],
                'page_number': self._estimate_page(line_num, len(lines)),
                'metadata': {
                    'source_line': line_num,
                    'table_type': 'structured'
                }
            }
            table_rows = []
            in_table = True
            continue
        
        if in_table and '|' in line:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
            if re.match(self.table_patterns['table_separator'], line):
                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏
            
            cells = [cell.strip() for cell in line.split('|') if cell.strip()]
            if cells:
                if not current_table['headers']:
                    current_table['headers'] = cells
                else:
                    table_rows.append(cells)
        elif in_table and current_table:
            # –ö–æ–Ω–µ—Ü —Ç–∞–±–ª–∏—Ü—ã
            if table_rows:
                current_table['rows'] = table_rows
                tables.append(current_table)
            in_table = False
            current_table = None
            table_rows = []
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–∞–±–ª–∏—Ü—É
    if current_table and table_rows:
        current_table['rows'] = table_rows
        tables.append(current_table)
    
    return tables

def _extract_enhanced_lists(self, content: str, lines: List[str]) -> List[Dict]:
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤"""
    lists = []
    current_list = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        list_match = self._match_list_line(line)
        if list_match:
            list_type, marker, text = list_match
            
            # –ù–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –∏–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ
            if not current_list or current_list['type'] != list_type:
                if current_list:
                    lists.append(current_list)
                
                current_list = {
                    'id': f'list_{len(lists)+1}',
                    'type': list_type,
                    'items': [],
                    'metadata': {
                        'list_type': list_type,
                        'item_count': 0
                    }
                }
            
            # –î–æ–±–∞–≤–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å–ø–∏—Å–∫–∞
            current_list['items'].append({
                'marker': marker,
                'text': text,
                'level': self._determine_list_level(line)
            })
        elif current_list:
            # –ö–æ–Ω–µ—Ü —Å–ø–∏—Å–∫–∞
            current_list['metadata']['item_count'] = len(current_list['items'])
            lists.append(current_list)
            current_list = None
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ø–∏—Å–æ–∫
    if current_list:
        current_list['metadata']['item_count'] = len(current_list['items'])
        lists.append(current_list)
    
    return lists

def _determine_list_level(self, line: str) -> int:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —ç–ª–µ–º–µ–Ω—Ç–∞ —Å–ø–∏—Å–∫–∞"""
    leading_spaces = len(line) - len(line.lstrip())
    return max(1, leading_spaces // 4 + 1)

def _calculate_enhanced_statistics(self, content: str, elements: List[DocumentElement], 
                                 tables: List[Dict], lists: List[Dict]) -> Dict[str, Any]:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    
    # –ü–æ–¥—Å—á–µ—Ç —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–∏–ø–∞–º
    element_counts = {}
    for element in elements:
        element_type = type(element).__name__
        element_counts[element_type] = element_counts.get(element_type, 0) + 1
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–¥–µ–ª–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—è–º
    section_levels = {}
    for element in elements:
        if isinstance(element, DocumentSection):
            level = element.level
            section_levels[f'level_{level}'] = section_levels.get(f'level_{level}', 0) + 1
    
    return {
        'content_length': len(content),
        'word_count': len(content.split()),
        'line_count': len(content.split('\n')),
        'paragraph_count': len([p for p in content.split('\n\n') if p.strip()]),
        'element_counts': element_counts,
        'section_levels': section_levels,
        'table_count': len(tables),
        'list_count': len(lists),
        'list_items_total': sum(len(lst.get('items', [])) for lst in lists),
        'has_appendices': any('appendix' in str(elem.type).lower() for elem in elements if hasattr(elem, 'type')),
        'technical_density': self._calculate_technical_density(content),
        'avg_section_length': np.mean([len(elem.content) for elem in elements if isinstance(elem, DocumentSection)]) if any(isinstance(elem, DocumentSection) for elem in elements) else 0
    }

def _calculate_technical_density(self, content: str) -> float:
    """–†–∞—Å—á–µ—Ç –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤"""
    technical_patterns = [
        r'\b\d+(?:\.\d+)?\s*(?:–º–º|—Å–º|–º|–∫–º|–≥|–∫–≥|—Ç|–ú–ü–∞|–∫–ü–∞|¬∞C)\b',
        r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü)\s+[\d.-]+',
        r'\b\d+(?:\.\d+)*\s*%\b'
    ]
    
    technical_count = 0
    for pattern in technical_patterns:
        technical_count += len(re.findall(pattern, content, re.IGNORECASE))
    
    word_count = len(content.split())
    return technical_count / word_count if word_count > 0 else 0.0

def _calculate_extraction_quality_score(self, elements: List[DocumentElement], tables: List[Dict], 
                                      lists: List[Dict], metadata: Dict[str, Any]) -> float:
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
    score = 0.0
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (30%)
    metadata_fields = ['document_title', 'document_number', 'organization']
    filled_metadata = sum(1 for field in metadata_fields if metadata.get(field))
    metadata_score = (filled_metadata / len(metadata_fields)) * 0.3
    score += metadata_score
    
    # –ö–∞—á–µ—Å—Ç–≤–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã (40%)
    sections_count = sum(1 for elem in elements if isinstance(elem, DocumentSection))
    structure_score = min(sections_count / 10.0, 1.0) * 0.4
    score += structure_score
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–∞–±–ª–∏—Ü (15%)
    table_score = min(len(tables) / 5.0, 1.0) * 0.15
    score += table_score
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–æ–≤ (15%)
    list_score = min(len(lists) / 8.0, 1.0) * 0.15
    score += list_score
    
    return min(score, 1.0)

def _extract_document_keywords(self, content: str, doc_type: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    keywords = []
    
    # –¢–∏–ø–æ—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    type_patterns = {
        'sp': [r'\b(—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ|–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ|—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è|–Ω–æ—Ä–º—ã|–ø—Ä–∞–≤–∏–ª–∞)\b'],
        'gost': [r'\b(—Å—Ç–∞–Ω–¥–∞—Ä—Ç|–∫–∞—á–µ—Å—Ç–≤–æ|–∏—Å–ø—ã—Ç–∞–Ω–∏—è|–º–µ—Ç–æ–¥—ã|—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è)\b'],
        'snip': [r'\b(–Ω–æ—Ä–º—ã|–ø—Ä–∞–≤–∏–ª–∞|—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ|–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)\b'],
        'rd': [r'\b(—Ä—É–∫–æ–≤–æ–¥—è—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç|–º–µ—Ç–æ–¥–∏–∫–∞|–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è|—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏)\b']
    }
    
    patterns = type_patterns.get(doc_type, type_patterns['sp'])
    
    for pattern in patterns:
        matches = re.findall(pattern, content.lower(), re.IGNORECASE)
        keywords.extend(matches)
    
    return list(dict.fromkeys(keywords))[:10]  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã

def _extract_document_references(self, content: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
    patterns = [
        r'\b(?:–ì–û–°–¢|–°–ü|–°–ù–∏–ü|–¢–£|–û–°–¢)\s+[\d.-]+(?:-\d+)?',
        r'\b(?:–ø\.|–ø—É–Ω–∫—Ç|—Ä–∞–∑–¥–µ–ª)\s+\d+(?:\.\d+)*'
    ]
    
    references = []
    for pattern in patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        references.extend(matches)
    
    return list(dict.fromkeys(references))[:20]

# –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–æ–¥—ã –∫ –∫–ª–∞—Å—Å—É
AdvancedStructureExtractor._build_sections_hierarchy = _build_sections_hierarchy
AdvancedStructureExtractor._extract_paragraphs = _extract_paragraphs
AdvancedStructureExtractor._extract_enhanced_tables = _extract_enhanced_tables
AdvancedStructureExtractor._extract_enhanced_lists = _extract_enhanced_lists
AdvancedStructureExtractor._determine_list_level = _determine_list_level
AdvancedStructureExtractor._calculate_enhanced_statistics = _calculate_enhanced_statistics
AdvancedStructureExtractor._calculate_technical_density = _calculate_technical_density
AdvancedStructureExtractor._calculate_extraction_quality_score = _calculate_extraction_quality_score
AdvancedStructureExtractor._extract_document_keywords = _extract_document_keywords
AdvancedStructureExtractor._extract_document_references = _extract_document_references

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
def process_document_with_intelligent_chunking(content: str, file_path: str = "") -> Dict[str, Any]:
    """
    üîß –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —á–∞–Ω–∫–∏–Ω–≥–æ–º
    
    Args:
        content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
    
    Returns:
        API-—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏
    """
    system = IntegratedStructureChunkingSystem()
    return system.process_document(content, file_path)

if __name__ == "__main__":
    print("üîß Integrated Structure & Chunking System v3 - Ready!")
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    test_content = """
–°–ü 50.13330.2012

–¢–ï–ü–õ–û–í–ê–Ø –ó–ê–©–ò–¢–ê –ó–î–ê–ù–ò–ô

–ê–∫—Ç—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–µ–¥–∞–∫—Ü–∏—è –°–ù–∏–ü 23-02-2003

–£–¢–í–ï–†–ñ–î–ï–ù–û
–ú–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤–æ–º —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è –†–§
–ø—Ä–∏–∫–∞–∑–æ–º –æ—Ç 30 –∏—é–Ω—è 2012 –≥. ‚Ññ 265

1. –û–ë–©–ò–ï –ü–û–õ–û–ñ–ï–ù–ò–Ø

1.1 –ù–∞—Å—Ç–æ—è—â–∏–π —Å–≤–æ–¥ –ø—Ä–∞–≤–∏–ª —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∑–∞—â–∏—Ç—ã –∑–¥–∞–Ω–∏–π –≤—Å–µ—Ö –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–π.

1.2 –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ —Å–≤–æ–¥–∞ –ø—Ä–∞–≤–∏–ª —è–≤–ª—è—é—Ç—Å—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–º–∏ –¥–ª—è –≤—Å–µ—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏—Ö –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã—Ö —Ñ–æ—Ä–º.

2. –ù–û–†–ú–ê–¢–ò–í–ù–´–ï –°–°–´–õ–ö–ò

–í –Ω–∞—Å—Ç–æ—è—â–µ–º —Å–≤–æ–¥–µ –ø—Ä–∞–≤–∏–ª –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã:
- –ì–û–°–¢ 30494-2011 –ó–¥–∞–Ω–∏—è –∂–∏–ª—ã–µ –∏ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∏–∫—Ä–æ–∫–ª–∏–º–∞—Ç–∞ –≤ –ø–æ–º–µ—â–µ–Ω–∏—è—Ö
- –°–ü 23-101-2004 –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∑–∞—â–∏—Ç—ã –∑–¥–∞–Ω–∏–π

–¢–∞–±–ª–∏—Ü–∞ 1 - –ù–æ—Ä–º–∏—Ä—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏—è —Ç–µ–ø–ª–æ–ø–µ—Ä–µ–¥–∞—á–µ

|–¢–∏–ø –∑–¥–∞–Ω–∏—è|R‚ÇÄ, –º¬≤¬∑¬∞–°/–í—Ç|–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ|
|–ñ–∏–ª—ã–µ|3,5|–î–ª—è –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —É—Å–ª–æ–≤–∏–π|
|–û–±—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ|2,8|–ü—Ä–∏ —Ä–∞—Å—á–µ—Ç–Ω–æ–π —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ|

3. –¢–ï–†–ú–ò–ù–´ –ò –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø

3.1 –í –Ω–∞—Å—Ç–æ—è—â–µ–º —Å–≤–æ–¥–µ –ø—Ä–∞–≤–∏–ª –ø—Ä–∏–º–µ–Ω–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ç–µ—Ä–º–∏–Ω—ã —Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏:

–∞) —Ç–µ–ø–ª–æ–≤–∞—è –∑–∞—â–∏—Ç–∞ –∑–¥–∞–Ω–∏—è: –∫–æ–º–ø–ª–µ–∫—Å –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π;
–±) —Å–æ–ø—Ä–æ—Ç–∏–≤–ª–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–ø–µ—Ä–µ–¥–∞—á–µ: –≤–µ–ª–∏—á–∏–Ω–∞, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É—é—â–∞—è —Ç–µ–ø–ª–æ–∑–∞—â–∏—Ç–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞.
"""
    
    result = process_document_with_intelligent_chunking(test_content, "test_sp.pdf")
    
    print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
    print(f"  üìÑ –î–æ–∫—É–º–µ–Ω—Ç: {result['document_info']['title']}")
    print(f"  üî¢ –ù–æ–º–µ—Ä: {result['document_info']['number']}")
    print(f"  üìä –†–∞–∑–¥–µ–ª–æ–≤: {len(result['sections'])}")
    print(f"  üß© –ß–∞–Ω–∫–æ–≤: {len(result['chunks'])}")
    print(f"  üìã –¢–∞–±–ª–∏—Ü: {len(result['tables'])}")
    print(f"  üìà –ö–∞—á–µ—Å—Ç–≤–æ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {result['processing_info']['structure_quality']:.2f}")
    print(f"  üéØ –ö–∞—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–∏–Ω–≥–∞: {result['processing_info']['chunking_quality']:.2f}")
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —á–∞–Ω–∫–æ–≤:")
    for chunk_type, count in result['statistics']['chunk_types_distribution'].items():
        print(f"  - {chunk_type}: {count}")
        
    print("\nüîß –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Enhanced RAG Trainer!")