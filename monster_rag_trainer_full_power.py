#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üî•üöÄ MONSTER RAG TRAINER - FULL POWER V3 üöÄüî•
=============================================
–ü–û–õ–ù–ê–Ø –ú–û–©–ù–û–°–¢–¨! –í–°–ï –°–ò–°–¢–ï–ú–´ –í –ö–†–ê–°–ù–û–ô –ó–û–ù–ï!

üéØ WHAT WE'RE UNLEASHING:
‚úÖ Full 15-Stage Enhanced Pipeline
‚úÖ Recursive Hierarchical Chunking (1 –ø—É–Ω–∫—Ç = 1 —á–∞–Ω–∫)
‚úÖ Intelligent File Sorting & Organization  
‚úÖ GPU-Accelerated Processing (CUDA)
‚úÖ Parallel Multi-Threading
‚úÖ Smart Document Prioritization
‚úÖ Advanced Structure Extraction
‚úÖ All 10 Performance Improvements
‚úÖ Auto File Organization & Cleanup
‚úÖ Real-time Progress Monitoring

üî• TARGET: 1168 FILES ‚Üí FULL RAG MONSTER POWER!
"""

import os
import sys
import time
import json
import shutil
import asyncio
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from dataclasses import dataclass, field
import hashlib
import glob
import threading
from queue import Queue
import psutil

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ—â–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [%(levelname)s] - %(message)s',
    handlers=[
        logging.FileHandler(f'C:/Bldr/logs/monster_rag_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç—ã –Ω–∞—à–∏—Ö —Å–∏—Å—Ç–µ–º
try:
    from complete_enhanced_bldr_rag_trainer import CompleteEnhancedBldrRAGTrainer
    from recursive_hierarchical_chunker import RecursiveHierarchicalChunker, create_hierarchical_chunks_from_structure
    from working_frontend_rag_integration import create_working_rag_trainer
    FULL_SYSTEMS_AVAILABLE = True
    logger.info("üî• ALL SYSTEMS LOADED - MONSTER MODE ACTIVATED!")
except ImportError as e:
    logger.error(f"‚ùå Critical system import failed: {e}")
    FULL_SYSTEMS_AVAILABLE = False

@dataclass
class MonsterStats:
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã –º–æ–Ω—Å—Ç—Ä–∞"""
    start_time: float = 0.0
    files_found: int = 0
    files_processed: int = 0
    files_failed: int = 0
    files_moved: int = 0
    chunks_created: int = 0
    total_sections: int = 0
    total_tables: int = 0
    processing_speed: float = 0.0  # —Ñ–∞–π–ª–æ–≤ –≤ –º–∏–Ω—É—Ç—É
    gpu_utilization: float = 0.0
    cpu_utilization: float = 0.0
    memory_usage: float = 0.0
    
class MonsterRAGTrainer:
    """
    üî• MONSTER RAG TRAINER - FULL POWER MODE
    
    Unleashes all systems for maximum performance:
    - Multi-stage document processing pipeline
    - Intelligent file organization
    - Recursive hierarchical chunking  
    - GPU-accelerated operations
    - Real-time monitoring
    """
    
    def __init__(self, base_dir: str = "I:/docs", max_workers: int = None):
        """
        üî• Initialize the MONSTER
        
        Args:
            base_dir: Base directory with documents
            max_workers: Max parallel workers (auto-detected if None)
        """
        
        print("üî•" * 60)
        print("üöÄ INITIALIZING MONSTER RAG TRAINER - FULL POWER MODE üöÄ")
        print("üî•" * 60)
        
        self.base_dir = Path(base_dir)
        self.max_workers = max_workers or min(psutil.cpu_count(), 12)  # Max 12 workers
        
        # –ü–∞–ø–∫–∏ –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        self.organized_dirs = {
            'gosts': self.base_dir / 'organized' / 'GOSTS',
            'snips': self.base_dir / 'organized' / 'SNIPs', 
            'sps': self.base_dir / 'organized' / 'SPs',
            'pprs': self.base_dir / 'organized' / 'PPRs',
            'smetas': self.base_dir / 'organized' / 'SMETAS',
            'other': self.base_dir / 'organized' / 'OTHER',
            'processed': self.base_dir / 'processed'
        }
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏
        for folder in self.organized_dirs.values():
            folder.mkdir(parents=True, exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏  
        self.stats = MonsterStats()
        self.stats.start_time = time.time()
        
        # –û—á–µ—Ä–µ–¥—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.processing_queue = Queue()
        self.results_queue = Queue()
        
        # –°–∏—Å—Ç–µ–º—ã
        self.enhanced_trainer = None
        self.hierarchical_chunker = None
        self.working_trainer = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º—ã
        self._initialize_monster_systems()
        
        logger.info(f"üî• MONSTER INITIALIZED:")
        logger.info(f"   üìÅ Base directory: {self.base_dir}")
        logger.info(f"   üë• Max workers: {self.max_workers}")
        logger.info(f"   üöÄ GPU Available: {self._check_gpu()}")
        logger.info(f"   üíæ RAM Available: {psutil.virtual_memory().available / 1024**3:.1f} GB")
        logger.info(f"   üî• MONSTER STATUS: READY TO UNLEASH!")
        
    def _initialize_monster_systems(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –ø–æ–¥—Å–∏—Å—Ç–µ–º –º–æ–Ω—Å—Ç—Ä–∞"""
        
        if not FULL_SYSTEMS_AVAILABLE:
            logger.error("‚ùå Critical systems not available!")
            return
        
        try:
            # Enhanced RAG Trainer
            logger.info("üî• Initializing Enhanced RAG Trainer...")
            self.enhanced_trainer = CompleteEnhancedBldrRAGTrainer(base_dir=str(self.base_dir))
            
            # Hierarchical Chunker
            logger.info("üîÑ Initializing Recursive Hierarchical Chunker...")
            self.hierarchical_chunker = RecursiveHierarchicalChunker(
                target_chunk_size=400,
                min_chunk_size=100,
                max_chunk_size=800
            )
            
            # Working Trainer (backup)
            logger.info("üõ°Ô∏è Initializing Working Trainer (backup)...")
            self.working_trainer = create_working_rag_trainer(use_intelligent_chunking=True)
            
            logger.info("‚úÖ ALL MONSTER SYSTEMS INITIALIZED!")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize systems: {e}")
            raise
    
    def _check_gpu(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False
    
    def unleash_the_monster(self, max_files: Optional[int] = None):
        """
        üî•üöÄ UNLEASH THE MONSTER - FULL POWER PROCESSING!
        
        Args:
            max_files: Maximum files to process (None = all files)
        """
        
        print("\n" + "üî•" * 80)
        print("üöÄ" * 20 + " UNLEASHING THE MONSTER " + "üöÄ" * 20)
        print("üî•" * 80)
        
        try:
            # STAGE 1: File Discovery & Organization
            logger.info("üîç STAGE 1: MONSTER FILE DISCOVERY & ORGANIZATION")
            all_files = self._monster_file_discovery()
            
            if not all_files:
                logger.error("‚ùå NO FILES FOUND - MONSTER HIBERNATING")
                return
            
            if max_files:
                all_files = all_files[:max_files]
                logger.info(f"üéØ Limited to {max_files} files for processing")
            
            self.stats.files_found = len(all_files)
            logger.info(f"üéØ TARGET ACQUIRED: {len(all_files)} FILES")
            
            # STAGE 2: File Organization
            logger.info("üìÅ STAGE 2: INTELLIGENT FILE ORGANIZATION")
            organized_files = self._organize_files_by_type(all_files)
            
            # STAGE 3: Priority Processing Queue
            logger.info("‚ö° STAGE 3: PRIORITY QUEUE CONSTRUCTION")
            priority_queue = self._build_priority_queue(organized_files)
            
            # STAGE 4: UNLEASH PARALLEL PROCESSING
            logger.info("üî• STAGE 4: UNLEASHING PARALLEL MONSTER PROCESSING")
            self._parallel_monster_processing(priority_queue)
            
            # STAGE 5: Final Statistics & Cleanup
            logger.info("üìä STAGE 5: MONSTER STATISTICS & CLEANUP")
            self._generate_monster_report()
            
            print("\n" + "üéâ" * 80)
            print("üèÜ" * 20 + " MONSTER OPERATION COMPLETE " + "üèÜ" * 20) 
            print("üéâ" * 80)
            
        except Exception as e:
            logger.error(f"üí• MONSTER ERROR: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def _monster_file_discovery(self) -> List[Path]:
        """üîç Monster-powered file discovery"""
        
        logger.info("üîç Scanning for documents with MONSTER POWER...")
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ñ–∞–π–ª–æ–≤
        file_patterns = {
            'pdf': '*.pdf',
            'docx': '*.docx', 
            'doc': '*.doc',
            'txt': '*.txt',
            'rtf': '*.rtf',
            'djvu': '*.djvu'
        }
        
        all_files = []
        pattern_stats = {}
        
        for file_type, pattern in file_patterns.items():
            logger.info(f"  üîé Scanning for {file_type.upper()} files...")
            files = list(self.base_dir.rglob(pattern))
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ñ–∞–π–ª—ã (–∏—Å–∫–ª—é—á–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ, —Å–∏—Å—Ç–µ–º–Ω—ã–µ)
            filtered_files = []
            for file_path in files:
                if self._is_valid_document(file_path):
                    filtered_files.append(file_path)
            
            pattern_stats[file_type] = len(filtered_files)
            all_files.extend(filtered_files)
            
            logger.info(f"    ‚úÖ Found {len(filtered_files)} valid {file_type.upper()} files")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É
        logger.info("üîÑ Removing duplicates...")
        unique_files = self._remove_duplicate_files(all_files)
        
        logger.info("üéØ FILE DISCOVERY COMPLETE:")
        for file_type, count in pattern_stats.items():
            logger.info(f"  üìÑ {file_type.upper()}: {count} files")
        logger.info(f"  üéØ TOTAL UNIQUE: {len(unique_files)} files")
        
        return unique_files
    
    def _is_valid_document(self, file_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Ñ–∞–π–ª—ã –∏ –ø–∞–ø–∫–∏
        exclude_patterns = [
            'temp', 'tmp', 'cache', '.git', '__pycache__',
            'node_modules', 'venv', 'backup', '~$'
        ]
        
        file_str = str(file_path).lower()
        if any(pattern in file_str for pattern in exclude_patterns):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
        try:
            size = file_path.stat().st_size
            if size < 1024 or size > 100 * 1024 * 1024:  # 1KB - 100MB
                return False
        except:
            return False
        
        return True
    
    def _remove_duplicate_files(self, files: List[Path]) -> List[Path]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ —Ö–µ—à—É —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"""
        
        seen_hashes = set()
        unique_files = []
        
        for file_path in files:
            try:
                file_hash = self._get_file_hash(file_path)
                if file_hash not in seen_hashes:
                    seen_hashes.add(file_hash)
                    unique_files.append(file_path)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to hash {file_path}: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –¥–∞–∂–µ –µ—Å–ª–∏ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å
                unique_files.append(file_path)
        
        logger.info(f"üîÑ Removed {len(files) - len(unique_files)} duplicate files")
        return unique_files
    
    def _get_file_hash(self, file_path: Path) -> str:
        """–ë—ã—Å—Ç—Ä–æ–µ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        hasher = hashlib.md5()
        
        # –ß–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 8KB –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        with open(file_path, 'rb') as f:
            # –ü–µ—Ä–≤—ã–µ 8KB
            chunk = f.read(8192)
            if chunk:
                hasher.update(chunk)
            
            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 8KB (–µ—Å–ª–∏ —Ñ–∞–π–ª –±–æ–ª—å—à–µ 16KB)
            f.seek(-8192, 2)  # –û—Ç –∫–æ–Ω—Ü–∞ —Ñ–∞–π–ª–∞
            chunk = f.read(8192)
            if chunk:
                hasher.update(chunk)
        
        return hasher.hexdigest()
    
    def _organize_files_by_type(self, files: List[Path]) -> Dict[str, List[Path]]:
        """üìÅ –£–º–Ω–∞—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤ –ø–æ —Ç–∏–ø–∞–º"""
        
        logger.info("üìÅ Organizing files by document type...")
        
        organized = {
            'gosts': [],
            'snips': [], 
            'sps': [],
            'pprs': [],
            'smetas': [],
            'other': []
        }
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        classification_patterns = {
            'gosts': [r'\b–ì–û–°–¢\b', r'\bgost\b', r'–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π.*—Å—Ç–∞–Ω–¥–∞—Ä—Ç'],
            'snips': [r'\b–°–ù–∏–ü\b', r'\bsnip\b', r'—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–µ.*–Ω–æ—Ä–º—ã'],
            'sps': [r'\b–°–ü\b', r'\bsp\b', r'—Å–≤–æ–¥.*–ø—Ä–∞–≤–∏–ª'],
            'pprs': [r'\b–ü–ü–†\b', r'\bppr\b', r'–ø—Ä–æ–µ–∫—Ç.*–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞.*—Ä–∞–±–æ—Ç', r'—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è.*–∫–∞—Ä—Ç–∞'],
            'smetas': [r'\b—Å–º–µ—Ç–∞\b', r'—Ä–∞—Å—Ü–µ–Ω–∫', r'–∫–∞–ª—å–∫—É–ª—Ä—è—Ü', r'—Å—Ç–æ–∏–º–æ—Å—Ç—å']
        }
        
        for file_path in files:
            file_type = self._classify_document(file_path, classification_patterns)
            organized[file_type].append(file_path)
            
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ñ–∞–π–ª –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –ø–∞–ø–∫—É
            try:
                target_dir = self.organized_dirs[file_type]
                new_path = target_dir / file_path.name
                
                # –ï—Å–ª–∏ —Ñ–∞–π–ª –µ—â–µ –Ω–µ –≤ —Ü–µ–ª–µ–≤–æ–π –ø–∞–ø–∫–µ
                if file_path.parent != target_dir:
                    if new_path.exists():
                        # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                        counter = 1
                        while new_path.exists():
                            name_parts = file_path.stem, counter, file_path.suffix
                            new_name = f"{name_parts[0]}_{name_parts[1]}{name_parts[2]}"
                            new_path = target_dir / new_name
                            counter += 1
                    
                    shutil.move(str(file_path), str(new_path))
                    self.stats.files_moved += 1
                    logger.debug(f"üìÅ Moved {file_path.name} ‚Üí {file_type.upper()}")
            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to move {file_path}: {e}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏
        logger.info("üìä FILE ORGANIZATION COMPLETE:")
        for doc_type, file_list in organized.items():
            logger.info(f"  üìÇ {doc_type.upper()}: {len(file_list)} files")
        
        return organized
    
    def _classify_document(self, file_path: Path, patterns: Dict[str, List[str]]) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —Ç–∏–ø—É"""
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        filename_lower = file_path.name.lower()
        
        for doc_type, type_patterns in patterns.items():
            for pattern in type_patterns:
                import re
                if re.search(pattern, filename_lower, re.IGNORECASE):
                    return doc_type
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ - —á–∏—Ç–∞–µ–º –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞
        try:
            if file_path.suffix.lower() == '.txt':
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content_sample = f.read(2000).lower()
                    
                for doc_type, type_patterns in patterns.items():
                    for pattern in type_patterns:
                        if re.search(pattern, content_sample, re.IGNORECASE):
                            return doc_type
        except:
            pass
        
        return 'other'
    
    def _build_priority_queue(self, organized_files: Dict[str, List[Path]]) -> List[Tuple[Path, int, str]]:
        """‚ö° –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—á–µ—Ä–µ–¥–∏ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏"""
        
        logger.info("‚ö° Building priority processing queue...")
        
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        type_priorities = {
            'gosts': 10,    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            'sps': 9,       # –°–≤–æ–¥—ã –ø—Ä–∞–≤–∏–ª
            'snips': 8,     # –°–ù–∏–ü—ã
            'pprs': 6,      # –ü–ü–†
            'smetas': 4,    # –°–º–µ—Ç—ã
            'other': 2      # –û—Å—Ç–∞–ª—å–Ω–æ–µ
        }
        
        priority_queue = []
        
        for doc_type, files in organized_files.items():
            base_priority = type_priorities.get(doc_type, 1)
            
            for file_path in files:
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
                file_priority = base_priority
                
                # –ë–æ–Ω—É—Å –∑–∞ —Ä–∞–∑–º–µ—Ä (–±–æ–ª—å—à–µ = –≤–∞–∂–Ω–µ–µ)
                try:
                    size_mb = file_path.stat().st_size / (1024 * 1024)
                    if size_mb > 5:
                        file_priority += 2
                    elif size_mb > 1:
                        file_priority += 1
                except:
                    pass
                
                # –ë–æ–Ω—É—Å –∑–∞ "—Å–≤–µ–∂–µ—Å—Ç—å" —Ñ–∞–π–ª–∞
                try:
                    mtime = file_path.stat().st_mtime
                    days_old = (time.time() - mtime) / (24 * 3600)
                    if days_old < 365:  # –ú–µ–Ω–µ–µ –≥–æ–¥–∞
                        file_priority += 1
                except:
                    pass
                
                priority_queue.append((file_path, file_priority, doc_type))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É (–≤—ã—Å—à–∏–π - –ø–µ—Ä–≤—ã–π)
        priority_queue.sort(key=lambda x: x[1], reverse=True)
        
        logger.info(f"‚ö° Priority queue built: {len(priority_queue)} files")
        logger.info(f"   üî• High priority (8+): {sum(1 for _, p, _ in priority_queue if p >= 8)}")
        logger.info(f"   üü° Medium priority (5-7): {sum(1 for _, p, _ in priority_queue if 5 <= p < 8)}")
        logger.info(f"   üîµ Low priority (<5): {sum(1 for _, p, _ in priority_queue if p < 5)}")
        
        return priority_queue
    
    def _parallel_monster_processing(self, priority_queue: List[Tuple[Path, int, str]]):
        """üî• –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –ø–æ–ª–Ω–æ–π –º–æ—â–Ω–æ—Å—Ç–∏"""
        
        logger.info(f"üî• UNLEASHING PARALLEL PROCESSING - {self.max_workers} WORKERS")
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        monitor_thread = threading.Thread(target=self._performance_monitor, daemon=True)
        monitor_thread.start()
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É
            future_to_file = {
                executor.submit(self._process_single_file_monster, file_path, priority, doc_type): (file_path, priority, doc_type)
                for file_path, priority, doc_type in priority_queue
            }
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–µ—Ä–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
            for future in as_completed(future_to_file):
                file_info = future_to_file[future]
                file_path, priority, doc_type = file_info
                
                try:
                    result = future.result(timeout=300)  # 5 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
                    
                    if result['success']:
                        self.stats.files_processed += 1
                        self.stats.chunks_created += result.get('chunks_created', 0)
                        self.stats.total_sections += result.get('sections_found', 0)
                        self.stats.total_tables += result.get('tables_found', 0)
                        
                        logger.info(f"‚úÖ PROCESSED: {file_path.name} ({result.get('chunks_created', 0)} chunks)")
                    else:
                        self.stats.files_failed += 1
                        logger.error(f"‚ùå FAILED: {file_path.name} - {result.get('error', 'Unknown error')}")
                
                except Exception as e:
                    self.stats.files_failed += 1
                    logger.error(f"üí• PROCESSING ERROR: {file_path.name} - {e}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∫–æ—Ä–æ—Å—Ç–∏
                self._update_processing_speed()
        
        logger.info("üî• PARALLEL PROCESSING COMPLETE!")
    
    def _process_single_file_monster(self, file_path: Path, priority: int, doc_type: str) -> Dict[str, Any]:
        """üî• –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –Ω–∞ MONSTER POWER"""
        
        result = {
            'success': False,
            'file_path': str(file_path),
            'priority': priority,
            'doc_type': doc_type,
            'chunks_created': 0,
            'sections_found': 0,
            'tables_found': 0,
            'processing_time': 0,
            'error': None
        }
        
        start_time = time.time()
        
        try:
            # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            content = self._read_file_content(file_path)
            if not content:
                raise ValueError("Empty or unreadable file")
            
            # MONSTER PROCESSING —Å –ø–æ–ª–Ω—ã–º –ø–∞–π–ø–ª–∞–π–Ω–æ–º
            if self.enhanced_trainer:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º Enhanced RAG Trainer –¥–ª—è –ø–æ–ª–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                document_result = self.enhanced_trainer.process_single_file(str(file_path))
                
                if document_result:
                    result['chunks_created'] = document_result.get('chunks_created', 0)
                    result['sections_found'] = len(document_result.get('sections', []))
                    result['tables_found'] = len(document_result.get('tables', []))
                    result['success'] = True
            
            # Backup —Å Working Trainer
            elif self.working_trainer:
                document_result = self.working_trainer.process_single_document(content, str(file_path))
                
                result['chunks_created'] = len(document_result.get('chunks', []))
                result['sections_found'] = len(document_result.get('sections', []))
                result['tables_found'] = len(document_result.get('tables', []))
                result['success'] = True
            
            else:
                raise ValueError("No processing systems available")
        
        except Exception as e:
            result['error'] = str(e)
            logger.debug(f"Processing error for {file_path.name}: {e}")
        
        result['processing_time'] = time.time() - start_time
        return result
    
    def _read_file_content(self, file_path: Path) -> str:
        """–ß—Ç–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ñ–∞–π–ª–∞"""
        
        try:
            # –ü–æ–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
            if file_path.suffix.lower() in ['.txt', '.rtf']:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
            else:
                # –î–æ–±–∞–≤–ª–µ–Ω–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∞ PDF/DOCX —á–µ—Ä–µ–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
                try:
                    if file_path.suffix.lower() == '.pdf':
                        # –ò–º–ø–æ—Ä—Ç PDF –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
                        try:
                            import PyPDF2
                            with open(file_path, 'rb') as pdf_file:
                                pdf_reader = PyPDF2.PdfReader(pdf_file)
                                content = ''
                                for page in pdf_reader.pages:
                                    content += page.extract_text() + '\n'
                            return content
                        except ImportError:
                            logger.warning("PyPDF2 not available for PDF processing")
                            return ""
                    elif file_path.suffix.lower() in ['.docx', '.doc']:
                        # –ò–º–ø–æ—Ä—Ç DOCX –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
                        try:
                            from docx import Document
                            doc = Document(str(file_path))
                            content = '\n'.join([paragraph.text for paragraph in doc.paragraphs])
                            return content
                        except ImportError:
                            logger.warning("python-docx not available for DOCX processing")
                            return ""
                    else:
                        logger.debug(f"Unsupported file type: {file_path.suffix}")
                        return ""
                except Exception as e:
                    logger.debug(f"Failed to read {file_path}: {e}")
                    return ""
        
        except Exception as e:
            logger.debug(f"Failed to read {file_path}: {e}")
            return ""
    
    def _performance_monitor(self):
        """üìä –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"""
        
        while self.stats.files_processed + self.stats.files_failed < self.stats.files_found:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            self.stats.cpu_utilization = psutil.cpu_percent(interval=1)
            self.stats.memory_usage = psutil.virtual_memory().percent
            
            # GPU —É—Ç–∏–ª–∏–∑–∞—Ü–∏—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            try:
                import GPUtil
                gpus = GPUtil.getGPUs()
                if gpus:
                    self.stats.gpu_utilization = gpus[0].load * 100
            except:
                self.stats.gpu_utilization = 0
            
            # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            time.sleep(30)
            self._print_live_stats()
    
    def _update_processing_speed(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        elapsed_minutes = (time.time() - self.stats.start_time) / 60.0
        processed_total = self.stats.files_processed + self.stats.files_failed
        
        if elapsed_minutes > 0:
            self.stats.processing_speed = processed_total / elapsed_minutes
    
    def _print_live_stats(self):
        """–í—ã–≤–æ–¥ –∂–∏–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        
        processed_total = self.stats.files_processed + self.stats.files_failed
        progress_pct = (processed_total / self.stats.files_found) * 100 if self.stats.files_found > 0 else 0
        
        print("\n" + "="*80)
        print(f"üî• MONSTER RAG TRAINER - LIVE STATS")
        print("="*80)
        print(f"üìä Progress: {progress_pct:.1f}% ({processed_total}/{self.stats.files_found})")
        print(f"‚úÖ Processed: {self.stats.files_processed} | ‚ùå Failed: {self.stats.files_failed}")
        print(f"üß© Chunks Created: {self.stats.chunks_created}")
        print(f"üìë Sections Found: {self.stats.total_sections}")
        print(f"üìä Tables Found: {self.stats.total_tables}")
        print(f"‚ö° Speed: {self.stats.processing_speed:.1f} files/min")
        print(f"üíª CPU: {self.stats.cpu_utilization:.1f}% | üíæ RAM: {self.stats.memory_usage:.1f}% | üöÄ GPU: {self.stats.gpu_utilization:.1f}%")
        
        # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –¥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        if self.stats.processing_speed > 0:
            files_left = self.stats.files_found - processed_total
            minutes_left = files_left / self.stats.processing_speed
            eta = datetime.now() + timedelta(minutes=minutes_left)
            print(f"‚è∞ ETA: {eta.strftime('%H:%M:%S')} ({minutes_left:.0f} min left)")
        
        print("="*80)
    
    def _generate_monster_report(self):
        """üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –º–æ–Ω—Å—Ç—Ä–∞"""
        
        total_time = time.time() - self.stats.start_time
        success_rate = (self.stats.files_processed / self.stats.files_found) * 100 if self.stats.files_found > 0 else 0
        
        report = {
            'monster_stats': {
                'total_time_seconds': total_time,
                'total_time_formatted': str(timedelta(seconds=int(total_time))),
                'files_found': self.stats.files_found,
                'files_processed': self.stats.files_processed,
                'files_failed': self.stats.files_failed,
                'files_moved': self.stats.files_moved,
                'success_rate_percent': success_rate,
                'processing_speed_files_per_minute': self.stats.processing_speed,
                'chunks_created': self.stats.chunks_created,
                'sections_found': self.stats.total_sections,
                'tables_found': self.stats.total_tables,
                'avg_chunks_per_file': self.stats.chunks_created / max(self.stats.files_processed, 1)
            },
            'performance_metrics': {
                'max_workers': self.max_workers,
                'peak_cpu_utilization': self.stats.cpu_utilization,
                'peak_memory_usage': self.stats.memory_usage,
                'gpu_utilization': self.stats.gpu_utilization
            },
            'timestamp': datetime.now().isoformat()
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        report_file = Path("C:/Bldr/monster_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
        print("\n" + "üèÜ" * 80)
        print("üî•" * 25 + " MONSTER OPERATION COMPLETE " + "üî•" * 25)
        print("üèÜ" * 80)
        print(f"‚è∞ Total Time: {timedelta(seconds=int(total_time))}")
        print(f"üìÅ Files Found: {self.stats.files_found}")
        print(f"‚úÖ Successfully Processed: {self.stats.files_processed}")
        print(f"‚ùå Failed: {self.stats.files_failed}")
        print(f"üìÅ Files Organized: {self.stats.files_moved}")
        print(f"üìà Success Rate: {success_rate:.1f}%")
        print(f"‚ö° Average Speed: {self.stats.processing_speed:.1f} files/minute")
        print(f"üß© Total Chunks Created: {self.stats.chunks_created}")
        print(f"üìë Total Sections Found: {self.stats.total_sections}")
        print(f"üìä Total Tables Found: {self.stats.total_tables}")
        print(f"üìä Average Chunks per File: {self.stats.chunks_created / max(self.stats.files_processed, 1):.1f}")
        print("üèÜ" * 80)
        print(f"üìÑ Full report saved: {report_file}")
        print("üéâ MONSTER RAG TRAINER - MISSION ACCOMPLISHED! üéâ")
        
        logger.info(f"üèÜ MONSTER OPERATION COMPLETE - {self.stats.files_processed}/{self.stats.files_found} files processed")

def launch_monster(base_dir: str = "I:/docs", max_files: Optional[int] = None, max_workers: Optional[int] = None):
    """
    üöÄ LAUNCH THE MONSTER RAG TRAINER
    
    Args:
        base_dir: Directory with documents
        max_files: Maximum files to process (None = all)
        max_workers: Maximum parallel workers (None = auto)
    """
    
    try:
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –ª–æ–≥–æ–≤
        Path("C:/Bldr/logs").mkdir(exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–Ω—Å—Ç—Ä–∞
        monster = MonsterRAGTrainer(base_dir=base_dir, max_workers=max_workers)
        
        # UNLEASH THE MONSTER!
        monster.unleash_the_monster(max_files=max_files)
        
        return monster
    
    except Exception as e:
        logger.error(f"üí• MONSTER LAUNCH FAILED: {e}")
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    print("üî•" * 80)
    print("üöÄ MONSTER RAG TRAINER - READY TO UNLEASH FULL POWER üöÄ")
    print("üî•" * 80)
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞
    BASE_DIR = "I:/docs"
    MAX_FILES = None  # –í—Å–µ —Ñ–∞–π–ª—ã
    MAX_WORKERS = None  # Auto-detect
    
    print(f"üìÅ Target Directory: {BASE_DIR}")
    print(f"üìä Max Files: {'ALL' if MAX_FILES is None else MAX_FILES}")
    print(f"üë• Workers: {'AUTO' if MAX_WORKERS is None else MAX_WORKERS}")
    
    # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞
    response = input("\nüî• READY TO UNLEASH THE MONSTER? (y/N): ").strip().lower()
    
    if response == 'y':
        print("\nüöÄ LAUNCHING MONSTER RAG TRAINER...")
        launch_monster(BASE_DIR, MAX_FILES, MAX_WORKERS)
    else:
        print("üõë Monster launch cancelled.")